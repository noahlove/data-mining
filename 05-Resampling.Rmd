---
title: "Resampling Notes"
author: "Noah Love"
date: "1/28/2021"
output: html_document
---

## Resampling

Often with inbalanced data sets you want to upsample (or downsample). You can create or get rid of data to make the sample more proportionate.

Alternatively you can Bootstrap the data. It is like asking what is the distribution of something we don't know. We can use simulation to get a sense of what the distribution is (often used for uncertainty).

However, we always need to avoid overfitting. It is important to also do cross-validation then.

### Up-sampling

![Up-sampling](A:%5CDocuments%5CSemester%206%5Cdata-mining-at-columbia%5CMy%20Class%20Notes%5CImages%5Cupsampling.JPG)

It works, but it is important to remember you aren't creating new data. You are essentially adding more weight to the already existing data. But it does help.

### Bootstrapping

If the data changes, how much will my model change? You have these questions about slightly different sets. So you create a similar bootstrap sample and do the same model on both. Then you look at the results. By collapsing across the samples (and doing more), you can see how sensitive the data is and how it changes with different inputs.

![Bootstrapping](A:%5CDocuments%5CSemester%206%5Cdata-mining-at-columbia%5CMy%20Class%20Notes%5CImages%5Cbootstrap.JPG)

### Cross Validation

When you generalize beyond the data set you have, how bad is your error? This is the problem of overfitting. You can't generalize. Cross validation tries to tell you how bad of a problem you have. If you use tuning hyperparameters however, you can help fix this. You can leave part of the original set out to test on later. You switch the training set and validation set for all of the test.

In general, when tuning for hyperparameters (like degrees of polynomials) you ahve to make a choice. For example earlier with polynomials we graphed it using validation and training. Once we pick the optimal hyperparameter from the training set tested on the validation set. Then we apply it onto the test set to get the generalization error.

In short: - Cross validation between train/validation creates the hyperparameter. (i.e. degrees of polynomial) - Then you use all the training and validation data to get the parameters. (coefficients given the polynomial) - Then predict on the test data to get the generalization error.

This should prevent the overfitting from the hyperparameter. If you reapply onto a test set that wasn't used to help you pick this should tell you if there is a problem.

There is some debate on double cross validation. You can then change the test set and revalidate. Some people argue why not? Our model should be more robust if we cycle through everything.

Reasons not to: Computational. It is resource intesnive. You can also leak data and ruin you generalization error.

## Initial Work

```{r}
df <- read.csv("Datasets/small_non_retweets_dc_inaug_steal.csv")
head(df,2)
```

Let's understand the flags, like steal_flag.

```{r}
df$steal_flag <- as.numeric(df$steal_flag == "True")
mean(df$steal_flag)
```

We are turning true or false into 0 or 1. This flag is rare. It is about 1.7 percent.

Steal means if the word "steal" was present in a tweet. We can examine some of them for example by using:

```{r}
head(df[df$steal_flag > 0, "text"], 4)
```

## A basic model with upsampling

We can create a new data object that stores only the indices of the places where steal_flag exists.

```{r}
pos_ind <- which(df$steal_flag > 0)
head(pos_ind)
```

So in this case, tweets 28, 91, 114... etc have the word steal.

Let's sample the indices now. This requires us to figure out what we want to upsample our data to. To do so we will make N 10 times the length of our current amount of steal_flags. What we are doing is artificially increasing our data set. Upsampling is somewhat of a hack in the sense. It makes the objective function give more weight to the positive yet rare cases in our data. It is sorta dirty but also helpful, especially for preliminary exploration. There are other ways to be better at weighing certain aspects.

Note: you can also downsample the data. However, this is much more rare. It should have a very similar affect but usually you don't want to throw data away!

```{r}
N <- length(pos_ind)*10
sample_ind <- sample(pos_ind, N, replace = TRUE)
```

```{r}
table(sample_ind)
```

This table shows the number of times we get each entry. The above number is the specific record and below it is the number of times.

So let's create a new dataframe. We want to grab the particular indices from the above dataframe multiple times. Something like this seems intuitive.

```{r}
new_df <- df[sample_ind,]
```

However there is a major problem!

```{r}
dim(new_df)
dim(df)
```

We lost a lot of cases! In fact, we threw away all of the negative cases. So instead, we need to stack the dataframes together.

```{r}
new_df <- rbind(df[sample_ind, ], df)
dim(new_df)
```

Note the features themselves haven't changed at all. Instead, the proportion of the features have changed. There we go. Also note we did not overwrite df, we created a new dataframe. This is good practice in functional programming. Now we can do things like linear regression!

```{r}
ols <- lm(steal_flag ~ trump + capitol.police, data = new_df)
summary(ols)
```

Regressing with trump and capitol police, it appears capitol police isn't significant but trump is. That is his slogan so it shouldn't be too surprising but there it is.

How to mix upsample and also cross validate can only be done one way. You *don't* want to upsample first! If you did that, you could end up with the same record in both the train and test. Then you would be too confident in those cases. 
