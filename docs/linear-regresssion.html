<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Linear Regresssion | Data Mining</title>
  <meta name="description" content="Chapter 3 Linear Regresssion | Data Mining" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Linear Regresssion | Data Mining" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Linear Regresssion | Data Mining" />
  
  
  

<meta name="author" content="Noah Love" />


<meta name="date" content="2021-03-05" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="tidyverse-tools.html"/>
<link rel="next" href="classification.html"/>
<script src="libs/header-attrs-2.6/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Data Mining</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> A Brief Introduction</a></li>
<li class="chapter" data-level="2" data-path="tidyverse-tools.html"><a href="tidyverse-tools.html"><i class="fa fa-check"></i><b>2</b> Tidyverse tools</a>
<ul>
<li class="chapter" data-level="2.1" data-path="tidyverse-tools.html"><a href="tidyverse-tools.html#select"><i class="fa fa-check"></i><b>2.1</b> Select</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="tidyverse-tools.html"><a href="tidyverse-tools.html#original-data-set"><i class="fa fa-check"></i><b>2.1.1</b> Original Data Set</a></li>
<li class="chapter" data-level="2.1.2" data-path="tidyverse-tools.html"><a href="tidyverse-tools.html#other-select-functions"><i class="fa fa-check"></i><b>2.1.2</b> Other select functions:</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="tidyverse-tools.html"><a href="tidyverse-tools.html#filter"><i class="fa fa-check"></i><b>2.2</b> Filter</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="tidyverse-tools.html"><a href="tidyverse-tools.html#logical-tests-in-r"><i class="fa fa-check"></i><b>2.2.1</b> Logical Tests in R</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="tidyverse-tools.html"><a href="tidyverse-tools.html#mutate"><i class="fa fa-check"></i><b>2.3</b> Mutate</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="tidyverse-tools.html"><a href="tidyverse-tools.html#useful-mutate-functions"><i class="fa fa-check"></i><b>2.3.1</b> Useful mutate functions:</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="tidyverse-tools.html"><a href="tidyverse-tools.html#summarise-summarize"><i class="fa fa-check"></i><b>2.4</b> Summarise / Summarize</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="tidyverse-tools.html"><a href="tidyverse-tools.html#useful-summary-functions"><i class="fa fa-check"></i><b>2.4.1</b> Useful summary functions</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="tidyverse-tools.html"><a href="tidyverse-tools.html#arrange"><i class="fa fa-check"></i><b>2.5</b> Arrange()</a></li>
<li class="chapter" data-level="2.6" data-path="tidyverse-tools.html"><a href="tidyverse-tools.html#group_by"><i class="fa fa-check"></i><b>2.6</b> Group_by()</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="linear-regresssion.html"><a href="linear-regresssion.html"><i class="fa fa-check"></i><b>3</b> Linear Regresssion</a>
<ul>
<li class="chapter" data-level="3.1" data-path="linear-regresssion.html"><a href="linear-regresssion.html#non-linear-data"><i class="fa fa-check"></i><b>3.1</b> Non linear data</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="linear-regresssion.html"><a href="linear-regresssion.html#interactions-subtracting-variables"><i class="fa fa-check"></i><b>3.1.1</b> Interactions + subtracting variables</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="linear-regresssion.html"><a href="linear-regresssion.html#last-trap-ording-of-the-data"><i class="fa fa-check"></i><b>3.2</b> Last trap, ording of the data</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="linear-regresssion.html"><a href="linear-regresssion.html#missing"><i class="fa fa-check"></i><b>3.2.1</b> Missing</a></li>
<li class="chapter" data-level="3.2.2" data-path="linear-regresssion.html"><a href="linear-regresssion.html#glm"><i class="fa fa-check"></i><b>3.2.2</b> GLM</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>4</b> Classification</a>
<ul>
<li class="chapter" data-level="4.1" data-path="classification.html"><a href="classification.html#music-dataset"><i class="fa fa-check"></i><b>4.1</b> Music dataset</a></li>
<li class="chapter" data-level="4.2" data-path="classification.html"><a href="classification.html#logistic-regression-review"><i class="fa fa-check"></i><b>4.2</b> Logistic Regression Review</a></li>
<li class="chapter" data-level="4.3" data-path="classification.html"><a href="classification.html#resampling"><i class="fa fa-check"></i><b>4.3</b> Resampling</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="classification.html"><a href="classification.html#up-sampling"><i class="fa fa-check"></i><b>4.3.1</b> Up-sampling</a></li>
<li class="chapter" data-level="4.3.2" data-path="classification.html"><a href="classification.html#bootstrapping"><i class="fa fa-check"></i><b>4.3.2</b> Bootstrapping</a></li>
<li class="chapter" data-level="4.3.3" data-path="classification.html"><a href="classification.html#cross-validation"><i class="fa fa-check"></i><b>4.3.3</b> Cross Validation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="linear-model-selection.html"><a href="linear-model-selection.html"><i class="fa fa-check"></i><b>5</b> Linear Model Selection</a>
<ul>
<li class="chapter" data-level="5.1" data-path="linear-model-selection.html"><a href="linear-model-selection.html#regression-requires-more-data-than-features"><i class="fa fa-check"></i><b>5.1</b> Regression requires more data than features</a></li>
<li class="chapter" data-level="5.2" data-path="linear-model-selection.html"><a href="linear-model-selection.html#simulating-collinearity-sparsity"><i class="fa fa-check"></i><b>5.2</b> simulating collinearity + sparsity</a></li>
<li class="chapter" data-level="5.3" data-path="linear-model-selection.html"><a href="linear-model-selection.html#demo-on-glmnet-functionalities"><i class="fa fa-check"></i><b>5.3</b> Demo on glmnet functionalities</a></li>
<li class="chapter" data-level="5.4" data-path="linear-model-selection.html"><a href="linear-model-selection.html#principal-component-analysis"><i class="fa fa-check"></i><b>5.4</b> Principal Component Analysis</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="linear-model-selection.html"><a href="linear-model-selection.html#principle-component-analysis"><i class="fa fa-check"></i><b>5.4.1</b> Principle Component Analysis</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="linear-model-selection.html"><a href="linear-model-selection.html#typical-machine-learning-approach"><i class="fa fa-check"></i><b>5.5</b> Typical machine learning approach</a></li>
<li class="chapter" data-level="5.6" data-path="linear-model-selection.html"><a href="linear-model-selection.html#what-we-would-do-in-data-mining"><i class="fa fa-check"></i><b>5.6</b> What we would do in data mining</a></li>
<li class="chapter" data-level="5.7" data-path="linear-model-selection.html"><a href="linear-model-selection.html#principal-component-analysis-applied"><i class="fa fa-check"></i><b>5.7</b> Principal Component Analysis Applied!</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="linear-model-selection.html"><a href="linear-model-selection.html#other-example"><i class="fa fa-check"></i><b>5.7.1</b> Other example</a></li>
<li class="chapter" data-level="5.7.2" data-path="linear-model-selection.html"><a href="linear-model-selection.html#explore-the-data"><i class="fa fa-check"></i><b>5.7.2</b> Explore the data</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="linear-model-selection.html"><a href="linear-model-selection.html#pca-on-weather-data-example"><i class="fa fa-check"></i><b>5.8</b> PCA on weather data example</a></li>
<li class="chapter" data-level="5.9" data-path="linear-model-selection.html"><a href="linear-model-selection.html#different-noramlizations"><i class="fa fa-check"></i><b>5.9</b> Different noramlizations</a>
<ul>
<li class="chapter" data-level="5.9.1" data-path="linear-model-selection.html"><a href="linear-model-selection.html#no-normalization"><i class="fa fa-check"></i><b>5.9.1</b> No normalization</a></li>
<li class="chapter" data-level="5.9.2" data-path="linear-model-selection.html"><a href="linear-model-selection.html#centering-and-scaling"><i class="fa fa-check"></i><b>5.9.2</b> Centering and scaling</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="comparing-different-models.html"><a href="comparing-different-models.html"><i class="fa fa-check"></i><b>6</b> Comparing different models</a>
<ul>
<li class="chapter" data-level="6.1" data-path="comparing-different-models.html"><a href="comparing-different-models.html#question-0"><i class="fa fa-check"></i><b>6.1</b> Question 0</a></li>
<li class="chapter" data-level="6.2" data-path="comparing-different-models.html"><a href="comparing-different-models.html#question-1"><i class="fa fa-check"></i><b>6.2</b> Question 1</a></li>
<li class="chapter" data-level="6.3" data-path="comparing-different-models.html"><a href="comparing-different-models.html#question-2"><i class="fa fa-check"></i><b>6.3</b> Question 2</a></li>
<li class="chapter" data-level="6.4" data-path="comparing-different-models.html"><a href="comparing-different-models.html#question-3"><i class="fa fa-check"></i><b>6.4</b> Question 3</a></li>
<li class="chapter" data-level="6.5" data-path="comparing-different-models.html"><a href="comparing-different-models.html#question-4"><i class="fa fa-check"></i><b>6.5</b> Question 4</a></li>
<li class="chapter" data-level="6.6" data-path="comparing-different-models.html"><a href="comparing-different-models.html#question-5"><i class="fa fa-check"></i><b>6.6</b> Question 5</a></li>
<li class="chapter" data-level="6.7" data-path="comparing-different-models.html"><a href="comparing-different-models.html#question-6"><i class="fa fa-check"></i><b>6.7</b> Question 6</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="homework-3-project.html"><a href="homework-3-project.html"><i class="fa fa-check"></i><b>7</b> Homework 3 Project</a>
<ul>
<li class="chapter" data-level="7.1" data-path="homework-3-project.html"><a href="homework-3-project.html#hw1"><i class="fa fa-check"></i><b>7.1</b> HW1</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="homework-3-project.html"><a href="homework-3-project.html#q1"><i class="fa fa-check"></i><b>7.1.1</b> Q1</a></li>
<li class="chapter" data-level="7.1.2" data-path="homework-3-project.html"><a href="homework-3-project.html#q2"><i class="fa fa-check"></i><b>7.1.2</b> Q2</a></li>
<li class="chapter" data-level="7.1.3" data-path="homework-3-project.html"><a href="homework-3-project.html#q3"><i class="fa fa-check"></i><b>7.1.3</b> Q3</a></li>
<li class="chapter" data-level="7.1.4" data-path="homework-3-project.html"><a href="homework-3-project.html#q4"><i class="fa fa-check"></i><b>7.1.4</b> Q4</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="homework-3-project.html"><a href="homework-3-project.html#r-markdown"><i class="fa fa-check"></i><b>7.2</b> R Markdown</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Mining</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear-regresssion" class="section level1" number="3">
<h1><span class="header-section-number">Chapter 3</span> Linear Regresssion</h1>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="linear-regresssion.html#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span></code></pre></div>
<pre><code>## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ──</code></pre>
<pre><code>## ✓ ggplot2 3.3.3     ✓ purrr   0.3.4
## ✓ tibble  3.0.6     ✓ dplyr   1.0.4
## ✓ tidyr   1.1.2     ✓ stringr 1.4.0
## ✓ readr   1.4.0     ✓ forcats 0.5.1</code></pre>
<pre><code>## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()</code></pre>
<p>We have a sample of 100, 5 different variables (p), and an X matrix. We are making uniform variables, and we will jam them into a matrix such that the number of rows is equal to n.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="linear-regresssion.html#cb5-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb5-2"><a href="linear-regresssion.html#cb5-2" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="dv">5</span></span>
<span id="cb5-3"><a href="linear-regresssion.html#cb5-3" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(p <span class="sc">*</span> n),</span>
<span id="cb5-4"><a href="linear-regresssion.html#cb5-4" aria-hidden="true" tabindex="-1"></a>            <span class="at">nrow=</span>n)</span></code></pre></div>
<p>Then we can create some Y’s based on betas and noise. To do so, we need betas. We will make all of them zeros, except for one of them! We also need to define our noise.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="linear-regresssion.html#cb6-1" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, p)</span>
<span id="cb6-2"><a href="linear-regresssion.html#cb6-2" aria-hidden="true" tabindex="-1"></a>beta[<span class="fu">sample</span>(p, <span class="dv">1</span>)] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb6-3"><a href="linear-regresssion.html#cb6-3" aria-hidden="true" tabindex="-1"></a>noise <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n)</span>
<span id="cb6-4"><a href="linear-regresssion.html#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="linear-regresssion.html#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="co"># %*% Matrix multiplication in R</span></span>
<span id="cb6-6"><a href="linear-regresssion.html#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="co">#X is a matrix of 100 by 5</span></span>
<span id="cb6-7"><a href="linear-regresssion.html#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co">#Beta is a matrix of 5</span></span>
<span id="cb6-8"><a href="linear-regresssion.html#cb6-8" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> X <span class="sc">%*%</span> beta <span class="sc">+</span> noise</span></code></pre></div>
<p>The above is generating data according to a linear model! So what does this do? We have X and a bunch of 0 betas, except for one special one. The Y then will depend only on the one column!</p>
<p>If we don’t know which beta matters, we just fit y against x and print out a summary.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="linear-regresssion.html#cb7-1" aria-hidden="true" tabindex="-1"></a>ols <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> X)</span>
<span id="cb7-2"><a href="linear-regresssion.html#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(ols)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ X)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.98594 -0.67326  0.05564  0.64407  2.06935 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept) -0.03983    0.38385  -0.104  0.91758   
## X1           1.07992    0.32427   3.330  0.00124 **
## X2          -0.29117    0.32254  -0.903  0.36896   
## X3           0.52511    0.31626   1.660  0.10017   
## X4          -0.22179    0.31998  -0.693  0.48993   
## X5           0.26733    0.30940   0.864  0.38976   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.8905 on 94 degrees of freedom
## Multiple R-squared:  0.1551, Adjusted R-squared:  0.1101 
## F-statistic:  3.45 on 5 and 94 DF,  p-value: 0.006602</code></pre>
<p>So only one of them is statistically significant! This shows it should be the 5th! We can see the truth:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="linear-regresssion.html#cb9-1" aria-hidden="true" tabindex="-1"></a>beta</span></code></pre></div>
<pre><code>## [1] 1 0 0 0 0</code></pre>
<p>This is a good example of fitting basic regression.</p>
<p>Lets look at the y and subtract the the coefficients to find the residuals: note: length(ols$coefficients) is 6, but dimensions of X is going to be 5.So we will need to cbind. So this is y minus the fitted data from the regression. When we do summary of ols, we get a bunch of estimates. The intercept and the estimated coefficients. These are estimates from the regression based on the data. Using these, we can recover and estimate and then find the difference to see what the residuals are!</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="linear-regresssion.html#cb11-1" aria-hidden="true" tabindex="-1"></a>resid <span class="ot">&lt;-</span>  Y <span class="sc">-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, X) <span class="sc">%*%</span> ols<span class="sc">$</span>coefficients</span>
<span id="cb11-2"><a href="linear-regresssion.html#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="co">#this is the manual version of:</span></span>
<span id="cb11-3"><a href="linear-regresssion.html#cb11-3" aria-hidden="true" tabindex="-1"></a>resid <span class="ot">&lt;-</span>  ols<span class="sc">$</span>residuals</span></code></pre></div>
<p>Now let’s do the residuals against the true beta! We are subtracting the x values against the true beta. True beta means this is the beta that created the data. This is how god created the world. It is the actual physical value. The above residuals are from the regression of the fitted data.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="linear-regresssion.html#cb12-1" aria-hidden="true" tabindex="-1"></a>resid_from_truth <span class="ot">&lt;-</span> Y <span class="sc">-</span> X <span class="sc">%*%</span> beta</span></code></pre></div>
<p>We can plot this as well!</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="linear-regresssion.html#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">density</span>(resid_from_truth))</span>
<span id="cb13-2"><a href="linear-regresssion.html#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="fu">density</span>(resid), <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb13-3"><a href="linear-regresssion.html#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> <span class="dv">0</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>This might not be enough contrast for us to tell. How can we quantify the difference? Let’s see the difference between the two:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="linear-regresssion.html#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(<span class="fu">abs</span>(resid))</span></code></pre></div>
<pre><code>## [1] 0.7062042</code></pre>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="linear-regresssion.html#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(<span class="fu">abs</span>(resid_from_truth))</span></code></pre></div>
<pre><code>## [1] 0.744579</code></pre>
<p>We want the smaller value! The simulated data based values will always have a smaller residual mean than the real values! This is rather disturbing. The fitted coefficients from your regression will always be better than the “true” coefficients. Why is this bad?</p>
<p>Normally we want to use regression to find the natural coefficients or natural facts about the world. These are based on some “truth”. If you can collect noisy data but our algo prefers the fitted data rather than the truth we have a problem. We want our error minimized at the “truth”. Our regression here doesn’t like the true answer and it prefers something else. It actually prefers the training data.</p>
<p>This is because we use the same data to train and evaluate. Our data is just optimized for this one thing. If we know how you are going to evaluate me, I will just optimize for that specific thing.</p>
<p>So let’s generate another set of data: new Y using the same beta and x but with new noise values: Testing data generated from the same population but not used for training the model</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="linear-regresssion.html#cb18-1" aria-hidden="true" tabindex="-1"></a>new_noise <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n)</span>
<span id="cb18-2"><a href="linear-regresssion.html#cb18-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-3"><a href="linear-regresssion.html#cb18-3" aria-hidden="true" tabindex="-1"></a>new_Y <span class="ot">&lt;-</span> X <span class="sc">%*%</span> beta <span class="sc">+</span> new_noise</span>
<span id="cb18-4"><a href="linear-regresssion.html#cb18-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-5"><a href="linear-regresssion.html#cb18-5" aria-hidden="true" tabindex="-1"></a>new_resid <span class="ot">&lt;-</span> new_Y <span class="sc">-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, X) <span class="sc">%*%</span> ols<span class="sc">$</span>coefficients</span>
<span id="cb18-6"><a href="linear-regresssion.html#cb18-6" aria-hidden="true" tabindex="-1"></a>new_resid_from_truth <span class="ot">&lt;-</span>  new_Y <span class="sc">-</span> X <span class="sc">%*%</span> beta</span>
<span id="cb18-7"><a href="linear-regresssion.html#cb18-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-8"><a href="linear-regresssion.html#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(<span class="fu">abs</span>(new_resid))</span></code></pre></div>
<pre><code>## [1] 0.8249776</code></pre>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="linear-regresssion.html#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(<span class="fu">abs</span>(new_resid_from_truth))</span></code></pre></div>
<pre><code>## [1] 0.7946041</code></pre>
<p>Our takeaway is don’t always cross over your data because you risk overfitting.</p>
<div id="non-linear-data" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> Non linear data</h2>
<p>Let’s create new data that is nonlinear. This is mini sine data.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="linear-regresssion.html#cb22-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb22-2"><a href="linear-regresssion.html#cb22-2" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">100</span>, <span class="at">max =</span> <span class="dv">2</span> <span class="sc">*</span> pi <span class="sc">/</span> <span class="dv">4</span> <span class="sc">*</span> <span class="dv">3</span>)</span>
<span id="cb22-3"><a href="linear-regresssion.html#cb22-3" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fl">0.1</span><span class="sc">+-</span><span class="dv">3</span> <span class="sc">*</span> <span class="fu">sin</span>(X) <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="at">sd =</span> <span class="fl">0.5</span>)</span></code></pre></div>
<p>Then we can recreate our linear regression and the plot.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="linear-regresssion.html#cb23-1" aria-hidden="true" tabindex="-1"></a>ols <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> X)</span>
<span id="cb23-2"><a href="linear-regresssion.html#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(ols)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ X)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.7254 -0.9476 -0.3520  1.1400  2.9925 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  -2.9718     0.2590 -11.472  &lt; 2e-16 ***
## X             0.9877     0.1039   9.509 1.41e-15 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.33 on 98 degrees of freedom
## Multiple R-squared:  0.4799, Adjusted R-squared:  0.4746 
## F-statistic: 90.43 on 1 and 98 DF,  p-value: 1.405e-15</code></pre>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="linear-regresssion.html#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(X, Y)</span>
<span id="cb25-2"><a href="linear-regresssion.html#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span>(Y <span class="sc">~</span> X))</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>As expected our linear model created a line that doesn’t look to fit too well. Let’s look at the residual plot instead. We</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="linear-regresssion.html#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb26-2"><a href="linear-regresssion.html#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(ols<span class="sc">$</span>residuals)</span>
<span id="cb26-3"><a href="linear-regresssion.html#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="dv">0</span>)</span>
<span id="cb26-4"><a href="linear-regresssion.html#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(X, ols<span class="sc">$</span>residuals) </span>
<span id="cb26-5"><a href="linear-regresssion.html#cb26-5" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="dv">0</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>Xs were generated in a random order so we we just use the index, it looks random and normal. But when we sort by x value, we see that there is definitely a problem and we need to add a quadratic term.</p>
<p>We can then find the mean squared error:</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="linear-regresssion.html#cb27-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb27-2"><a href="linear-regresssion.html#cb27-2" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fl">0.1</span><span class="sc">+-</span><span class="dv">3</span> <span class="sc">*</span> <span class="fu">sin</span>(X) <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="at">sd =</span> <span class="fl">0.5</span>)</span>
<span id="cb27-3"><a href="linear-regresssion.html#cb27-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-4"><a href="linear-regresssion.html#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="co">#Y - y hat squared</span></span>
<span id="cb27-5"><a href="linear-regresssion.html#cb27-5" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((Y <span class="sc">-</span> ols<span class="sc">$</span>fitted.values) <span class="sc">^</span> <span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 1.812336</code></pre>
<p><strong>Plot the MSE vs the number of polynomials used on the x axis. Do this for both the training vs the testing data.</strong></p>
<p>We can regress on matrices in R which makes this really easy. So let us create an X_matrix, where for each degree, we raise x to that number of degrees. This matrix will be n rows and degrees columns.</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="linear-regresssion.html#cb29-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb29-2"><a href="linear-regresssion.html#cb29-2" aria-hidden="true" tabindex="-1"></a>degrees <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">50</span></span>
<span id="cb29-3"><a href="linear-regresssion.html#cb29-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-4"><a href="linear-regresssion.html#cb29-4" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="at">max =</span> <span class="dv">2</span> <span class="sc">*</span> pi <span class="sc">/</span> <span class="dv">4</span> <span class="sc">*</span> <span class="dv">3</span>)</span>
<span id="cb29-5"><a href="linear-regresssion.html#cb29-5" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fl">0.1</span><span class="sc">+-</span><span class="dv">3</span> <span class="sc">*</span> <span class="fu">sin</span>(X) <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="at">sd =</span> <span class="fl">0.5</span>)</span>
<span id="cb29-6"><a href="linear-regresssion.html#cb29-6" aria-hidden="true" tabindex="-1"></a>new_Y <span class="ot">&lt;-</span> <span class="fl">0.1</span><span class="sc">+-</span><span class="dv">3</span> <span class="sc">*</span> <span class="fu">sin</span>(X) <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="at">sd =</span> <span class="fl">0.5</span>)</span>
<span id="cb29-7"><a href="linear-regresssion.html#cb29-7" aria-hidden="true" tabindex="-1"></a>X_mat <span class="ot">&lt;-</span> <span class="fu">sapply</span>(degrees, <span class="cf">function</span>(i)</span>
<span id="cb29-8"><a href="linear-regresssion.html#cb29-8" aria-hidden="true" tabindex="-1"></a>  X <span class="sc">^</span> i)</span></code></pre></div>
<p>For example</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="linear-regresssion.html#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(X, X_mat[,<span class="dv">5</span>])</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>We can do this through a loop.</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="linear-regresssion.html#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co">#create an empty vector</span></span>
<span id="cb31-2"><a href="linear-regresssion.html#cb31-2" aria-hidden="true" tabindex="-1"></a>MSEs <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, <span class="fu">length</span>(degrees))</span>
<span id="cb31-3"><a href="linear-regresssion.html#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="co">#Create empty vector for tests</span></span>
<span id="cb31-4"><a href="linear-regresssion.html#cb31-4" aria-hidden="true" tabindex="-1"></a>test_MSEs <span class="ot">&lt;-</span> MSEs</span>
<span id="cb31-5"><a href="linear-regresssion.html#cb31-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-6"><a href="linear-regresssion.html#cb31-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="fu">seq_along</span>(degrees)) {</span>
<span id="cb31-7"><a href="linear-regresssion.html#cb31-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># regress for each power on each loop</span></span>
<span id="cb31-8"><a href="linear-regresssion.html#cb31-8" aria-hidden="true" tabindex="-1"></a>  ols <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> X_mat[, <span class="dv">1</span><span class="sc">:</span>i])</span>
<span id="cb31-9"><a href="linear-regresssion.html#cb31-9" aria-hidden="true" tabindex="-1"></a>  <span class="co"># record the MSEs</span></span>
<span id="cb31-10"><a href="linear-regresssion.html#cb31-10" aria-hidden="true" tabindex="-1"></a>  MSEs[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>(ols<span class="sc">$</span>residuals <span class="sc">^</span> <span class="dv">2</span>)</span>
<span id="cb31-11"><a href="linear-regresssion.html#cb31-11" aria-hidden="true" tabindex="-1"></a>  <span class="co"># do again for new set, only word because we use the same X</span></span>
<span id="cb31-12"><a href="linear-regresssion.html#cb31-12" aria-hidden="true" tabindex="-1"></a>  new_errors <span class="ot">&lt;-</span> new_Y <span class="sc">-</span> ols<span class="sc">$</span>fitted.values</span>
<span id="cb31-13"><a href="linear-regresssion.html#cb31-13" aria-hidden="true" tabindex="-1"></a>  <span class="co"># record</span></span>
<span id="cb31-14"><a href="linear-regresssion.html#cb31-14" aria-hidden="true" tabindex="-1"></a>  test_MSEs[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>(new_errors <span class="sc">^</span> <span class="dv">2</span>)</span>
<span id="cb31-15"><a href="linear-regresssion.html#cb31-15" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>Plot in base R</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="linear-regresssion.html#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(degrees, MSEs, <span class="at">type =</span> <span class="st">&quot;b&quot;</span>,</span>
<span id="cb32-2"><a href="linear-regresssion.html#cb32-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fu">max</span>(test_MSEs)))</span>
<span id="cb32-3"><a href="linear-regresssion.html#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(degrees, test_MSEs, <span class="at">type =</span> <span class="st">&quot;b&quot;</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb32-4"><a href="linear-regresssion.html#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topright&quot;</span>,</span>
<span id="cb32-5"><a href="linear-regresssion.html#cb32-5" aria-hidden="true" tabindex="-1"></a>       <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Test&quot;</span>, <span class="st">&quot;Train&quot;</span>),</span>
<span id="cb32-6"><a href="linear-regresssion.html#cb32-6" aria-hidden="true" tabindex="-1"></a>       <span class="at">fill =</span> <span class="fu">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;black&quot;</span>))</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>Plot in tidyverse</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="linear-regresssion.html#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(ols)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ X_mat[, 1:i])
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.19658 -0.38102  0.03311  0.28621  1.15494 
## 
## Coefficients: (28 not defined because of singularities)
##                  Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)    -2.356e+00  3.663e+00  -0.643    0.522
## X_mat[, 1:i]1   1.693e+02  1.978e+02   0.856    0.395
## X_mat[, 1:i]2  -2.741e+03  2.872e+03  -0.954    0.343
## X_mat[, 1:i]3   1.988e+04  1.975e+04   1.007    0.317
## X_mat[, 1:i]4  -8.187e+04  7.825e+04  -1.046    0.299
## X_mat[, 1:i]5   2.131e+05  1.976e+05   1.079    0.284
## X_mat[, 1:i]6  -3.734e+05  3.373e+05  -1.107    0.272
## X_mat[, 1:i]7   4.564e+05  4.034e+05   1.132    0.261
## X_mat[, 1:i]8  -3.966e+05  3.439e+05  -1.153    0.252
## X_mat[, 1:i]9   2.452e+05  2.091e+05   1.173    0.244
## X_mat[, 1:i]10 -1.057e+05  8.879e+04  -1.190    0.238
## X_mat[, 1:i]11  2.979e+04  2.472e+04   1.205    0.232
## X_mat[, 1:i]12 -4.504e+03  3.696e+03  -1.219    0.227
## X_mat[, 1:i]13         NA         NA      NA       NA
## X_mat[, 1:i]14  9.917e+01  7.997e+01   1.240    0.219
## X_mat[, 1:i]15         NA         NA      NA       NA
## X_mat[, 1:i]16 -3.438e+00  2.740e+00  -1.255    0.213
## X_mat[, 1:i]17         NA         NA      NA       NA
## X_mat[, 1:i]18  9.420e-02  7.450e-02   1.264    0.210
## X_mat[, 1:i]19         NA         NA      NA       NA
## X_mat[, 1:i]20         NA         NA      NA       NA
## X_mat[, 1:i]21 -4.239e-04  3.340e-04  -1.269    0.208
## X_mat[, 1:i]22         NA         NA      NA       NA
## X_mat[, 1:i]23         NA         NA      NA       NA
## X_mat[, 1:i]24  2.308e-06  1.825e-06   1.265    0.210
## X_mat[, 1:i]25         NA         NA      NA       NA
## X_mat[, 1:i]26         NA         NA      NA       NA
## X_mat[, 1:i]27 -9.535e-09  7.613e-09  -1.252    0.214
## X_mat[, 1:i]28         NA         NA      NA       NA
## X_mat[, 1:i]29         NA         NA      NA       NA
## X_mat[, 1:i]30         NA         NA      NA       NA
## X_mat[, 1:i]31  4.865e-12  3.965e-12   1.227    0.224
## X_mat[, 1:i]32         NA         NA      NA       NA
## X_mat[, 1:i]33         NA         NA      NA       NA
## X_mat[, 1:i]34         NA         NA      NA       NA
## X_mat[, 1:i]35         NA         NA      NA       NA
## X_mat[, 1:i]36 -3.924e-16  3.315e-16  -1.184    0.240
## X_mat[, 1:i]37         NA         NA      NA       NA
## X_mat[, 1:i]38         NA         NA      NA       NA
## X_mat[, 1:i]39         NA         NA      NA       NA
## X_mat[, 1:i]40         NA         NA      NA       NA
## X_mat[, 1:i]41         NA         NA      NA       NA
## X_mat[, 1:i]42  5.047e-21  4.505e-21   1.120    0.266
## X_mat[, 1:i]43         NA         NA      NA       NA
## X_mat[, 1:i]44         NA         NA      NA       NA
## X_mat[, 1:i]45         NA         NA      NA       NA
## X_mat[, 1:i]46         NA         NA      NA       NA
## X_mat[, 1:i]47         NA         NA      NA       NA
## X_mat[, 1:i]48 -4.413e-26  4.209e-26  -1.049    0.298
## X_mat[, 1:i]49         NA         NA      NA       NA
## X_mat[, 1:i]50         NA         NA      NA       NA
## 
## Residual standard error: 0.5185 on 77 degrees of freedom
## Multiple R-squared:  0.9546, Adjusted R-squared:  0.9416 
## F-statistic: 73.51 on 22 and 77 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>This should be really concerning. It would mean strong colinearity. R is doing us a favor and automatically dropping some redundant features.</p>
<p>Create data under the so called regression model. Regression is an algo, but it is also a model for creating data. ODS is a least square algoristhm.</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="linear-regresssion.html#cb35-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb35-2"><a href="linear-regresssion.html#cb35-2" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="dv">5</span></span>
<span id="cb35-3"><a href="linear-regresssion.html#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="co"># God knows this!</span></span>
<span id="cb35-4"><a href="linear-regresssion.html#cb35-4" aria-hidden="true" tabindex="-1"></a>params <span class="ot">&lt;-</span> <span class="fu">runif</span>(p, <span class="sc">-</span><span class="dv">10</span>, <span class="dv">10</span>)</span>
<span id="cb35-5"><a href="linear-regresssion.html#cb35-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-6"><a href="linear-regresssion.html#cb35-6" aria-hidden="true" tabindex="-1"></a>features <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(n <span class="sc">*</span> p),</span>
<span id="cb35-7"><a href="linear-regresssion.html#cb35-7" aria-hidden="true" tabindex="-1"></a>                   <span class="at">nrow=</span>n, <span class="at">ncol=</span>p)</span>
<span id="cb35-8"><a href="linear-regresssion.html#cb35-8" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> features</span>
<span id="cb35-9"><a href="linear-regresssion.html#cb35-9" aria-hidden="true" tabindex="-1"></a>X1 <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(n <span class="sc">*</span> p),</span>
<span id="cb35-10"><a href="linear-regresssion.html#cb35-10" aria-hidden="true" tabindex="-1"></a>              <span class="at">nrow=</span>n, <span class="at">ncol=</span>p)</span>
<span id="cb35-11"><a href="linear-regresssion.html#cb35-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-12"><a href="linear-regresssion.html#cb35-12" aria-hidden="true" tabindex="-1"></a>noise <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n)</span>
<span id="cb35-13"><a href="linear-regresssion.html#cb35-13" aria-hidden="true" tabindex="-1"></a>noise1 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n)</span>
<span id="cb35-14"><a href="linear-regresssion.html#cb35-14" aria-hidden="true" tabindex="-1"></a>Y1 <span class="ot">&lt;-</span> X <span class="sc">%*%</span> params <span class="sc">+</span> noise</span>
<span id="cb35-15"><a href="linear-regresssion.html#cb35-15" aria-hidden="true" tabindex="-1"></a>Y2 <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb35-16"><a href="linear-regresssion.html#cb35-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="fu">seq_len</span>(p)){</span>
<span id="cb35-17"><a href="linear-regresssion.html#cb35-17" aria-hidden="true" tabindex="-1"></a>  Y2 <span class="ot">&lt;-</span> Y2 <span class="sc">+</span> params[i] <span class="sc">*</span> X1[, i]</span>
<span id="cb35-18"><a href="linear-regresssion.html#cb35-18" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb35-19"><a href="linear-regresssion.html#cb35-19" aria-hidden="true" tabindex="-1"></a>Y2 <span class="ot">&lt;-</span> Y2 <span class="sc">+</span> noise1</span>
<span id="cb35-20"><a href="linear-regresssion.html#cb35-20" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(X[,<span class="dv">1</span>], Y1)</span>
<span id="cb35-21"><a href="linear-regresssion.html#cb35-21" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(X1[, <span class="dv">1</span>], Y2, <span class="at">col=</span><span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="linear-regresssion.html#cb36-1" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(<span class="fu">cbind</span>(Y,X))</span>
<span id="cb36-2"><a href="linear-regresssion.html#cb36-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-3"><a href="linear-regresssion.html#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(df)</span></code></pre></div>
<pre><code>## [1] 100   6</code></pre>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="linear-regresssion.html#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(df,<span class="dv">2</span>)</span></code></pre></div>
<pre><code>##          Y         V2         V3         V4         V5        V6
## 1 -3.99537 -0.4858853 -0.5873763 -0.7844641 -2.7058657  1.205373
## 2 -3.48547  0.3690574 -0.4735389  1.0506131  0.1672846 -0.557476</code></pre>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="linear-regresssion.html#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(df) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Y&quot;</span>, <span class="fu">paste0</span>(<span class="st">&quot;X&quot;</span>, <span class="dv">1</span><span class="sc">:</span>p))</span>
<span id="cb40-2"><a href="linear-regresssion.html#cb40-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-3"><a href="linear-regresssion.html#cb40-3" aria-hidden="true" tabindex="-1"></a>ols <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> ., df)</span>
<span id="cb40-4"><a href="linear-regresssion.html#cb40-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(ols)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ ., data = df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.8526 -1.4350 -0.5161  1.5859  4.6446 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)  
## (Intercept)  -0.5234     0.2192  -2.388   0.0189 *
## X1            0.1570     0.2147   0.731   0.4665  
## X2           -0.3258     0.2052  -1.588   0.1158  
## X3           -0.1442     0.2028  -0.711   0.4789  
## X4            0.4843     0.2039   2.376   0.0195 *
## X5           -0.3564     0.2214  -1.610   0.1108  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.095 on 94 degrees of freedom
## Multiple R-squared:  0.09452,    Adjusted R-squared:  0.04635 
## F-statistic: 1.962 on 5 and 94 DF,  p-value: 0.09139</code></pre>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="linear-regresssion.html#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="fu">class</span>(ols)</span></code></pre></div>
<pre><code>## [1] &quot;lm&quot;</code></pre>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="linear-regresssion.html#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(ols)</span></code></pre></div>
<pre><code>##           1           2           3           4           5           6 
## -2.03523648 -0.18294221 -1.46358980 -1.65085658 -1.04029745 -0.77761486 
##           7           8           9          10          11          12 
## -0.78792248 -0.07398802  0.30804699  0.24291643 -1.33488427 -0.01642422 
##          13          14          15          16          17          18 
## -0.93915160 -0.15066757 -0.94359918  0.89266782 -1.61379010 -0.66984388 
##          19          20          21          22          23          24 
## -0.43120650 -0.75901154 -0.36441423 -1.24636740 -0.06481710 -1.71616739 
##          25          26          27          28          29          30 
##  0.10885537 -0.61796643 -1.29099775  0.57069354 -0.56359134 -0.78449874 
##          31          32          33          34          35          36 
##  0.80963416  0.35523045  0.04525086 -0.61684416 -1.76282557 -0.44688237 
##          37          38          39          40          41          42 
## -0.75519923 -0.57303324 -0.41658712 -1.32172155 -0.84848116 -0.75512042 
##          43          44          45          46          47          48 
##  0.20223711 -0.18266803 -0.24114676 -0.47844964  0.30403804 -1.26595079 
##          49          50          51          52          53          54 
## -0.29308310 -1.13981635 -0.50192225 -1.11516277 -0.95169952 -1.31653325 
##          55          56          57          58          59          60 
## -0.06256561 -0.39958896 -0.69367261 -0.70071502 -0.76898893 -0.91084578 
##          61          62          63          64          65          66 
## -1.55280287 -0.55185651  0.39845490  0.14976143 -0.38076391 -0.43235326 
##          67          68          69          70          71          72 
## -1.26707178 -0.24272956 -0.43306373 -1.74545866 -0.34235903 -1.19119903 
##          73          74          75          76          77          78 
## -0.35608283  0.08757120 -0.12821228 -0.43874868  0.41933537 -0.41046665 
##          79          80          81          82          83          84 
## -1.10557050 -0.87477843 -0.94850634 -0.87814556 -1.42022057 -0.26339642 
##          85          86          87          88          89          90 
## -1.65267708  0.05355826  0.02671337 -2.62196068 -0.34564823  0.53312529 
##          91          92          93          94          95          96 
## -0.50079887 -1.28935509 -2.03348658 -0.66589458 -0.54966533 -0.39267202 
##          97          98          99         100 
## -0.79074612 -0.68465754 -0.32901811 -1.29527017</code></pre>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="linear-regresssion.html#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co">#if you pass in a lm or glm, predict will use predict.lm or predict.glm anyways. It is smart. </span></span>
<span id="cb46-2"><a href="linear-regresssion.html#cb46-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-3"><a href="linear-regresssion.html#cb46-3" aria-hidden="true" tabindex="-1"></a><span class="co">#There is an argument in predict that uses &quot;new data&quot;. You need to pass in what the new data is. It should be tempting for us to just pass in X1. </span></span>
<span id="cb46-4"><a href="linear-regresssion.html#cb46-4" aria-hidden="true" tabindex="-1"></a><span class="co">#This shouldn&#39;t work! But why?</span></span>
<span id="cb46-5"><a href="linear-regresssion.html#cb46-5" aria-hidden="true" tabindex="-1"></a><span class="co">#predict(ols, newdata = as.data.frame(X1))</span></span>
<span id="cb46-6"><a href="linear-regresssion.html#cb46-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-7"><a href="linear-regresssion.html#cb46-7" aria-hidden="true" tabindex="-1"></a><span class="co">#convert to df</span></span>
<span id="cb46-8"><a href="linear-regresssion.html#cb46-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-9"><a href="linear-regresssion.html#cb46-9" aria-hidden="true" tabindex="-1"></a><span class="co">#now there is an error that we don&#39;t know what X2 is. The data frame you are passing it needs to have the same names that you are training on. </span></span>
<span id="cb46-10"><a href="linear-regresssion.html#cb46-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-11"><a href="linear-regresssion.html#cb46-11" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(df)</span></code></pre></div>
<pre><code>## [1] &quot;Y&quot;  &quot;X1&quot; &quot;X2&quot; &quot;X3&quot; &quot;X4&quot; &quot;X5&quot;</code></pre>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="linear-regresssion.html#cb48-1" aria-hidden="true" tabindex="-1"></a>df1  <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(X1)</span>
<span id="cb48-2"><a href="linear-regresssion.html#cb48-2" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(df1) <span class="ot">&lt;-</span> <span class="fu">names</span>(df)[<span class="sc">-</span><span class="dv">1</span>]</span>
<span id="cb48-3"><a href="linear-regresssion.html#cb48-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-4"><a href="linear-regresssion.html#cb48-4" aria-hidden="true" tabindex="-1"></a>test_preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(</span>
<span id="cb48-5"><a href="linear-regresssion.html#cb48-5" aria-hidden="true" tabindex="-1"></a>  ols, </span>
<span id="cb48-6"><a href="linear-regresssion.html#cb48-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">newdata =</span> df1)</span>
<span id="cb48-7"><a href="linear-regresssion.html#cb48-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-8"><a href="linear-regresssion.html#cb48-8" aria-hidden="true" tabindex="-1"></a><span class="co">#The data you are passing, you need the data to look identical to the data you trained the model with. The names of the data frames must agree. </span></span>
<span id="cb48-9"><a href="linear-regresssion.html#cb48-9" aria-hidden="true" tabindex="-1"></a><span class="co">#classic workflow</span></span>
<span id="cb48-10"><a href="linear-regresssion.html#cb48-10" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(test_preds , Y2)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="linear-regresssion.html#cb49-1" aria-hidden="true" tabindex="-1"></a>test_errors <span class="ot">&lt;-</span> Y2 <span class="sc">-</span> test_preds</span>
<span id="cb49-2"><a href="linear-regresssion.html#cb49-2" aria-hidden="true" tabindex="-1"></a>test_rmse <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>(test_errors <span class="sc">^</span> <span class="dv">2</span>))</span>
<span id="cb49-3"><a href="linear-regresssion.html#cb49-3" aria-hidden="true" tabindex="-1"></a>test_rmse</span></code></pre></div>
<pre><code>## [1] 12.7242</code></pre>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="linear-regresssion.html#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sd</span>(noise)</span></code></pre></div>
<pre><code>## [1] 1.011156</code></pre>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="linear-regresssion.html#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sd</span>(noise1)</span></code></pre></div>
<pre><code>## [1] 1.054824</code></pre>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="linear-regresssion.html#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="co">#You cannot reduce beyond this. </span></span></code></pre></div>
<p>R will automatically throw out extremely high colinearity instances. In the real world this would be rare. This is unique to R.</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="linear-regresssion.html#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(ols)</span></code></pre></div>
<pre><code>##  [1] &quot;coefficients&quot;  &quot;residuals&quot;     &quot;effects&quot;       &quot;rank&quot;         
##  [5] &quot;fitted.values&quot; &quot;assign&quot;        &quot;qr&quot;            &quot;df.residual&quot;  
##  [9] &quot;xlevels&quot;       &quot;call&quot;          &quot;terms&quot;         &quot;model&quot;</code></pre>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="linear-regresssion.html#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="co">#probably the most important</span></span>
<span id="cb58-2"><a href="linear-regresssion.html#cb58-2" aria-hidden="true" tabindex="-1"></a>ols<span class="sc">$</span>coefficients</span></code></pre></div>
<pre><code>## (Intercept)          X1          X2          X3          X4          X5 
##  -0.5233605   0.1569933  -0.3257545  -0.1441812   0.4842982  -0.3563999</code></pre>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="linear-regresssion.html#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(ols<span class="sc">$</span>coefficients, <span class="fu">c</span>(<span class="dv">0</span>,params))</span>
<span id="cb60-2"><a href="linear-regresssion.html#cb60-2" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a =</span> <span class="dv">0</span>, <span class="at">b=</span><span class="dv">1</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="linear-regresssion.html#cb61-1" aria-hidden="true" tabindex="-1"></a>train_features <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>,<span class="fu">as.matrix</span>(df[,<span class="sc">-</span><span class="dv">1</span>])) <span class="co">#take out y column</span></span>
<span id="cb61-2"><a href="linear-regresssion.html#cb61-2" aria-hidden="true" tabindex="-1"></a><span class="co">#fitted_vals &lt;- train_features %*% </span></span>
<span id="cb61-3"><a href="linear-regresssion.html#cb61-3" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb61-4"><a href="linear-regresssion.html#cb61-4" aria-hidden="true" tabindex="-1"></a>fitted_vals <span class="ot">&lt;-</span> train_features <span class="sc">%*%</span> ols<span class="sc">$</span>coefficients</span>
<span id="cb61-5"><a href="linear-regresssion.html#cb61-5" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">abs</span>(fitted_vals <span class="sc">-</span> ols<span class="sc">$</span>fitted.values))</span></code></pre></div>
<pre><code>## [1] 4.735101e-14</code></pre>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="linear-regresssion.html#cb63-1" aria-hidden="true" tabindex="-1"></a>res <span class="ot">&lt;-</span> df<span class="sc">$</span>Y <span class="sc">-</span> ols<span class="sc">$</span>fitted.values</span>
<span id="cb63-2"><a href="linear-regresssion.html#cb63-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">abs</span>(res <span class="sc">-</span> ols<span class="sc">$</span>residuals))</span></code></pre></div>
<pre><code>## [1] 8.881784e-16</code></pre>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="linear-regresssion.html#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(ols<span class="sc">$</span>residuals)</span>
<span id="cb65-2"><a href="linear-regresssion.html#cb65-2" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="dv">0</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="linear-regresssion.html#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="co">#you can also put plot onto the regression function itself</span></span>
<span id="cb66-2"><a href="linear-regresssion.html#cb66-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(ols)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-25-1.png" width="672" /><img src="bookdownproj_files/figure-html/unnamed-chunk-25-2.png" width="672" /><img src="bookdownproj_files/figure-html/unnamed-chunk-25-3.png" width="672" /><img src="bookdownproj_files/figure-html/unnamed-chunk-25-4.png" width="672" /></p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="linear-regresssion.html#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="co">#residuals vs fitted values. This is what we saw earlier but much fancier</span></span>
<span id="cb67-2"><a href="linear-regresssion.html#cb67-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-3"><a href="linear-regresssion.html#cb67-3" aria-hidden="true" tabindex="-1"></a><span class="co">#QQ Plot</span></span>
<span id="cb67-4"><a href="linear-regresssion.html#cb67-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-5"><a href="linear-regresssion.html#cb67-5" aria-hidden="true" tabindex="-1"></a><span class="co">#scale location not that important</span></span>
<span id="cb67-6"><a href="linear-regresssion.html#cb67-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-7"><a href="linear-regresssion.html#cb67-7" aria-hidden="true" tabindex="-1"></a><span class="co">#leverage to look for outliers</span></span></code></pre></div>
<div id="traps" class="section level4" number="3.1.0.1">
<h4><span class="header-section-number">3.1.0.1</span> Traps</h4>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="linear-regresssion.html#cb68-1" aria-hidden="true" tabindex="-1"></a>df_missing <span class="ot">&lt;-</span> df</span>
<span id="cb68-2"><a href="linear-regresssion.html#cb68-2" aria-hidden="true" tabindex="-1"></a>df_missing[<span class="dv">20</span>, <span class="st">&quot;Y&quot;</span>]  <span class="ot">&lt;-</span> <span class="cn">NA</span> <span class="co">#purposeffully lose the value</span></span>
<span id="cb68-3"><a href="linear-regresssion.html#cb68-3" aria-hidden="true" tabindex="-1"></a>ols <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span>., df_missing)</span>
<span id="cb68-4"><a href="linear-regresssion.html#cb68-4" aria-hidden="true" tabindex="-1"></a><span class="fu">length</span>(ols<span class="sc">$</span>residuals)</span></code></pre></div>
<pre><code>## [1] 99</code></pre>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="linear-regresssion.html#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="co">#lm drops missing value before matrix multiplication. So the residuals will change. </span></span></code></pre></div>
</div>
<div id="interactions-subtracting-variables" class="section level3" number="3.1.1">
<h3><span class="header-section-number">3.1.1</span> Interactions + subtracting variables</h3>
<div id="another-trap" class="section level4" number="3.1.1.1">
<h4><span class="header-section-number">3.1.1.1</span> Another trap</h4>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="linear-regresssion.html#cb71-1" aria-hidden="true" tabindex="-1"></a>ols <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> X1 <span class="sc">+</span> X2 <span class="sc">+</span> X2<span class="sc">*</span>X3, df)</span>
<span id="cb71-2"><a href="linear-regresssion.html#cb71-2" aria-hidden="true" tabindex="-1"></a>ols <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> X1 <span class="sc">+</span> X2 <span class="sc">+</span> X2<span class="sc">:</span>X3, df)</span>
<span id="cb71-3"><a href="linear-regresssion.html#cb71-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-4"><a href="linear-regresssion.html#cb71-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(ols)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ X1 + X2 + X2:X3, data = df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.6660 -1.6766 -0.4909  1.6007  4.9059 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept)  -0.6166     0.2156  -2.860   0.0052 **
## X1            0.1228     0.2212   0.555   0.5802   
## X2           -0.1646     0.2168  -0.759   0.4495   
## X2:X3         0.1777     0.2056   0.864   0.3897   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.155 on 96 degrees of freedom
## Multiple R-squared:  0.0218, Adjusted R-squared:  -0.00877 
## F-statistic: 0.7131 on 3 and 96 DF,  p-value: 0.5465</code></pre>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="linear-regresssion.html#cb73-1" aria-hidden="true" tabindex="-1"></a>test_preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(ols, df1)</span>
<span id="cb73-2"><a href="linear-regresssion.html#cb73-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(df1,<span class="dv">2</span>)</span></code></pre></div>
<pre><code>##           X1        X2         X3         X4         X5
## 1 -1.9378190 1.2128464 -0.4286954  0.8357413  0.5587248
## 2 -0.2943809 0.9672296  1.0417259 -0.8419097 -0.3193651</code></pre>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb75-1"><a href="linear-regresssion.html#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="co">#when you start manipulating the data inbetween then you get the problems</span></span>
<span id="cb75-2"><a href="linear-regresssion.html#cb75-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-3"><a href="linear-regresssion.html#cb75-3" aria-hidden="true" tabindex="-1"></a><span class="co">#If you do feature engineering for test, then ADD SOMETHING</span></span></code></pre></div>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="linear-regresssion.html#cb76-1" aria-hidden="true" tabindex="-1"></a>ols <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> . <span class="sc">-</span> X4, df)</span>
<span id="cb76-2"><a href="linear-regresssion.html#cb76-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(ols)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ . - X4, data = df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.4297 -1.6169 -0.4508  1.4788  4.6394 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept) -0.58889    0.22268  -2.645  0.00957 **
## X1           0.16316    0.21990   0.742  0.45993   
## X2          -0.23767    0.20670  -1.150  0.25308   
## X3          -0.07868    0.20577  -0.382  0.70303   
## X5          -0.35892    0.22676  -1.583  0.11678   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.145 on 95 degrees of freedom
## Multiple R-squared:  0.04015,    Adjusted R-squared:  -0.0002612 
## F-statistic: 0.9935 on 4 and 95 DF,  p-value: 0.415</code></pre>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="linear-regresssion.html#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="co">#get rid of intercept</span></span>
<span id="cb78-2"><a href="linear-regresssion.html#cb78-2" aria-hidden="true" tabindex="-1"></a>ols <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y  <span class="sc">~</span> . <span class="sc">-</span><span class="dv">1</span>, df)</span>
<span id="cb78-3"><a href="linear-regresssion.html#cb78-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(ols)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ . - 1, data = df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.6406 -1.8902 -0.9127  1.1197  4.1611 
## 
## Coefficients:
##    Estimate Std. Error t value Pr(&gt;|t|)   
## X1  0.17062    0.21990   0.776  0.43973   
## X2 -0.35453    0.20985  -1.689  0.09442 . 
## X3 -0.04459    0.20332  -0.219  0.82686   
## X4  0.54556    0.20719   2.633  0.00987 **
## X5 -0.41891    0.22524  -1.860  0.06600 . 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.146 on 95 degrees of freedom
## Multiple R-squared:  0.1158, Adjusted R-squared:  0.06923 
## F-statistic: 2.488 on 5 and 95 DF,  p-value: 0.03662</code></pre>
<p>INSERT STUFF ABOUT INTERACTION TERMS: COLON THING</p>
</div>
</div>
</div>
<div id="last-trap-ording-of-the-data" class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> Last trap, ording of the data</h2>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="linear-regresssion.html#cb80-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="sc">-</span><span class="dv">2</span> ,<span class="dv">2</span>)</span>
<span id="cb80-2"><a href="linear-regresssion.html#cb80-2" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> x<span class="sc">^</span><span class="dv">2</span> <span class="sc">*</span> <span class="dv">3</span></span>
<span id="cb80-3"><a href="linear-regresssion.html#cb80-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-4"><a href="linear-regresssion.html#cb80-4" aria-hidden="true" tabindex="-1"></a>ols <span class="ot">&lt;-</span>  <span class="fu">lm</span>(y<span class="sc">~</span>x)</span>
<span id="cb80-5"><a href="linear-regresssion.html#cb80-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(ols)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.3739 -3.3211 -0.4189  3.2166  8.0077 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   4.3648     0.3671  11.889   &lt;2e-16 ***
## x             0.4014     0.3030   1.325    0.188    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.659 on 98 degrees of freedom
## Multiple R-squared:  0.0176, Adjusted R-squared:  0.007573 
## F-statistic: 1.755 on 1 and 98 DF,  p-value: 0.1883</code></pre>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="linear-regresssion.html#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(ols, <span class="fu">data.frame</span>(<span class="at">x =</span> <span class="fu">runif</span>(n,<span class="sc">-</span><span class="dv">2</span>,<span class="dv">2</span>)))</span></code></pre></div>
<pre><code>##        1        2        3        4        5        6        7        8 
## 5.106049 4.842574 3.937752 4.206981 4.902509 3.677574 4.939795 4.136371 
##        9       10       11       12       13       14       15       16 
## 4.369066 3.935517 4.027980 4.388961 5.115381 3.719298 3.580939 3.699432 
##       17       18       19       20       21       22       23       24 
## 3.738683 4.336000 4.618249 5.047126 4.431023 3.601594 4.078987 4.145642 
##       25       26       27       28       29       30       31       32 
## 3.864399 3.775180 4.760625 4.530870 5.075906 4.131424 4.320244 4.673412 
##       33       34       35       36       37       38       39       40 
## 3.814423 4.582483 4.795278 3.964230 4.164369 4.544594 4.756879 3.921163 
##       41       42       43       44       45       46       47       48 
## 4.237075 3.779582 4.550385 4.951418 4.433857 4.274919 4.124860 3.937461 
##       49       50       51       52       53       54       55       56 
## 4.883550 3.940454 5.145065 4.054113 3.969304 4.313458 4.276917 4.280787 
##       57       58       59       60       61       62       63       64 
## 5.132945 5.042830 3.822147 4.115994 4.184052 4.724140 3.764775 4.893813 
##       65       66       67       68       69       70       71       72 
## 5.006856 3.981253 3.668645 3.772675 4.518990 4.805206 3.836124 5.112823 
##       73       74       75       76       77       78       79       80 
## 5.076476 3.682843 4.083817 4.747796 3.846645 4.482560 4.993103 4.529510 
##       81       82       83       84       85       86       87       88 
## 4.463361 4.784376 4.160317 4.660654 4.971043 4.215374 4.679877 4.114744 
##       89       90       91       92       93       94       95       96 
## 3.845933 4.873646 4.129768 4.383853 4.881578 5.046319 3.942813 4.834986 
##       97       98       99      100 
## 4.768836 5.165950 4.722129 4.202319</code></pre>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="linear-regresssion.html#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(ols, <span class="fu">data.frame</span>(<span class="at">x =</span> <span class="fu">runif</span>(n, <span class="sc">-</span><span class="dv">2</span>,<span class="dv">2</span>)))</span></code></pre></div>
<pre><code>##        1        2        3        4        5        6        7        8 
## 4.024704 4.942866 4.953035 4.459428 4.304575 5.134069 4.556407 3.842406 
##        9       10       11       12       13       14       15       16 
## 4.945627 4.512183 4.655865 4.446232 4.930747 3.639291 4.807271 4.381954 
##       17       18       19       20       21       22       23       24 
## 4.375776 4.387474 3.925237 5.034666 3.596060 4.055826 5.121786 4.764068 
##       25       26       27       28       29       30       31       32 
## 4.539492 5.075040 4.948702 3.935831 4.518043 5.021789 4.771444 4.989120 
##       33       34       35       36       37       38       39       40 
## 4.202693 4.545207 3.683356 4.737926 3.580671 5.088198 3.897924 4.847755 
##       41       42       43       44       45       46       47       48 
## 4.501001 4.332773 4.553379 5.071036 4.864777 3.696231 4.695520 4.135846 
##       49       50       51       52       53       54       55       56 
## 3.902262 4.318945 4.395795 4.418020 3.579741 4.322299 4.351609 3.900325 
##       57       58       59       60       61       62       63       64 
## 4.497519 3.749433 4.565140 3.781536 4.319586 4.683591 4.703224 4.926231 
##       65       66       67       68       69       70       71       72 
## 4.136512 3.860778 4.836507 4.375402 4.095719 3.847294 4.172436 3.802586 
##       73       74       75       76       77       78       79       80 
## 4.522997 3.989132 4.678407 4.121535 4.109222 4.296215 3.631729 5.074071 
##       81       82       83       84       85       86       87       88 
## 4.229215 4.353049 5.103992 4.878645 4.756014 4.129723 5.142449 3.855923 
##       89       90       91       92       93       94       95       96 
## 3.673434 4.984974 3.600499 3.902618 4.971062 4.758742 5.095689 4.038327 
##       97       98       99      100 
## 3.901395 4.632767 4.827831 4.760751</code></pre>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="linear-regresssion.html#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x,y)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb87-1"><a href="linear-regresssion.html#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(ols<span class="sc">$</span>residuals)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-29-2.png" width="672" /></p>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb88-1"><a href="linear-regresssion.html#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="co">#we should expect the residuals to be quadratic as well</span></span>
<span id="cb88-2"><a href="linear-regresssion.html#cb88-2" aria-hidden="true" tabindex="-1"></a><span class="co"># we need to order the data correctly</span></span>
<span id="cb88-3"><a href="linear-regresssion.html#cb88-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-4"><a href="linear-regresssion.html#cb88-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(ols<span class="sc">$</span>fitted.values,</span>
<span id="cb88-5"><a href="linear-regresssion.html#cb88-5" aria-hidden="true" tabindex="-1"></a>     ols<span class="sc">$</span>residuals)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-29-3.png" width="672" /></p>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="linear-regresssion.html#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="co">#remember the reisdualds are ordered the same as the data. If the data was random, then the residuals will be random. </span></span>
<span id="cb89-2"><a href="linear-regresssion.html#cb89-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-3"><a href="linear-regresssion.html#cb89-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(ols)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-29-4.png" width="672" /><img src="bookdownproj_files/figure-html/unnamed-chunk-29-5.png" width="672" /><img src="bookdownproj_files/figure-html/unnamed-chunk-29-6.png" width="672" /><img src="bookdownproj_files/figure-html/unnamed-chunk-29-7.png" width="672" /></p>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb90-1"><a href="linear-regresssion.html#cb90-1" aria-hidden="true" tabindex="-1"></a><span class="co">#naming comment</span></span>
<span id="cb90-2"><a href="linear-regresssion.html#cb90-2" aria-hidden="true" tabindex="-1"></a><span class="co">#If we decided to </span></span>
<span id="cb90-3"><a href="linear-regresssion.html#cb90-3" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(X)</span></code></pre></div>
<pre><code>## [1] 100   5</code></pre>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb92-1"><a href="linear-regresssion.html#cb92-1" aria-hidden="true" tabindex="-1"></a><span class="fu">length</span>(Y)</span></code></pre></div>
<pre><code>## [1] 100</code></pre>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb94-1"><a href="linear-regresssion.html#cb94-1" aria-hidden="true" tabindex="-1"></a>ols <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> X[,<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>])</span>
<span id="cb94-2"><a href="linear-regresssion.html#cb94-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(ols)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ X[, 1:2])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.4381 -1.6227 -0.4786  1.5618  4.9223 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept)  -0.6177     0.2153  -2.869  0.00505 **
## X[, 1:2]1     0.1409     0.2200   0.640  0.52339   
## X[, 1:2]2    -0.2195     0.2070  -1.060  0.29166   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.152 on 97 degrees of freedom
## Multiple R-squared:  0.01419,    Adjusted R-squared:  -0.006135 
## F-statistic: 0.6981 on 2 and 97 DF,  p-value: 0.5</code></pre>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb96-1"><a href="linear-regresssion.html#cb96-1" aria-hidden="true" tabindex="-1"></a><span class="co">#the naming is a mess!</span></span>
<span id="cb96-2"><a href="linear-regresssion.html#cb96-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-3"><a href="linear-regresssion.html#cb96-3" aria-hidden="true" tabindex="-1"></a>wrong_stuff <span class="ot">&lt;-</span> <span class="fu">predict</span>(ols, <span class="fu">data.frame</span>(<span class="st">&quot;X[, 1:2]&quot;</span> <span class="ot">=</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>,</span>
<span id="cb96-4"><a href="linear-regresssion.html#cb96-4" aria-hidden="true" tabindex="-1"></a>                        <span class="st">&quot;X[, 1:2]2&quot;</span> <span class="ot">=</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>))</span></code></pre></div>
<pre><code>## Warning: &#39;newdata&#39; had 3 rows but variables found have 100 rows</code></pre>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb98-1"><a href="linear-regresssion.html#cb98-1" aria-hidden="true" tabindex="-1"></a>mysterious_vals <span class="ot">&lt;-</span> <span class="fu">predict</span>(ols)                       </span>
<span id="cb98-2"><a href="linear-regresssion.html#cb98-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">abs</span>(mysterious_vals <span class="sc">-</span> ols<span class="sc">$</span>fitted.values))</span></code></pre></div>
<pre><code>## [1] 3.555489e-14</code></pre>
<div id="missing" class="section level3" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Missing</h3>
<p>If there is no overlap in the data. Then there is no overlapping data. You can’t run regression in that case.</p>
<p>you should probably have a reason to say you need a certain amount of overlap. The best way to get there is to find the needed level of confidence and then back in the answer.</p>
</div>
<div id="glm" class="section level3" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> GLM</h3>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb100-1"><a href="linear-regresssion.html#cb100-1" aria-hidden="true" tabindex="-1"></a>inv_logit <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {</span>
<span id="cb100-2"><a href="linear-regresssion.html#cb100-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">exp</span>(x) <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(x)))</span>
<span id="cb100-3"><a href="linear-regresssion.html#cb100-3" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb100-4"><a href="linear-regresssion.html#cb100-4" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(n, <span class="dv">1</span>, <span class="at">prob =</span> <span class="fu">inv_logit</span>(X <span class="sc">%*%</span> params))</span>
<span id="cb100-5"><a href="linear-regresssion.html#cb100-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(X[, <span class="dv">1</span>], y)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="linear-regresssion.html#cb101-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(X[, <span class="dv">2</span>], y)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-30-2.png" width="672" /></p>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb102-1"><a href="linear-regresssion.html#cb102-1" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(<span class="fu">cbind</span>(y, X))</span>
<span id="cb102-2"><a href="linear-regresssion.html#cb102-2" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(df) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Y&quot;</span>, <span class="fu">paste0</span>(<span class="st">&quot;X&quot;</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>))</span>
<span id="cb102-3"><a href="linear-regresssion.html#cb102-3" aria-hidden="true" tabindex="-1"></a>log_reg <span class="ot">&lt;-</span> <span class="fu">glm</span>(Y <span class="sc">~</span> ., df,</span>
<span id="cb102-4"><a href="linear-regresssion.html#cb102-4" aria-hidden="true" tabindex="-1"></a>               <span class="at">family =</span> <span class="fu">binomial</span>(logit))</span></code></pre></div>
<pre><code>## Warning: glm.fit: algorithm did not converge</code></pre>
<pre><code>## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred</code></pre>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="linear-regresssion.html#cb105-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(log_reg)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = Y ~ ., family = binomial(logit), data = df)
## 
## Deviance Residuals: 
##        Min          1Q      Median          3Q         Max  
## -9.290e-05  -2.100e-08   0.000e+00   2.100e-08   7.065e-05  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept)    -3.366  10196.030   0.000    1.000
## X1             -5.953  20477.695   0.000    1.000
## X2           -111.671  33835.494  -0.003    0.997
## X3            160.509  44097.998   0.004    0.997
## X4           -118.958  34469.702  -0.003    0.997
## X5             66.250  18522.441   0.004    0.997
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1.3863e+02  on 99  degrees of freedom
## Residual deviance: 2.5698e-08  on 94  degrees of freedom
## AIC: 12
## 
## Number of Fisher Scoring iterations: 25</code></pre>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="linear-regresssion.html#cb107-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(log_reg)</span></code></pre></div>
<pre><code>##          1          2          3          4          5          6          7 
##  340.94779  159.11802  465.48533   78.73982 -442.02009 -152.37337  -74.01504 
##          8          9         10         11         12         13         14 
## -146.80808   19.80870  194.75835  -93.19903 -562.33202 -290.30952  -85.66608 
##         15         16         17         18         19         20         21 
##  -86.15584 -542.61657   76.77688 -145.66973  191.66985  217.26380  -20.31911 
##         22         23         24         25         26         27         28 
##  201.39673  -97.20738  216.29321 -145.91812   50.03701   20.34030 -592.59858 
##         29         30         31         32         33         34         35 
## -141.41869 -176.34113  -24.95050   19.85694 -203.01659 -372.85779 -368.08717 
##         36         37         38         39         40         41         42 
##  154.30843  445.14718   25.41849  212.85419  496.25755   22.90221 -175.99150 
##         43         44         45         46         47         48         49 
## -329.33933  157.46263 -121.23482   95.58043  -19.26113  -22.61131 -477.42596 
##         50         51         52         53         54         55         56 
## -193.95259 -122.17868   80.00863  293.34383  188.64096   99.98181 -238.24529 
##         57         58         59         60         61         62         63 
## -119.27651  431.06603 -178.27433 -107.59265 -239.51644   49.21045  -69.81197 
##         64         65         66         67         68         69         70 
## -376.60284 -312.81466   95.64755  -80.81469  193.83711  427.17627  424.07873 
##         71         72         73         74         75         76         77 
##   98.77221  338.87437  106.83231 -400.46821  -47.03111  316.90314 -363.65862 
##         78         79         80         81         82         83         84 
##  288.70575  182.82647  -21.62191  -69.48390 -263.90270   84.27067   54.08654 
##         85         86         87         88         89         90         91 
##  279.08307   91.88068  110.40911  100.82915 -126.60938 -362.91775 -404.00873 
##         92         93         94         95         96         97         98 
##  163.32074  159.56763  134.46685 -445.07685  198.88447 -136.46412  118.40490 
##         99        100 
## -441.75954   82.64561</code></pre>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb109-1"><a href="linear-regresssion.html#cb109-1" aria-hidden="true" tabindex="-1"></a>myst_vals <span class="ot">&lt;-</span> <span class="fu">predict</span>(log_reg, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>)</span>
<span id="cb109-2"><a href="linear-regresssion.html#cb109-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb109-3"><a href="linear-regresssion.html#cb109-3" aria-hidden="true" tabindex="-1"></a>X_test <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(n <span class="sc">*</span> p), <span class="at">nrow =</span> n)</span>
<span id="cb109-4"><a href="linear-regresssion.html#cb109-4" aria-hidden="true" tabindex="-1"></a>X_test_df <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(X_test)</span>
<span id="cb109-5"><a href="linear-regresssion.html#cb109-5" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(X_test_df) <span class="ot">&lt;-</span> <span class="fu">names</span>(df)[<span class="sc">-</span><span class="dv">1</span>]</span>
<span id="cb109-6"><a href="linear-regresssion.html#cb109-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb109-7"><a href="linear-regresssion.html#cb109-7" aria-hidden="true" tabindex="-1"></a>test_preds <span class="ot">&lt;-</span></span>
<span id="cb109-8"><a href="linear-regresssion.html#cb109-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">predict</span>(log_reg, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>, <span class="at">newdata =</span> X_test_df)</span>
<span id="cb109-9"><a href="linear-regresssion.html#cb109-9" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(test_preds)</span></code></pre></div>
<pre><code>##            1            2            3            4            5            6 
## 2.220446e-16 1.000000e+00 1.000000e+00 1.000000e+00 2.220446e-16 1.000000e+00</code></pre>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb111-1"><a href="linear-regresssion.html#cb111-1" aria-hidden="true" tabindex="-1"></a>params</span></code></pre></div>
<pre><code>## [1]  0.7020676 -5.6922590  8.7655680 -5.7972309  3.2304250</code></pre>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="tidyverse-tools.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="classification.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/noahlove/data-mining/edit/master/03-linear-regression.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/noahlove/data-mining/blob/master/03-linear-regression.Rmd",
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
