[["index.html", "Data Mining Chapter 1 A Brief Introduction", " Data Mining Noah Love 2021-02-19 Chapter 1 A Brief Introduction "],["the-pool-of-tears.html", "Chapter 2 The pool of tears", " Chapter 2 The pool of tears "],["a-caucus-race-and-a-long-tale.html", "Chapter 3 A caucus-race and a long tale", " Chapter 3 A caucus-race and a long tale "],["principal-component-analysis.html", "Chapter 4 Principal Component Analysis 4.1 Principle Component Analysis 4.2 Typical machine learning approach 4.3 What we would do in data mining", " Chapter 4 Principal Component Analysis 4.1 Principle Component Analysis hidden_p &lt;- 5 observ_p &lt;- 30 prob &lt;- NULL # runif(hidden_p) h2o &lt;- sample(hidden_p, #hidden to observed observ_p, replace=TRUE, prob=prob) h2o &lt;- sort(h2o) sample_size &lt;- 1000 hidden_z &lt;- sapply( seq_len(hidden_p), function(x) rnorm(sample_size)) corrs &lt;- runif(observ_p, 0.3, 0.8) #create five groups of colinear stuff observ_x &lt;- mapply( function(i, corr) { hidden_z[, i] * corr + rnorm(sample_size) * (1 - corr) *100 }, h2o, corrs) #observ_x is what you often see, but there is still hidden stuff image(cor(observ_x)) #This looks weird to what we expect. It doesn&#39;t look like five groups #if we sort instead it works h2o &lt;- sample(hidden_p, #hidden to observed observ_p, replace=TRUE, prob=prob) sample_size &lt;- 1000 hidden_z &lt;- sapply( seq_len(hidden_p), function(x) rnorm(sample_size)) #This effects how things are correlated! corrs &lt;- runif(observ_p, 0.3, 0.8) #create five groups of colinear stuff observ_x &lt;- mapply( function(i, corr) { hidden_z[, i] * corr + rnorm(sample_size) * (1 - corr) }, h2o, corrs) #observ_x is what you often see, but there is still hidden stuff image(cor(observ_x)) beta &lt;- runif(hidden_p, -10, 10) noise &lt;- rnorm(sample_size, sd=10) #hard to measure hidden forces! #we can only measure x, but x is only correlated to hidden stuff y &lt;- hidden_z %*% beta + noise df &lt;- data.frame(y, observ_x) #y depends on the hidden stuff not x Maybe there is a hidden gene inside of you that makes you sick. We canâ€™t (yet) measure that hidden gene. But we can measure symptoms and things like your heart rate. This should be correlated. 4.2 Typical machine learning approach #training data set, first 800 points (80 percent) train &lt;- 1:800 ols &lt;- lm(y ~ ., df, subset=train) length(ols$residual) #correct length ## [1] 800 #predict on points we didn&#39;t use to train ols_pred &lt;- predict(ols, df[-train,]) #error: differrence between measured values against predict. Mean of this squared MSE! mean((df$y[-train] - ols_pred)^2) ## [1] 117.7051 #run ols #PCA TIME #only input X, (feature matrix) into the PCA function pr_out &lt;- prcomp(observ_x, scale=FALSE) #scale is used because Xs might not be in the same unit, so mean = 0, sd = 1 class(pr_out) ## [1] &quot;prcomp&quot; #it has its own class names(pr_out) ## [1] &quot;sdev&quot; &quot;rotation&quot; &quot;center&quot; ## [4] &quot;scale&quot; &quot;x&quot; #squared value of sdev is eigenvalue eigen_val &lt;- pr_out$sdev^2 #cumulative sum as a fraction of total eigenvalues plot(cumsum(eigen_val) / sum(eigen_val)) abline(h=.9) #here after the 5th point, the slope tapers off! This is directly related to the hidden_p value at the beginning. It should help show you how many important hidden features there are. #it is the percent of variabilitiy caputured by the first k components #If you don&#39;t know what to choose, 90% variability is a good point plot(pr_out$sdev) #Very similar, but not as interpretable as percent of variability. #These steps is how k is chosen. #K is the dimension of W. Data came n x p. We need to shrink it to k. If you don&#39;t have #a clear cut, use 90% #we don&#39;t want to keep all the variability because not all features provide useful #information. Some of them are so colinear, they just add noise. cutoff &lt;- 5 #now we are looking at x #only pull out first k columns dim(pr_out$x) ## [1] 1000 30 dim(observ_x) ## [1] 1000 30 #these will be the same, but we choose a cutoff. W &lt;- pr_out$x[, 1:cutoff] df_w &lt;- data.frame(y, W) #should be like the ols from above pca &lt;- lm(y ~ ., df_w, subset=train) #same prediction pca_pred &lt;- predict(pca, df_w[-train,]) #prediction error mean((df_w$y[-train] - pca_pred)^2) ## [1] 122.8738 #that was the classic view of PCA 4.3 What we would do in data mining #Now looking at rotation #k is which column are we going to examine j &lt;- 2 plot(pr_out$rotation[, j]) abline(h = 0) which(abs(pr_out$rotation[, j]) &gt; 0.2) ## [1] 1 2 4 6 13 15 17 19 26 27 28 30 # what is the truth we should be comparing to? #What is rotation. Matrix (30 x 30 in this case) dim(pr_out$rotation) ## [1] 30 30 #it is actually p x p rather than p x k to give you more columns head(pr_out$rotation[,k]) ## [1] -0.22431148 -0.21057597 0.03151215 ## [4] 0.32194265 -0.01984672 -0.29001669 #kth eigenvector which correspods with the kth largest value (most significant) value #this will always be 1. Property of rotation matrix sum(pr_out$rotation[,k]^2) ## [1] 1 "]]
