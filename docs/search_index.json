[["index.html", "HW1 Chapter 1 A Brief Introduction", " HW1 Noah Love 1/18/2021 Chapter 1 A Brief Introduction "],["tidyverse-tools.html", "Chapter 2 Tidyverse tools 2.1 Select 2.2 Filter 2.3 Mutate 2.4 Summarise / Summarize 2.5 Arrange() 2.6 Group_by()", " Chapter 2 Tidyverse tools library(tidyverse) 2.1 Select 2.1.1 Original Data Set head(storms) ## # A tibble: 6 x 13 ## name year month day hour lat long status category wind pressure ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;ord&gt; &lt;int&gt; &lt;int&gt; ## 1 Amy 1975 6 27 0 27.5 -79 tropi… -1 25 1013 ## 2 Amy 1975 6 27 6 28.5 -79 tropi… -1 25 1013 ## 3 Amy 1975 6 27 12 29.5 -79 tropi… -1 25 1013 ## 4 Amy 1975 6 27 18 30.5 -79 tropi… -1 25 1013 ## 5 Amy 1975 6 28 0 31.5 -78.8 tropi… -1 25 1012 ## 6 Amy 1975 6 28 6 32.4 -78.7 tropi… -1 25 1012 ## # … with 2 more variables: ts_diameter &lt;dbl&gt;, hu_diameter &lt;dbl&gt; You can use (data source, columns) dplyr::select(storms, name, pressure) ## # A tibble: 10,010 x 2 ## name pressure ## &lt;chr&gt; &lt;int&gt; ## 1 Amy 1013 ## 2 Amy 1013 ## 3 Amy 1013 ## 4 Amy 1013 ## 5 Amy 1012 ## 6 Amy 1012 ## 7 Amy 1011 ## 8 Amy 1006 ## 9 Amy 1004 ## 10 Amy 1002 ## # … with 10,000 more rows We can also do (data source, -columns) to choose all but that column, i.e.: dplyr::select(storms, -name) ## # A tibble: 10,010 x 12 ## year month day hour lat long status category wind pressure ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;ord&gt; &lt;int&gt; &lt;int&gt; ## 1 1975 6 27 0 27.5 -79 tropi… -1 25 1013 ## 2 1975 6 27 6 28.5 -79 tropi… -1 25 1013 ## 3 1975 6 27 12 29.5 -79 tropi… -1 25 1013 ## 4 1975 6 27 18 30.5 -79 tropi… -1 25 1013 ## 5 1975 6 28 0 31.5 -78.8 tropi… -1 25 1012 ## 6 1975 6 28 6 32.4 -78.7 tropi… -1 25 1012 ## 7 1975 6 28 12 33.3 -78 tropi… -1 25 1011 ## 8 1975 6 28 18 34 -77 tropi… -1 30 1006 ## 9 1975 6 29 0 34.4 -75.8 tropi… 0 35 1004 ## 10 1975 6 29 6 34 -74.8 tropi… 0 40 1002 ## # … with 10,000 more rows, and 2 more variables: ts_diameter &lt;dbl&gt;, ## # hu_diameter &lt;dbl&gt; 2.1.2 Other select functions: Other useful selection function Function call Description - select everything but : select range contains() Select columns whose name contains a character string ends_with() Select columns whose name ends with a string everything() Select every column matches() Select columns who name matches a regular expression num_range() Select columns named x1,x2,x3… one_of() Select columns whose names are in a group of names starts_with() Select columns whose name starts with a character string 2.2 Filter This will apply a test to every row in the data frame and return just the rows that pass the test. You can combine by putting a comma in the state. It acts as AND. dplyr::filter(storms, wind &gt;= 50, name %in% c(&quot;alberto&quot;, &quot;Alex&quot;, &quot;allison&quot;)) ## # A tibble: 35 x 13 ## name year month day hour lat long status category wind ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;ord&gt; &lt;int&gt; ## 1 Alex 2004 8 2 12 31.3 -79 tropi… 0 50 ## 2 Alex 2004 8 2 18 31.8 -78.7 tropi… 0 50 ## 3 Alex 2004 8 3 0 32.4 -78.2 tropi… 0 60 ## 4 Alex 2004 8 3 6 33 -77.4 hurri… 1 70 ## 5 Alex 2004 8 3 12 34.2 -76.4 hurri… 2 85 ## 6 Alex 2004 8 3 18 35.3 -75.2 hurri… 2 85 ## 7 Alex 2004 8 4 0 36 -73.7 hurri… 1 80 ## 8 Alex 2004 8 4 6 36.8 -72.1 hurri… 1 80 ## 9 Alex 2004 8 4 12 37.3 -70.2 hurri… 2 85 ## 10 Alex 2004 8 4 18 37.8 -68.3 hurri… 2 95 ## # … with 25 more rows, and 3 more variables: pressure &lt;int&gt;, ## # ts_diameter &lt;dbl&gt;, hu_diameter &lt;dbl&gt; 2.2.1 Logical Tests in R The columns on the right show Boolean operator, these combine 2 or more logical tests into a single one, so you get true or false. The columns on the left actually do logical operations. Logical tests in R Logical Test ?Comparison Boolean Operator ?base::Logic &lt; &amp; boolean and &gt; Greater than | boolean or == Equal to xor exactly or &lt;= Less than or equal ! not &gt;= Greater than or equal any any true %in% Group membership all all true != Not equal is.na is NA !is.na Is not NA 2.3 Mutate The mutate function takes your data frame and it returns a copy of the data with, with additional variables that you derive from the variable set that exists in the data. So mutate is there for anything you can derive from the data you already have, and want to make a new column from it. storms %&gt;% mutate(ratio = pressure / wind) %&gt;% select(name, pressure, wind, ratio) ## # A tibble: 10,010 x 4 ## name pressure wind ratio ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Amy 1013 25 40.5 ## 2 Amy 1013 25 40.5 ## 3 Amy 1013 25 40.5 ## 4 Amy 1013 25 40.5 ## 5 Amy 1012 25 40.5 ## 6 Amy 1012 25 40.5 ## 7 Amy 1011 25 40.4 ## 8 Amy 1006 30 33.5 ## 9 Amy 1004 35 28.7 ## 10 Amy 1002 40 25.0 ## # … with 10,000 more rows You can also make multiple columns at the same time, even using columns that are created at the same time, as long as those come before chronologically, i.e. storms %&gt;% mutate(ratio = pressure / wind, inverse = ratio^-1) %&gt;% select(name, pressure, wind, ratio, inverse) ## # A tibble: 10,010 x 5 ## name pressure wind ratio inverse ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Amy 1013 25 40.5 0.0247 ## 2 Amy 1013 25 40.5 0.0247 ## 3 Amy 1013 25 40.5 0.0247 ## 4 Amy 1013 25 40.5 0.0247 ## 5 Amy 1012 25 40.5 0.0247 ## 6 Amy 1012 25 40.5 0.0247 ## 7 Amy 1011 25 40.4 0.0247 ## 8 Amy 1006 30 33.5 0.0298 ## 9 Amy 1004 35 28.7 0.0349 ## 10 Amy 1002 40 25.0 0.0399 ## # … with 10,000 more rows One thing to note: mutate doesn’t affect the original data frame. It simply returns a new temporary dataframe. If you want to save the columns you need to assign it to a new data frame (or overwrite the original) 2.3.1 Useful mutate functions: Function Description pmin(), pmax() Element wise min and max cummin(), cummax() cumulative min and max cumsum(), cumprod() Cumulative sum and product between() Are values between a and b? cume_dist() Cumulative distribution of values cumall(), cumany() Cumulative all and any cummean() Cumulative mean lead(), lag() Copy with values one position ntile() Bin vector into n buckets dense_rank(), min_rank(), percent_rank(), row_number() Various ranking methods 2.4 Summarise / Summarize Summarize allows us to take a data frame and calculate a summary statistic from it and get back a new data frame that is much smaller. storms %&gt;% summarize(median = median(pressure), variance = var(pressure), n = n()) ## # A tibble: 1 x 3 ## median variance n ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 999 381. 10010 Works very similar to mutate. 2.4.1 Useful summary functions Useful summary functions, top 5 are specific to dplyr Function Description min(), max() Minimum and maximum values mean() Mean value median() Median Value sum() Sum of values var, sd() Variance and standard deviation of a vector first() First value in a vector last() Last value in a vector nth Nth value in a vector n() The number of values in a vector n_distinct() The number of distinct values in a vector 2.5 Arrange() This doesn’t add or subtract from your data, but helps you organize your rows! storms %&gt;% arrange(desc(wind)) %&gt;% select(name, wind, pressure, year) ## # A tibble: 10,010 x 4 ## name wind pressure year ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Gilbert 160 888 1988 ## 2 Wilma 160 882 2005 ## 3 Gilbert 155 889 1988 ## 4 Mitch 155 905 1998 ## 5 Mitch 155 910 1998 ## 6 Rita 155 895 2005 ## 7 Rita 155 897 2005 ## 8 Anita 150 926 1977 ## 9 David 150 924 1979 ## 10 David 150 926 1979 ## # … with 10,000 more rows This way, ties are just ordered in the way they originally appeared in the dataframe. Alternatively, you can give a second column to sort by to break those ties, such as by year: storms %&gt;% arrange(desc(wind), desc(year)) %&gt;% select(name, wind, pressure, year) ## # A tibble: 10,010 x 4 ## name wind pressure year ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Wilma 160 882 2005 ## 2 Gilbert 160 888 1988 ## 3 Rita 155 895 2005 ## 4 Rita 155 897 2005 ## 5 Mitch 155 905 1998 ## 6 Mitch 155 910 1998 ## 7 Gilbert 155 889 1988 ## 8 Dean 150 907 2007 ## 9 Dean 150 905 2007 ## 10 Felix 150 935 2007 ## # … with 10,000 more rows 2.6 Group_by() A very powerful function to get summary statistics from just certain groups. Say we want to see median wind speed based on year for example: storms %&gt;% group_by(year) %&gt;% summarize(median = median(pressure), variance = var(pressure), n = n()) ## # A tibble: 41 x 4 ## year median variance n ## * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1975 997 231. 86 ## 2 1976 992 233. 52 ## 3 1977 1001 417. 53 ## 4 1978 1007 44.0 54 ## 5 1979 1002 395. 301 ## 6 1980 996 128. 161 ## 7 1981 997 217. 164 ## 8 1982 1001 232. 105 ## 9 1983 1005 138. 79 ## 10 1984 998 178. 236 ## # … with 31 more rows We can save that in a in data frame and graph it too! storms_graph &lt;- storms %&gt;% group_by(year) %&gt;% summarize(median = median(pressure), variance = var(pressure), n = n()) ggplot(data = storms_graph, mapping = aes(x = year, y = median)) + geom_point() + geom_smooth() + labs(title = &quot;Median wind speed of storms by year&quot;) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; You can also group by multiple things, just add a comma! "],["linear-regresssion.html", "Chapter 3 Linear Regresssion 3.1 Non linear data 3.2 Last trap, ording of the data", " Chapter 3 Linear Regresssion library(tidyverse) We have a sample of 100, 5 different variables (p), and an X matrix. We are making uniform variables, and we will jam them into a matrix such that the number of rows is equal to n. n &lt;- 100 p &lt;- 5 X &lt;- matrix(runif(p * n), nrow=n) Then we can create some Y’s based on betas and noise. To do so, we need betas. We will make all of them zeros, except for one of them! We also need to define our noise. beta &lt;- rep(0, p) beta[sample(p, 1)] &lt;- 1 noise &lt;- rnorm(n) # %*% Matrix multiplication in R #X is a matrix of 100 by 5 #Beta is a matrix of 5 Y &lt;- X %*% beta + noise The above is generating data according to a linear model! So what does this do? We have X and a bunch of 0 betas, except for one special one. The Y then will depend only on the one column! If we don’t know which beta matters, we just fit y against x and print out a summary. ols &lt;- lm(Y ~ X) summary(ols) ## ## Call: ## lm(formula = Y ~ X) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.03705 -0.52657 0.00186 0.63090 2.93349 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.4710 0.4387 1.074 0.286 ## X1 -0.1822 0.3480 -0.523 0.602 ## X2 -0.1492 0.3595 -0.415 0.679 ## X3 0.1422 0.3652 0.389 0.698 ## X4 0.5893 0.3639 1.619 0.109 ## X5 -0.3065 0.4047 -0.757 0.451 ## ## Residual standard error: 1.017 on 94 degrees of freedom ## Multiple R-squared: 0.03589, Adjusted R-squared: -0.01539 ## F-statistic: 0.6999 on 5 and 94 DF, p-value: 0.6249 So only one of them is statistically significant! This shows it should be the 5th! We can see the truth: beta ## [1] 0 0 0 1 0 This is a good example of fitting basic regression. Lets look at the y and subtract the the coefficients to find the residuals: note: length(ols$coefficients) is 6, but dimensions of X is going to be 5.So we will need to cbind. So this is y minus the fitted data from the regression. When we do summary of ols, we get a bunch of estimates. The intercept and the estimated coefficients. These are estimates from the regression based on the data. Using these, we can recover and estimate and then find the difference to see what the residuals are! resid &lt;- Y - cbind(1, X) %*% ols$coefficients #this is the manual version of: resid &lt;- ols$residuals Now let’s do the residuals against the true beta! We are subtracting the x values against the true beta. True beta means this is the beta that created the data. This is how god created the world. It is the actual physical value. The above residuals are from the regression of the fitted data. resid_from_truth &lt;- Y - X %*% beta We can plot this as well! plot(density(resid_from_truth)) lines(density(resid), col = &quot;red&quot;) abline(v = 0) This might not be enough contrast for us to tell. How can we quantify the difference? Let’s see the difference between the two: mean(abs(resid)) ## [1] 0.7565599 mean(abs(resid_from_truth)) ## [1] 0.7698125 We want the smaller value! The simulated data based values will always have a smaller residual mean than the real values! This is rather disturbing. The fitted coefficients from your regression will always be better than the “true” coefficients. Why is this bad? Normally we want to use regression to find the natural coefficients or natural facts about the world. These are based on some “truth”. If you can collect noisy data but our algo prefers the fitted data rather than the truth we have a problem. We want our error minimized at the “truth”. Our regression here doesn’t like the true answer and it prefers something else. It actually prefers the training data. This is because we use the same data to train and evaluate. Our data is just optimized for this one thing. If we know how you are going to evaluate me, I will just optimize for that specific thing. So let’s generate another set of data: new Y using the same beta and x but with new noise values: Testing data generated from the same population but not used for training the model new_noise &lt;- rnorm(n) new_Y &lt;- X %*% beta + new_noise new_resid &lt;- new_Y - cbind(1, X) %*% ols$coefficients new_resid_from_truth &lt;- new_Y - X %*% beta mean(abs(new_resid)) ## [1] 0.9326886 mean(abs(new_resid_from_truth)) ## [1] 0.9269536 Our takeaway is don’t always cross over your data because you risk overfitting. 3.1 Non linear data Let’s create new data that is nonlinear. This is mini sine data. n &lt;- 100 X &lt;- runif(100, max = 2 * pi / 4 * 3) Y &lt;- 0.1+-3 * sin(X) + rnorm(n, sd = 0.5) Then we can recreate our linear regression and the plot. ols &lt;- lm(Y ~ X) summary(ols) ## ## Call: ## lm(formula = Y ~ X) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.3998 -0.9334 -0.2654 0.9223 3.5911 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.6001 0.2826 -12.74 &lt;2e-16 *** ## X 1.2394 0.1032 12.01 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.316 on 98 degrees of freedom ## Multiple R-squared: 0.5955, Adjusted R-squared: 0.5914 ## F-statistic: 144.3 on 1 and 98 DF, p-value: &lt; 2.2e-16 plot(X, Y) abline(lm(Y ~ X)) As expected our linear model created a line that doesn’t look to fit too well. Let’s look at the residual plot instead. We par(mfrow=c(1,2)) plot(ols$residuals) abline(h=0) plot(X, ols$residuals) abline(h = 0) Xs were generated in a random order so we we just use the index, it looks random and normal. But when we sort by x value, we see that there is definitely a problem and we need to add a quadratic term. We can then find the mean squared error: n &lt;- 100 Y &lt;- 0.1+-3 * sin(X) + rnorm(n, sd = 0.5) #Y - y hat squared mean((Y - ols$fitted.values) ^ 2) ## [1] 1.773755 Plot the MSE vs the number of polynomials used on the x axis. Do this for both the training vs the testing data. We can regress on matrices in R which makes this really easy. So let us create an X_matrix, where for each degree, we raise x to that number of degrees. This matrix will be n rows and degrees columns. n &lt;- 100 degrees &lt;- 1:50 X &lt;- runif(n, max = 2 * pi / 4 * 3) Y &lt;- 0.1+-3 * sin(X) + rnorm(n, sd = 0.5) new_Y &lt;- 0.1+-3 * sin(X) + rnorm(n, sd = 0.5) X_mat &lt;- sapply(degrees, function(i) X ^ i) For example plot(X, X_mat[,5]) We can do this through a loop. #create an empty vector MSEs &lt;- rep(NA, length(degrees)) #Create empty vector for tests test_MSEs &lt;- MSEs for (i in seq_along(degrees)) { # regress for each power on each loop ols &lt;- lm(Y ~ X_mat[, 1:i]) # record the MSEs MSEs[i] &lt;- mean(ols$residuals ^ 2) # do again for new set, only word because we use the same X new_errors &lt;- new_Y - ols$fitted.values # record test_MSEs[i] &lt;- mean(new_errors ^ 2) } Plot in base R plot(degrees, MSEs, type = &quot;b&quot;, ylim = c(0, max(test_MSEs))) lines(degrees, test_MSEs, type = &quot;b&quot;, col = &quot;red&quot;) legend(&quot;topright&quot;, legend = c(&quot;Test&quot;, &quot;Train&quot;), fill = c(&quot;red&quot;, &quot;black&quot;)) Plot in tidyverse summary(ols) ## ## Call: ## lm(formula = Y ~ X_mat[, 1:i]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.20644 -0.24450 -0.01122 0.29082 0.89411 ## ## Coefficients: (28 not defined because of singularities) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.876e+00 3.780e+00 0.496 0.621 ## X_mat[, 1:i]1 -4.892e+01 1.342e+02 -0.365 0.716 ## X_mat[, 1:i]2 4.172e+02 1.778e+03 0.235 0.815 ## X_mat[, 1:i]3 -2.566e+03 1.215e+04 -0.211 0.833 ## X_mat[, 1:i]4 1.156e+04 4.929e+04 0.234 0.815 ## X_mat[, 1:i]5 -3.598e+04 1.289e+05 -0.279 0.781 ## X_mat[, 1:i]6 7.602e+04 2.289e+05 0.332 0.741 ## X_mat[, 1:i]7 -1.102e+05 2.851e+05 -0.387 0.700 ## X_mat[, 1:i]8 1.110e+05 2.532e+05 0.438 0.662 ## X_mat[, 1:i]9 -7.800e+04 1.605e+05 -0.486 0.628 ## X_mat[, 1:i]10 3.753e+04 7.104e+04 0.528 0.599 ## X_mat[, 1:i]11 -1.166e+04 2.064e+04 -0.565 0.574 ## X_mat[, 1:i]12 1.923e+03 3.226e+03 0.596 0.553 ## X_mat[, 1:i]13 NA NA NA NA ## X_mat[, 1:i]14 -4.969e+01 7.725e+01 -0.643 0.522 ## X_mat[, 1:i]15 NA NA NA NA ## X_mat[, 1:i]16 2.047e+00 3.043e+00 0.673 0.503 ## X_mat[, 1:i]17 NA NA NA NA ## X_mat[, 1:i]18 -7.467e-02 1.085e-01 -0.688 0.493 ## X_mat[, 1:i]19 NA NA NA NA ## X_mat[, 1:i]20 1.794e-03 2.588e-03 0.693 0.490 ## X_mat[, 1:i]21 NA NA NA NA ## X_mat[, 1:i]22 NA NA NA NA ## X_mat[, 1:i]23 -5.136e-06 7.493e-06 -0.686 0.495 ## X_mat[, 1:i]24 NA NA NA NA ## X_mat[, 1:i]25 NA NA NA NA ## X_mat[, 1:i]26 NA NA NA NA ## X_mat[, 1:i]27 3.057e-09 4.653e-09 0.657 0.513 ## X_mat[, 1:i]28 NA NA NA NA ## X_mat[, 1:i]29 NA NA NA NA ## X_mat[, 1:i]30 NA NA NA NA ## X_mat[, 1:i]31 -1.979e-12 3.212e-12 -0.616 0.540 ## X_mat[, 1:i]32 NA NA NA NA ## X_mat[, 1:i]33 NA NA NA NA ## X_mat[, 1:i]34 NA NA NA NA ## X_mat[, 1:i]35 NA NA NA NA ## X_mat[, 1:i]36 1.772e-16 3.184e-16 0.557 0.579 ## X_mat[, 1:i]37 NA NA NA NA ## X_mat[, 1:i]38 NA NA NA NA ## X_mat[, 1:i]39 NA NA NA NA ## X_mat[, 1:i]40 NA NA NA NA ## X_mat[, 1:i]41 NA NA NA NA ## X_mat[, 1:i]42 -2.417e-21 5.021e-21 -0.481 0.632 ## X_mat[, 1:i]43 NA NA NA NA ## X_mat[, 1:i]44 NA NA NA NA ## X_mat[, 1:i]45 NA NA NA NA ## X_mat[, 1:i]46 NA NA NA NA ## X_mat[, 1:i]47 NA NA NA NA ## X_mat[, 1:i]48 2.177e-26 5.338e-26 0.408 0.685 ## X_mat[, 1:i]49 NA NA NA NA ## X_mat[, 1:i]50 NA NA NA NA ## ## Residual standard error: 0.484 on 77 degrees of freedom ## Multiple R-squared: 0.9571, Adjusted R-squared: 0.9448 ## F-statistic: 78.05 on 22 and 77 DF, p-value: &lt; 2.2e-16 This should be really concerning. It would mean strong colinearity. R is doing us a favor and automatically dropping some redundant features. Create data under the so called regression model. Regression is an algo, but it is also a model for creating data. ODS is a least square algoristhm. n &lt;- 100 p &lt;- 5 # God knows this! params &lt;- runif(p, -10, 10) features &lt;- matrix(rnorm(n * p), nrow=n, ncol=p) X &lt;- features X1 &lt;- matrix(rnorm(n * p), nrow=n, ncol=p) noise &lt;- rnorm(n) noise1 &lt;- rnorm(n) Y1 &lt;- X %*% params + noise Y2 &lt;- 0 for(i in seq_len(p)){ Y2 &lt;- Y2 + params[i] * X1[, i] } Y2 &lt;- Y2 + noise1 plot(X[,1], Y1) points(X1[, 1], Y2, col=&quot;red&quot;) df &lt;- as.data.frame(cbind(Y,X)) dim(df) ## [1] 100 6 head(df,2) ## Y V2 V3 V4 V5 V6 ## 1 -2.844331 0.04596886 -0.0122414 -0.3030977 -0.5512452 -0.3911246 ## 2 -1.925567 0.11862475 -0.3060970 0.1959116 1.7319732 0.2819692 names(df) &lt;- c(&quot;Y&quot;, paste0(&quot;X&quot;, 1:p)) ols &lt;- lm(Y ~ ., df) summary(ols) ## ## Call: ## lm(formula = Y ~ ., data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.3024 -1.6785 -0.5325 1.6759 4.2063 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.80068 0.21045 -3.805 0.000253 *** ## X1 -0.06854 0.21737 -0.315 0.753204 ## X2 -0.07340 0.22268 -0.330 0.742433 ## X3 -0.01231 0.22200 -0.055 0.955883 ## X4 0.15328 0.20187 0.759 0.449566 ## X5 -0.09480 0.22102 -0.429 0.668978 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.102 on 94 degrees of freedom ## Multiple R-squared: 0.01157, Adjusted R-squared: -0.041 ## F-statistic: 0.2201 on 5 and 94 DF, p-value: 0.9531 class(ols) ## [1] &quot;lm&quot; predict(ols) ## 1 2 3 4 5 6 ## -0.8466142 -0.5500023 -0.8992452 -0.4872413 -0.7532348 -0.8425296 ## 7 8 9 10 11 12 ## -0.6692584 -0.8162210 -0.7691131 -0.9428197 -1.2205467 -0.6306511 ## 13 14 15 16 17 18 ## -1.0436556 -0.7739806 -0.6392084 -0.7110288 -1.0616260 -0.3217362 ## 19 20 21 22 23 24 ## -0.9223742 -0.6392848 -0.5225642 -0.7032897 -0.9370182 -1.0634000 ## 25 26 27 28 29 30 ## -0.9392657 -0.5489185 -0.5398687 -0.8362497 -0.5442889 -0.8389391 ## 31 32 33 34 35 36 ## -0.8999913 -0.6289889 -1.0035078 -0.7945077 -0.8974168 -0.6284758 ## 37 38 39 40 41 42 ## -1.1203263 -0.9797961 -1.2330808 -0.7039213 -0.9858462 -0.9522025 ## 43 44 45 46 47 48 ## -0.5711437 -0.9880247 -0.8444748 -0.7115474 -0.8318194 -1.0429827 ## 49 50 51 52 53 54 ## -0.9986723 -0.8391391 -0.7250816 -0.7315999 -0.9641818 -0.5468403 ## 55 56 57 58 59 60 ## -0.7657554 -1.1763156 -0.8306629 -1.0608708 -0.5520130 -0.5404172 ## 61 62 63 64 65 66 ## -1.3130770 -0.7961347 -0.8172254 -0.9659325 -0.4266729 -1.0805610 ## 67 68 69 70 71 72 ## -0.5982009 -0.7192003 -0.5474363 -0.8257202 -0.9346896 -0.8425413 ## 73 74 75 76 77 78 ## -0.6096661 -0.7735372 -0.8784366 -0.8872411 -0.9398991 -0.7309591 ## 79 80 81 82 83 84 ## -0.4472685 -0.4080720 -0.9593109 -0.3709285 -1.3096683 -0.7227044 ## 85 86 87 88 89 90 ## -0.8780641 -0.7603019 -0.3119936 -0.7293890 -0.9063437 -0.8402942 ## 91 92 93 94 95 96 ## -0.8011085 -0.9737516 -0.8073685 -0.9172409 -1.3326231 -0.9122026 ## 97 98 99 100 ## -0.8699353 -0.1898385 -0.7182788 -0.6951232 #if you pass in a lm or glm, predict will use predict.lm or predict.glm anyways. It is smart. #There is an argument in predict that uses &quot;new data&quot;. You need to pass in what the new data is. It should be tempting for us to just pass in X1. #This shouldn&#39;t work! But why? #predict(ols, newdata = as.data.frame(X1)) #convert to df #now there is an error that we don&#39;t know what X2 is. The data frame you are passing it needs to have the same names that you are training on. names(df) ## [1] &quot;Y&quot; &quot;X1&quot; &quot;X2&quot; &quot;X3&quot; &quot;X4&quot; &quot;X5&quot; df1 &lt;- as.data.frame(X1) names(df1) &lt;- names(df)[-1] test_preds &lt;- predict( ols, newdata = df1) #The data you are passing, you need the data to look identical to the data you trained the model with. The names of the data frames must agree. #classic workflow plot(test_preds , Y2) test_errors &lt;- Y2 - test_preds test_rmse &lt;- sqrt(mean(test_errors ^ 2)) test_rmse ## [1] 11.75753 sd(noise) ## [1] 0.9604043 sd(noise1) ## [1] 1.065565 #You cannot reduce beyond this. R will automatically throw out extremely high colinearity instances. In the real world this would be rare. This is unique to R. names(ols) ## [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; ## [5] &quot;fitted.values&quot; &quot;assign&quot; &quot;qr&quot; &quot;df.residual&quot; ## [9] &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; &quot;model&quot; #probably the most important ols$coefficients ## (Intercept) X1 X2 X3 X4 X5 ## -0.80067547 -0.06854461 -0.07339731 -0.01231419 0.15328160 -0.09479590 plot(ols$coefficients, c(0,params)) abline(a = 0, b=1) train_features &lt;- cbind(1,as.matrix(df[,-1])) #take out y column #fitted_vals &lt;- train_features %*% fitted_vals &lt;- train_features %*% ols$coefficients sum(abs(fitted_vals - ols$fitted.values)) ## [1] 7.954748e-14 res &lt;- df$Y - ols$fitted.values sum(abs(res - ols$residuals)) ## [1] 5.689893e-16 plot(ols$residuals) abline(h = 0) #you can also put plot onto the regression function itself plot(ols) #residuals vs fitted values. This is what we saw earlier but much fancier #QQ Plot #scale location not that important #leverage to look for outliers 3.1.0.1 Traps df_missing &lt;- df df_missing[20, &quot;Y&quot;] &lt;- NA #purposeffully lose the value ols &lt;- lm(Y ~., df_missing) length(ols$residuals) ## [1] 99 #lm drops missing value before matrix multiplication. So the residuals will change. 3.1.1 Interactions + subtracting variables 3.1.1.1 Another trap ols &lt;- lm(Y ~ X1 + X2 + X2*X3, df) ols &lt;- lm(Y ~ X1 + X2 + X2:X3, df) summary(ols) ## ## Call: ## lm(formula = Y ~ X1 + X2 + X2:X3, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.1755 -1.7025 -0.5164 1.6277 4.3279 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.79146 0.21051 -3.760 0.000292 *** ## X1 -0.08294 0.21541 -0.385 0.701074 ## X2 -0.05373 0.21758 -0.247 0.805479 ## X2:X3 0.06991 0.25185 0.278 0.781937 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.089 on 96 degrees of freedom ## Multiple R-squared: 0.003456, Adjusted R-squared: -0.02769 ## F-statistic: 0.111 on 3 and 96 DF, p-value: 0.9535 test_preds &lt;- predict(ols, df1) head(df1,2) ## X1 X2 X3 X4 X5 ## 1 -1.07142719 -1.1871989 -0.8844386 0.7901656 2.3611781 ## 2 0.06528965 -0.9471535 -0.1001621 0.4594735 0.6965748 #when you start manipulating the data inbetween then you get the problems #If you do feature engineering for test, then ADD SOMETHING ols &lt;- lm(Y ~ . - X4, df) summary(ols) ## ## Call: ## lm(formula = Y ~ . - X4, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.1062 -1.6155 -0.6362 1.6552 4.3852 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.7988528 0.2099621 -3.805 0.000251 *** ## X1 -0.0801909 0.2163431 -0.371 0.711712 ## X2 -0.0460911 0.2192705 -0.210 0.833960 ## X3 0.0001994 0.2208972 0.001 0.999282 ## X5 -0.1136023 0.2191393 -0.518 0.605383 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.097 on 95 degrees of freedom ## Multiple R-squared: 0.005509, Adjusted R-squared: -0.03636 ## F-statistic: 0.1316 on 4 and 95 DF, p-value: 0.9705 #get rid of intercept ols &lt;- lm(Y ~ . -1, df) summary(ols) ## ## Call: ## lm(formula = Y ~ . - 1, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.0580 -2.4583 -1.3273 0.8825 3.4138 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## X1 -0.103445 0.232067 -0.446 0.657 ## X2 -0.061040 0.237929 -0.257 0.798 ## X3 0.001876 0.237194 0.008 0.994 ## X4 0.144521 0.215698 0.670 0.504 ## X5 -0.090330 0.236174 -0.382 0.703 ## ## Residual standard error: 2.246 on 95 degrees of freedom ## Multiple R-squared: 0.01049, Adjusted R-squared: -0.04159 ## F-statistic: 0.2013 on 5 and 95 DF, p-value: 0.9612 INSERT STUFF ABOUT INTERACTION TERMS: COLON THING 3.2 Last trap, ording of the data x &lt;- runif(n, -2 ,2) y &lt;- x^2 * 3 ols &lt;- lm(y~x) summary(ols) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.426 -2.999 -1.280 2.085 7.910 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.4210 0.3440 9.944 &lt;2e-16 *** ## x -0.2748 0.3216 -0.855 0.395 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.437 on 98 degrees of freedom ## Multiple R-squared: 0.007397, Adjusted R-squared: -0.002731 ## F-statistic: 0.7304 on 1 and 98 DF, p-value: 0.3949 predict(ols, data.frame(x = runif(n,-2,2))) ## 1 2 3 4 5 6 7 8 ## 3.840287 3.895530 3.082596 3.967011 3.183809 3.882475 3.420739 3.559022 ## 9 10 11 12 13 14 15 16 ## 3.511337 3.744857 3.566703 3.870124 3.337779 3.805869 3.401721 3.747448 ## 17 18 19 20 21 22 23 24 ## 3.201034 3.965527 3.631151 3.007863 3.415257 2.871489 3.526979 3.212339 ## 25 26 27 28 29 30 31 32 ## 3.806844 3.512686 3.909634 3.461472 3.467289 2.961054 2.937773 3.132825 ## 33 34 35 36 37 38 39 40 ## 3.114901 3.300550 3.950654 3.190817 3.861283 2.929397 3.928009 3.955799 ## 41 42 43 44 45 46 47 48 ## 3.614185 3.311400 3.077159 3.409506 3.264064 3.006801 3.401457 2.944016 ## 49 50 51 52 53 54 55 56 ## 3.411810 3.483600 3.728660 2.958274 3.070491 2.998553 2.927704 3.145053 ## 57 58 59 60 61 62 63 64 ## 2.930598 3.750936 3.860646 3.646931 3.817446 3.630312 2.994190 3.312688 ## 65 66 67 68 69 70 71 72 ## 3.727644 3.878689 3.813655 3.788756 3.420362 2.928994 3.479459 3.545394 ## 73 74 75 76 77 78 79 80 ## 3.581095 3.591661 2.936144 3.151186 3.362050 3.261028 3.818321 3.676123 ## 81 82 83 84 85 86 87 88 ## 3.226600 3.946157 3.969481 3.076348 3.911955 3.784175 3.953343 3.682453 ## 89 90 91 92 93 94 95 96 ## 3.537163 3.366881 3.863851 3.719945 3.553456 3.158104 2.877040 3.200288 ## 97 98 99 100 ## 3.785358 3.805815 2.927946 3.623890 predict(ols, data.frame(x = runif(n, -2,2))) ## 1 2 3 4 5 6 7 8 ## 3.109591 3.921111 3.429573 3.718912 3.908007 3.510022 2.968095 3.316154 ## 9 10 11 12 13 14 15 16 ## 2.921517 3.266529 3.375247 3.151179 3.854968 3.297138 3.949759 3.607904 ## 17 18 19 20 21 22 23 24 ## 3.066055 3.963296 3.249269 3.440945 3.463565 3.224241 3.350758 3.934221 ## 25 26 27 28 29 30 31 32 ## 3.263613 3.209258 3.332712 3.198461 3.607299 3.758238 3.749192 3.290177 ## 33 34 35 36 37 38 39 40 ## 3.439731 3.872702 3.700448 3.909168 3.002585 3.302670 3.752721 3.793311 ## 41 42 43 44 45 46 47 48 ## 3.202104 3.009737 3.682404 3.926455 3.080774 3.229446 3.170134 3.210652 ## 49 50 51 52 53 54 55 56 ## 2.909685 3.371818 3.881306 3.863863 3.698930 3.827869 3.281986 3.230568 ## 57 58 59 60 61 62 63 64 ## 3.712393 3.092416 3.411363 3.373810 2.874708 3.641461 2.951438 2.917634 ## 65 66 67 68 69 70 71 72 ## 3.910263 3.837779 3.624870 2.902349 2.901413 3.164566 3.022147 3.452001 ## 73 74 75 76 77 78 79 80 ## 3.775202 3.599949 3.550948 3.308599 3.040813 3.475173 3.229017 3.037894 ## 81 82 83 84 85 86 87 88 ## 2.913443 3.222908 3.834751 3.233537 3.738819 3.113187 3.780592 3.963543 ## 89 90 91 92 93 94 95 96 ## 3.540378 3.241221 3.615064 3.019241 3.320209 3.500647 3.661962 3.798940 ## 97 98 99 100 ## 3.768286 3.560545 2.981589 3.402344 plot(x,y) plot(ols$residuals) #we should expect the residuals to be quadratic as well # we need to order the data correctly plot(ols$fitted.values, ols$residuals) #remember the reisdualds are ordered the same as the data. If the data was random, then the residuals will be random. plot(ols) #naming comment #If we decided to dim(X) ## [1] 100 5 length(Y) ## [1] 100 ols &lt;- lm(Y ~ X[,1:2]) summary(ols) ## ## Call: ## lm(formula = Y ~ X[, 1:2]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.2287 -1.7356 -0.5143 1.5971 4.2827 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.79833 0.20806 -3.837 0.000222 *** ## X[, 1:2]1 -0.08604 0.21409 -0.402 0.688664 ## X[, 1:2]2 -0.06260 0.21419 -0.292 0.770699 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.079 on 97 degrees of freedom ## Multiple R-squared: 0.002656, Adjusted R-squared: -0.01791 ## F-statistic: 0.1292 on 2 and 97 DF, p-value: 0.879 #the naming is a mess! wrong_stuff &lt;- predict(ols, data.frame(&quot;X[, 1:2]&quot; = 1:3, &quot;X[, 1:2]2&quot; = 1:3)) ## Warning: &#39;newdata&#39; had 3 rows but variables found have 100 rows mysterious_vals &lt;- predict(ols) sum(abs(mysterious_vals - ols$fitted.values)) ## [1] 5.928591e-14 3.2.1 Missing If there is no overlap in the data. Then there is no overlapping data. You can’t run regression in that case. you should probably have a reason to say you need a certain amount of overlap. The best way to get there is to find the needed level of confidence and then back in the answer. 3.2.2 GLM inv_logit &lt;- function(x) { return(exp(x) / (1 + exp(x))) } y &lt;- rbinom(n, 1, prob = inv_logit(X %*% params)) plot(X[, 1], y) plot(X[, 2], y) df &lt;- as.data.frame(cbind(y, X)) names(df) &lt;- c(&quot;Y&quot;, paste0(&quot;X&quot;, 1:5)) log_reg &lt;- glm(Y ~ ., df, family = binomial(logit)) ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred summary(log_reg) ## ## Call: ## glm(formula = Y ~ ., family = binomial(logit), data = df) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.55700 -0.00719 0.00000 0.03442 2.65678 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.7443 0.7479 -0.995 0.31959 ## X1 -2.8064 1.3453 -2.086 0.03697 * ## X2 7.6632 2.9431 2.604 0.00922 ** ## X3 -1.0879 0.7398 -1.471 0.14142 ## X4 8.2142 2.8049 2.928 0.00341 ** ## X5 -3.7470 1.6072 -2.331 0.01974 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 138.589 on 99 degrees of freedom ## Residual deviance: 15.224 on 94 degrees of freedom ## AIC: 27.224 ## ## Number of Fisher Scoring iterations: 10 predict(log_reg) ## 1 2 3 4 5 6 ## -3.6999292 9.5341582 -7.3779002 13.4527094 -10.5669276 -1.2529507 ## 7 8 9 10 11 12 ## 17.6467846 15.9095184 7.1672730 -14.1199174 -25.0201866 -0.5651663 ## 13 14 15 16 17 18 ## 2.6977897 -17.6902202 -16.4415932 12.9883464 -11.6574498 3.9836565 ## 19 20 21 22 23 24 ## -17.8344357 -4.9540985 0.9221493 -3.4323721 12.5637357 -24.9717316 ## 25 26 27 28 29 30 ## -20.9472646 11.5810627 -14.3785402 6.5527647 36.6323550 -0.7782374 ## 31 32 33 34 35 36 ## -17.5822125 12.1382850 -34.1483668 -13.3531265 -9.0331685 5.8818887 ## 37 38 39 40 41 42 ## -2.0841276 6.4988135 -12.0355978 4.7239670 3.4600884 -8.6990285 ## 43 44 45 46 47 48 ## 14.3504032 5.1436945 7.3833474 -12.1453353 14.3935154 -16.7825635 ## 49 50 51 52 53 54 ## -10.5554323 -16.5965524 1.7658996 -2.5617125 -3.4994745 3.5194480 ## 55 56 57 58 59 60 ## 1.1950382 -14.8082792 -9.8658048 -27.7611566 3.6309294 10.2953611 ## 61 62 63 64 65 66 ## -11.8484967 6.8784387 4.8006735 -6.7060764 10.1062385 -7.3193777 ## 67 68 69 70 71 72 ## 10.4309024 -20.9561651 -7.1760829 19.0406735 -13.7600290 0.8589286 ## 73 74 75 76 77 78 ## 9.1013287 -4.0438335 4.0662245 3.7384007 16.9113042 9.0109326 ## 79 80 81 82 83 84 ## 6.8157417 38.8337582 7.4470155 18.5934908 -10.5890653 4.8936962 ## 85 86 87 88 89 90 ## -5.3470952 -10.8186533 20.7228728 1.2541601 6.5458611 -4.8358950 ## 91 92 93 94 95 96 ## -19.0178771 -2.8661972 -5.7269349 -1.6268291 -19.1615157 -9.0543194 ## 97 98 99 100 ## -5.7117029 19.6122543 15.8195372 7.1440199 myst_vals &lt;- predict(log_reg, type = &quot;response&quot;) X_test &lt;- matrix(rnorm(n * p), nrow = n) X_test_df &lt;- as.data.frame(X_test) names(X_test_df) &lt;- names(df)[-1] test_preds &lt;- predict(log_reg, type = &quot;response&quot;, newdata = X_test_df) head(test_preds) ## 1 2 3 4 5 ## 4.876127e-03 4.349450e-09 1.000000e+00 9.505656e-01 9.492657e-01 ## 6 ## 9.437559e-01 params ## [1] -2.5628620 6.4970846 -0.7251178 9.3246968 -5.4726339 "],["classification.html", "Chapter 4 Classification 4.1 Music dataset 4.2 Logistic Regression Review 4.3 Resampling 4.4 Initial Work 4.5 A basic model with upsampling", " Chapter 4 Classification 4.1 Music dataset From the CORGIS data project I’ve obtained a music dataset. music &lt;- read.csv(&quot;Datasets/music_hottest_song_only.csv&quot;) y &lt;- as.numeric(music$artist.terms == &quot;hip hop&quot;) x &lt;- music$song.hotttnesss 4.2 Logistic Regression Review \\(Y \\sim Binomial(n=1, p(X))\\) \\(p(X) = \\frac{exp(X\\beta)}{\\exp(X\\beta) + 1}\\) logit &lt;- function(x){exp(x) / (1 + exp(x))} logistic_obj &lt;- function(params, X, y){ xbeta &lt;- cbind(1, X) %*% params p &lt;- logit(xbeta) return(-sum(dbinom(y, size=1, prob=p, log=TRUE))) } optim(c(1, 1), logistic_obj, X=x, y=y) ## $par ## [1] -3.2070659 0.2257483 ## ## $value ## [1] 721.6383 ## ## $counts ## function gradient ## 63 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL #Wayne created in class my_model &lt;- glm(y ~ x, family = binomial(logit)) summary(my_model) ## ## Call: ## glm(formula = y ~ x, family = binomial(logit)) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.3151 -0.2971 -0.2883 -0.2520 2.6326 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.20585 0.07808 -41.059 &lt;2e-16 *** ## x 0.22775 0.12107 1.881 0.0599 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1446.9 on 4411 degrees of freedom ## Residual deviance: 1443.3 on 4410 degrees of freedom ## AIC: 1447.3 ## ## Number of Fisher Scoring iterations: 6 How to run logistic regression in R? train &lt;- sample(c(TRUE, FALSE), nrow(music), replace = TRUE) mod &lt;- glm(y ~ x, family = binomial(link = &quot;logit&quot;), subset = train) #To use the model, we pass it to the predict function. We pass in the new data #We pass in the compliment of the subset we used (train = !test) #We also pass in response. #What is the probability that you think they are a 1 vs a 0 test_probs &lt;- predict(mod, newdata = data.frame(x = x[!train]), type = &quot;response&quot;) class(test_probs) ## [1] &quot;numeric&quot; #This should feel absolutely like it is 0 and not one hist(test_probs) #Round the numbers test_pred &lt;- test_probs &gt; 0.5 #Head the numbers head(test_pred) ## 1 2 3 4 5 6 ## FALSE FALSE FALSE FALSE FALSE FALSE #How good is my prediction #First, how many predications did we get correct same_as_data &lt;- as.numeric(test_pred) == y[!train] mean(same_as_data) ## [1] 0.9622054 #About 96 percent. Seems really good. Then we ask is the data unbalanced if you do you this for cancer in the general population for example. Could be bad if you have 96 percent accuracy We call this a classification error. The histogram should be a red flag. We are saying everything is 0, and then are just wrong when its 1. Iin this case, every song is not hip hop, and we are right 96% of the time. Y = 1 Y = 0 \\(\\hat{Y}\\) = 1 A B \\(\\hat{Y}\\) = 0 C D \\(\\frac{A}{A + C}\\) our recall! Of all the records that are “1”, how many are our model capturing \\(\\frac{B}{B + D}\\) our precision! Of all the records that are “1”, how many are our model capturing The amazing thing is that we can actually make one of these one! alpha &lt;- 0.5 test_pred &lt;- test_probs &gt; alpha truly1 &lt;- y[!train] ==1 called1 &lt;- as.numeric(test_pred) recall &lt;- sum(called1 &amp; truly1)/ sum(truly1) precision &lt;- sum(truly1 &amp; called1)/ sum(called1) recall ## [1] 0 precision ## [1] NaN Right now the recall is really bad: but lets make everything one and make our recall really good! alpha &lt;- 0 test_pred &lt;- test_probs &gt; alpha truly1 &lt;- y[!train] ==1 called1 &lt;- as.numeric(test_pred) recall &lt;- sum(called1 &amp; truly1)/ sum(truly1) precision &lt;- sum(truly1 &amp; called1)/ sum(called1) recall ## [1] 1 precision ## [1] 0.03779458 Now lets make a sequence! alphas &lt;- seq(0,1, length.out = 10000) recalls &lt;- rep(NA, length(alphas)) precisions &lt;- rep(NA, length(alphas)) for(i in seq_along(alphas)){ alpha &lt;- alphas[i] test_pred &lt;- test_probs &gt; alpha truly1 &lt;- y[!train] ==1 called1 &lt;- as.numeric(test_pred) recall &lt;- sum(called1 &amp; truly1)/ sum(truly1) precision &lt;- sum(truly1 &amp; called1)/ sum(called1) recall[i] &lt;- recall precision[i] &lt;- precision } #plot(recalls, precisions) What are the most popular “artist.terms” in the dataset? artists_sorted &lt;- sort(table(music$artist.terms), decreasing =T) artists_sorted[1] ## hip hop ## 171 Choose one “artist.terms” to predict for, then try running logistic regression vs usual lm() on all of the other variables, do they pick up different variables? What would your next steps be? logit &lt;- function(x){exp(x) / (1 + exp(x))} logistic_obj &lt;- function(params, X, y){ xbeta &lt;- cbind(1, X) %*% params p &lt;- logit(xbeta) return(-sum(dbinom(y, size=1, prob=p, log=TRUE))) } optim(c(1, 1), logistic_obj, X=x, y=y) ## $par ## [1] -3.2070659 0.2257483 ## ## $value ## [1] 721.6383 ## ## $counts ## function gradient ## 63 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL Imagine you’re doing fake news prediction, what metric(s) would you care more about the most? How would you recommend a target for these metrics for a company like Facebook? artist_cols &lt;- grepl(&quot;^artist&quot;, names(music)) my_mod &lt;- glm(y ~ ., data = music[,!artist_cols]) ols &lt;- lm(y ~ ., music[,!artist_cols]) We can plot this data and see the relationship! plot(summary(ols)$coefficients[, 4], summary(my_mod)$coefficients[, 4]) If we plot the coefficients, of logistic versus regression, against each other, you will get a very very strong relationship. Things that are significant in one will be significant in the other. If you only care about what features are important, there is very little difference between logistic and regression. Ultimately, when we are looking at the optimization, there is an xbeta term we created. It is just like in regression. Then you penalize by some magical calculation but eitherway it is all dependent on the xbeta term. In a sense, they are the same category of models. 4.3 Resampling Often with inbalanced data sets you want to upsample (or downsample). You can create or get rid of data to make the sample more proportionate. Alternatively you can Bootstrap the data. It is like asking what is the distribution of something we don’t know. We can use simulation to get a sense of what the distribution is (often used for uncertainty). However, we always need to avoid overfitting. It is important to also do cross-validation then. 4.3.1 Up-sampling Up-sampling It works, but it is important to remember you aren’t creating new data. You are essentially adding more weight to the already existing data. But it does help. 4.3.2 Bootstrapping If the data changes, how much will my model change? You have these questions about slightly different sets. So you create a similar bootstrap sample and do the same model on both. Then you look at the results. By collapsing across the samples (and doing more), you can see how sensitive the data is and how it changes with different inputs. Bootstrapping 4.3.3 Cross Validation When you generalize beyond the data set you have, how bad is your error? This is the problem of overfitting. You can’t generalize. Cross validation tries to tell you how bad of a problem you have. If you use tuning hyperparameters however, you can help fix this. You can leave part of the original set out to test on later. You switch the training set and validation set for all of the test. In general, when tuning for hyperparameters (like degrees of polynomials) you ahve to make a choice. For example earlier with polynomials we graphed it using validation and training. Once we pick the optimal hyperparameter from the training set tested on the validation set. Then we apply it onto the test set to get the generalization error. In short: - Cross validation between train/validation creates the hyperparameter. (i.e. degrees of polynomial) - Then you use all the training and validation data to get the parameters. (coefficients given the polynomial) - Then predict on the test data to get the generalization error. This should prevent the overfitting from the hyperparameter. If you reapply onto a test set that wasn’t used to help you pick this should tell you if there is a problem. There is some debate on double cross validation. You can then change the test set and revalidate. Some people argue why not? Our model should be more robust if we cycle through everything. Reasons not to: Computational. It is resource intesnive. You can also leak data and ruin you generalization error. 4.4 Initial Work df &lt;- read.csv(&quot;Datasets/small_non_retweets_dc_inaug_steal.csv&quot;) head(df,2) ## X created_at ## 1 36998 2021-01-20T17:19:45.000Z ## 2 38336 2021-01-21T13:43:51.000Z ## text ## 1 View of White House and Washington Monument #DC #WashingtonDC https://t.co/YxGxgocXsT ## 2 Fue un momento emotivo para @joebiden y su esposa antes de entrar a la residencia.\\n\\n@latinostimemagazine \\n\\n#InaugurationDay #Inauguration #president #usa #residenciapresidencial #washingtondc #latinostimemagazine https://t.co/fbqi7Mx1Md ## retweet_count reply_count like_count dc_flag inaug_flag steal_flag ## 1 0 0 1 True False False ## 2 0 0 0 True True False ## washington.dc biden sentomcotton white.house joebiden hillary amykremer ## 1 0 0 0 0.9123 0 0 0 ## 2 0 0 0 0.0000 1 0 0 ## realdonaldtrump trump peter.navarro fox.news biden.inauguration harris ## 1 0 0 0 0 0 0 ## 2 0 0 0 0 0 0 ## charles.curtis joe.biden kamala.harris jack senschumer sentedcruz ## 1 0 0 0 0 0 0 ## 2 0 0 0 0 0 0 ## whitehouse inauguration.day america rudygiuliani kloeffler perduesenate ## 1 0 0 0 0 0 0 ## 2 0 0 0 0 0 0 ## gasecofstate beschlossdc facebook twitter youtube d.c. patriots obama ## 1 0 0 0 0 0 0 0 0 ## 2 0 0 0 0 0 0 0 0 ## gop capitol joe ali speakerpelosi trumps ap u.s..capitol donald.trump ## 1 0 0 0 0 0 0 0 0 0 ## 2 0 0 0 0 0 0 0 0 0 ## scotland tedcruz washingtonpost oann pence senate president.trump god ## 1 0 0 0 0 0 0 0 0 ## 2 0 0 0 0 0 0 0 0 ## usatoday dnc democrats president.biden dc potus msnbc nbcnews thehill ## 1 0 0 0 0 0 0 0 0 0 ## 2 0 0 0 0 0 0 0 0 0 ## pelosi codemonkeyz jaketapper republicans china newsmax usa florida djt ## 1 0 0 0 0 0 0 0 0 0 ## 2 0 0 0 0 0 0 0 0 0 ## nancy.pelosi jesus secret.service rsbnetwork lindseygrahamsc azgop ## 1 0 0 0 0 0 0 ## 2 0 0 0 0 0 0 ## secpompeo republican cnn gatewaypundit yahoo vice.president americans ## 1 0 0 0 0 0 0 0 ## 2 0 0 0 0 0 0 0 ## llinwood briankempga jackposobiec foxnews texas realmattcouch ## 1 0 0 0 0 0 0 ## 2 0 0 0 0 0 0 ## seanhannity ingrahamangle congress ossoff mayorbowser ## 1 0 0 0 0 0 ## 2 0 0 0 0 0 ## united.states.of.america kamala jimmy.carter covid u.s. cbsnews vp ## 1 0 0 0 0 0 0 0 ## 2 0 0 0 0 0 0 0 ## randpaul gopleader tuckercarlson senronjohnson donald senatemajldr ## 1 0 0 0 0 0 0 ## 2 0 0 0 0 0 0 ## kamalaharris iraq us kayleighmcenany pentagon republican.party ## 1 0 0 0 0 0 0 ## 2 0 0 0 0 0 0 ## tomfitton mtgreenee dems thedemocrats georgia housegop senategop ## 1 0 0 0 0 0 0 0 ## 2 0 0 0 0 0 0 0 ## senatorromney fbi iran barackobama jim_jordan ivanka donwinslow ## 1 0 0 0 0 0 0 0 ## 2 0 0 0 0 0 0 0 ## sethabramson airbnb abc google senkamalaharris united.states washington ## 1 0 0 0 0 0 0 0 ## 2 0 0 0 0 0 0 0 ## president.donald.trump hawleymo richardgrenell gopchairwoman ## 1 0 0 0 0 ## 2 0 0 0 0 ## projectlincoln flotus nytimes parlertakes marcorubio sendavidperdue ## 1 0 0 0 0 0 0 ## 2 0 0 0 0 0 0 ## senatorloeffler jennaellisesq garepublicans drbiden donaldjtrumpjr ## 1 0 0 0 0 0 ## 2 0 0 0 0 0 ## repmattgaetz tomilahren johncornyn mariabartiromo chuckgrassley aoc ## 1 0 0 0 0 0 0 ## 2 0 0 0 0 0 0 ## catturd2 sexcounseling danscavino sidneypowell1 mike_pence sebgorka ## 1 0 0 0 0 0 0 ## 2 0 0 0 0 0 0 ## senrickscott repkinzinger democrat epochtimes ivankatrump nypost ## 1 0 0 0 0 0 0 ## 2 0 0 0 0 0 0 ## breitbartnews lisamurkowski sentoomey cnnbrk cia cbs nbc sensanders ## 1 0 0 0 0 0 0 0 0 ## 2 0 0 0 0 0 0 0 0 ## genflynn mcconnell blm erictrump amyklobuchar berniesanders lord ## 1 0 0 0 0 0 0 0 ## 2 0 0 0 0 0 0 0 ## cnnpolitics russia laurenboebert senatedems wusa9 hbwx tenacioustopper ## 1 0 0 0 0 0 0 0 ## 2 0 0 0 0 0 0 0 ## miriweather washington.d.c. アメリカ washingtondc dctogogo canada ## 1 0 0 0 0 0 0 ## 2 0 0 0 0 0 0 ## reuters antifa virginia statedept wsj npr bbcworld ## 1 0 0 0 0 0 0 0 ## 2 0 0 0 0 0 0 0 ## the.rhythm.and.blues.presidential.inaugural.ball randbreloaded ## 1 0 0 ## 2 0 0 ## mike.pence dcpolicedept us.capitol maryland thejusticedept election.day ## 1 0 0 0 0 0 0 ## 2 0 0 0 0 0 0 ## india national.guard president acosta bideninaugural transition46 ## 1 0 0 0 0 0 0 ## 2 0 0 0 0 0 0 ## politico narendramodi hillaryclinton estados.unidos amazon ## 1 0 0 0 0 0 ## 2 0 0 0 0 0 ## secretservice george.w..bush andrewfeinberg dhsgov michelleobama bernie ## 1 0 0 0 0 0 0 ## 2 0 0 0 0 0 0 ## dmvblacklives fbiwfo nationalguard guardia.nacional capitol.hill ## 1 0 0 0 0 0 ## 2 0 0 0 0 0 ## national.mall capitol.police bernie.sanders presssec nbc.news ## 1 0 0 0 0 0 ## 2 0 0 0 0 0 ## disclosetv u.s.capitol.building afghanistan browns steven.m..d.antuono ## 1 0 0 0 0 0 ## 2 0 0 0 0 0 ## wapo marvel lady.gaga ustreasury jrpsaki リンカーン大統領.新聞社300社 ## 1 0 0 0 0 0 0 ## 2 0 0 0 0 0 0 ## 選挙クーデター hotnews8net amanda.gorman theamandagorman ## 1 0 0 0 0 ## 2 0 0 0 0 Let’s understand the flags, like steal_flag. df$steal_flag &lt;- as.numeric(df$steal_flag == &quot;True&quot;) mean(df$steal_flag) ## [1] 0.01701149 We are turning true or false into 0 or 1. This flag is rare. It is about 1.7 percent. Steal means if the word “steal” was present in a tweet. We can examine some of them for example by using: head(df[df$steal_flag &gt; 0, &quot;text&quot;], 4) ## [1] @Rob_Noorollah @Christi76673318 Pick up a copy of Biden&#39;s new book... \\n\\n&quot;The Art of the Steal&quot;\\n\\n#StopTheSteaI2020 ## [2] @SraBlockerFV @EFtours What would be great is if you ddn’t steal $1000 from my kids after you candled their tour! #StopTheSteaI2020 #TheMoreYouKnow #WakeUpSlowM ## [3] #SecretService is gonna get you for revealing @realDonaldTrump&#39;s codename! \\n\\n#FightBack PATRIOTS \\n#StopTheSteal \\n#4MoreYears \\n#Jan6GuestHouse \\n#Jan6Rides \\n#WashingtonDC https://t.co/P9NkJQ5KLI ## [4] Me, heading to Tampa to Stop Brady’s Steal. Saints won in a landslide. We need to take our NFL back! #StopTheSteaI2020 https://t.co/OTL8nADgAY ## 4279 Levels: ¿Qué hacía ayer @realDonaldTrump mientras tomaban el Capitolio en #WashingtonDC? \\n\\nEn las imágenes se aprecia a Trump y su hija @IvankaTrump observando diversas pantallas que mostraban que lo ocurría en Washington DC, mientras en el fondo se escucha la canción &#39;Gloria&#39;. https://t.co/i3Cyhc0mTF ... 4.5 A basic model with upsampling We can create a new data object that stores only the indices of the places where steal_flag exists. pos_ind &lt;- which(df$steal_flag &gt; 0) head(pos_ind) ## [1] 28 91 114 171 176 285 So in this case, tweets 28, 91, 114… etc have the word steal. Let’s sample the indices now. This requires us to figure out what we want to upsample our data to. To do so we will make N 10 times the length of our current amount of steal_flags. What we are doing is artificially increasing our data set. Upsampling is somewhat of a hack in the sense. It makes the objective function give more weight to the positive yet rare cases in our data. It is sorta dirty but also helpful, especially for preliminary exploration. There are other ways to be better at weighing certain aspects. Note: you can also downsample the data. However, this is much more rare. It should have a very similar affect but usually you don’t want to throw data away! N &lt;- length(pos_ind)*10 sample_ind &lt;- sample(pos_ind, N, replace = TRUE) table(sample_ind) ## sample_ind ## 28 91 114 171 176 285 311 364 429 509 636 719 789 821 ## 9 11 15 13 11 14 8 12 7 5 11 5 9 8 ## 835 951 1063 1120 1339 1346 1401 1523 1561 1580 1607 1826 1854 1963 ## 13 12 15 10 11 6 5 7 5 9 10 11 7 10 ## 2050 2063 2082 2193 2244 2326 2453 2465 2578 2701 2702 2853 2927 2949 ## 8 16 14 6 11 8 5 14 12 8 8 11 7 9 ## 3084 3147 3198 3281 3297 3311 3387 3424 3537 3543 3554 3574 3577 3615 ## 8 10 8 8 11 14 13 11 12 7 3 11 13 15 ## 3678 3686 3699 3723 3740 3755 3791 3868 3894 3923 3946 3955 3984 4032 ## 11 13 9 14 8 11 15 9 10 10 10 14 7 10 ## 4076 4125 4136 4262 ## 7 12 12 8 This table shows the number of times we get each entry. The above number is the specific record and below it is the number of times. So let’s create a new dataframe. We want to grab the particular indices from the above dataframe multiple times. Something like this seems intuitive. new_df &lt;- df[sample_ind,] However there is a major problem! dim(new_df) ## [1] 740 259 dim(df) ## [1] 4350 259 We lost a lot of cases! In fact, we threw away all of the negative cases. So instead, we need to stack the dataframes together. new_df &lt;- rbind(df[sample_ind, ], df) dim(new_df) ## [1] 5090 259 Note the features themselves haven’t changed at all. Instead, the proportion of the features have changed. There we go. Also note we did not overwrite df, we created a new dataframe. This is good practice in functional programming. Now we can do things like linear regression! ols &lt;- lm(steal_flag ~ trump + capitol.police, data = new_df) summary(ols) ## ## Call: ## lm(formula = steal_flag ~ trump + capitol.police, data = new_df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.1672 -0.1672 -0.1672 -0.1141 0.8862 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.167170 0.005525 30.259 &lt; 2e-16 *** ## trump -0.053420 0.015169 -3.522 0.000433 *** ## capitol.police -0.305764 0.493761 -0.619 0.535777 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3662 on 5087 degrees of freedom ## Multiple R-squared: 0.002501, Adjusted R-squared: 0.002109 ## F-statistic: 6.377 on 2 and 5087 DF, p-value: 0.001714 Regressing with trump and capitol police, it appears capitol police isn’t significant but trump is. That is his slogan so it shouldn’t be too surprising but there it is. How to mix upsample and also cross validate can only be done one way. You don’t want to upsample first! If you did that, you could end up with the same record in both the train and test. Then you would be too confident in those cases. "],["linear-model-selection.html", "Chapter 5 Linear Model Selection 5.1 Regression requires more data than features 5.2 Lasso 5.3 simulating collinearity + sparsity 5.4 Demo on glmnet functionalities 5.5 Principal Component Analysis 5.6 Typical machine learning approach 5.7 What we would do in data mining 5.8 Principal Component Analysis Applied! 5.9 PCA on weather data example 5.10 Different noramlizations", " Chapter 5 Linear Model Selection 5.1 Regression requires more data than features library(glmnet) import the dataset df &lt;- read.csv(&quot;Datasets/non_retweets_dc_inaug_steal.csv&quot;) Down-sampling for faster processing samp_ind &lt;- sample(nrow(df), 8000) df &lt;- df[samp_ind,] Combining the different flags steal_index &lt;- grep(&quot;steai&quot;, names(df)) names(df)[steal_index] ## [1] &quot;X.stopthesteai&quot; &quot;X.stopthesteai2020&quot; &quot;X.stopthesteai2021&quot; df[, &quot;X.stopthesteai2020&quot;] &lt;- ( df[, &quot;X.stopthesteai2020&quot;] + df[, &quot;X.stopthesteai&quot;] + df[, &quot;X.stopthesteai2021&quot;]) df &lt;- df[, -grep(&quot;(X.stopthesteai$|X.stopthesteai2021$)&quot;, names(df))] Removing features that are not numeric text &lt;- df[, grep(&quot;tweet_body&quot;, names(df))] df &lt;- df[,-grep(&quot;(tweet_body|created_at)&quot;, names(df))] names(df)[grep(&quot;_&quot;, names(df))] ## [1] &quot;X.washington_dc&quot; &quot;X.washington_dcprotest&quot; ## [3] &quot;X.washington_dcriot&quot; &quot;X.washington_dcwx&quot; ## [5] &quot;X.jim_jordan&quot; &quot;X.mike_p&quot; ## [7] &quot;X.washington_dc.1&quot; &quot;like_count&quot; ## [9] &quot;reply_count&quot; &quot;retweet_count&quot; ## [11] &quot;washington_dc&quot; head(df$created_at) ## NULL df &lt;- df[, sapply(df, class) == &quot;numeric&quot;] Fitting the model y_ind &lt;- grep(&quot;X.stopthesteai2020&quot;, names(df)) x &lt;- as.matrix(df[, -y_ind]) x &lt;- scale(x) y &lt;- as.numeric(df[, y_ind]) #bad_data &lt;- which(is.na(x[, colnames(x) == &quot;xinaugur&quot;])) bad_data &lt;- which(is.na(x[, colnames(x) == &quot;zone&quot;])) x &lt;- x[-bad_data,] y &lt;- y[-bad_data] table(sapply(df,class)) ## ## numeric ## 835 names(df)[sapply(df,class) == &quot;character&quot;] ## character(0) sum(apply(x,2, function(x) mean(is.na(x) &gt; 0))) ## [1] 0 Examine the data head(x[,colnames(x)==&quot;zone&quot;]) ## 948 10785 13779 4594 14969 5407 ## -0.05598926 -0.05598926 -0.05598926 -0.05598926 -0.05598926 -0.05598926 tail(x[,colnames(x)==&quot;zone&quot;]) ## 11866 11289 5095 804 9381 6359 ## -0.05598926 -0.05598926 -0.05598926 -0.05598926 -0.05598926 -0.05598926 head(y) ## [1] 0 0 0 0 1 0 Run the model ols &lt;- lm(y ~ x) ols_coeffs &lt;- summary(ols)$coefficients To do data mining with p-values, do not treat them like probabilities, but you can use them as a metric to guide you. length(ols$coefficients) ## [1] 835 p_ord &lt;- order(ols_coeffs[, 4]) rownames(ols_coeffs)[head(p_ord, 10)] ## [1] &quot;(Intercept)&quot; &quot;xX.washington_dc&quot; ## [3] &quot;xinaugur&quot; &quot;xX.washington_dc.1&quot; ## [5] &quot;xX.electionintegr&quot; &quot;xX.maga&quot; ## [7] &quot;xX.realdonaldtrump&quot; &quot;xfraud&quot; ## [9] &quot;xX.trump2020tosaveamerica&quot; &quot;xX.maga2020&quot; 5.2 Lasso lasso_cv &lt;- cv.glmnet(x, y, alpha=1, nfolds = 5, intercept=TRUE) plot(lasso_cv) abline(v=log(lasso_cv$lambda.min), col=&quot;blue&quot;) abline(v=log(lasso_cv$lambda.1se), col=&quot;green&quot;) lasso.fits &lt;- lasso_cv$glmnet.fit lasso_cv$lambda[which.min(lasso_cv$cvm)] ## [1] 0.004118487 lasso_cv$lambda.min ## [1] 0.004118487 lambda_ind &lt;- which.min(lasso_cv$cvm) plot( ols$coefficients[-1], lasso_cv$glmnet.fit$beta[, lambda_ind], xlab = &quot;OLS Coeffs&quot;, ylab = &quot;LASSO Coeffs&quot;, xlim = c(-.1, 0.1), ylim = c(-.05, 0.05) ) abline(h = 0) abline(a = 0, b = 1) lasso_coeffs &lt;- lasso_cv$glmnet.fit$beta[, lambda_ind] lasso_coeffs[abs(lasso_coeffs) &gt; 0.05] ## X.washington_dc X.washington_dc.1 inaugur ## -0.27870638 -0.06757916 -0.26022122 hist(lasso_cv$glmnet.fit$beta[, lambda_ind]) 5.3 simulating collinearity + sparsity sample_size &lt;- 1000 p &lt;- 100 useful_p &lt;- 50 # number of corr features collin_p &lt;- 50 useful_ind &lt;- sample(p, useful_p) corr_ind &lt;- sample(p, collin_p) # independent variables z &lt;- rnorm(sample_size) corrs &lt;- rep(0, p) corrs[corr_ind] &lt;- runif(collin_p, 0.3, 0.9) x &lt;- sapply(corrs, function(corr) corr * z + (1 - corr) * rnorm(sample_size)) noise &lt;- rnorm(sample_size) #first option: generate y according to z #y &lt;- 2 * z + noise #second option create a beta that defaults to beta (useless) and for some unknown #locations of beta, assign random useful coefficients. beta &lt;- rep(0, p) beta[useful_ind] &lt;- runif(useful_p, -1.2, 1.2) #matrix multiplication to get Y: most beta is zero and then add noise on top y &lt;- x %*% beta + noise ols &lt;- lm(y ~ x) ols_summ &lt;- summary(ols)$coefficients rownames(ols_summ)[ols_summ[,4] &lt; 0.05] ## [1] &quot;x3&quot; &quot;x7&quot; &quot;x9&quot; &quot;x11&quot; &quot;x13&quot; &quot;x15&quot; &quot;x16&quot; &quot;x18&quot; &quot;x19&quot; &quot;x20&quot; ## [11] &quot;x22&quot; &quot;x23&quot; &quot;x27&quot; &quot;x28&quot; &quot;x31&quot; &quot;x35&quot; &quot;x39&quot; &quot;x41&quot; &quot;x42&quot; &quot;x44&quot; ## [21] &quot;x45&quot; &quot;x48&quot; &quot;x49&quot; &quot;x51&quot; &quot;x53&quot; &quot;x54&quot; &quot;x56&quot; &quot;x59&quot; &quot;x60&quot; &quot;x63&quot; ## [31] &quot;x65&quot; &quot;x66&quot; &quot;x67&quot; &quot;x71&quot; &quot;x72&quot; &quot;x74&quot; &quot;x75&quot; &quot;x78&quot; &quot;x79&quot; &quot;x80&quot; ## [41] &quot;x81&quot; &quot;x83&quot; &quot;x85&quot; &quot;x87&quot; &quot;x89&quot; &quot;x90&quot; &quot;x93&quot; &quot;x96&quot; &quot;x97&quot; &quot;x99&quot; ## [51] &quot;x100&quot; length(rownames(ols_summ)[ols_summ[,4]&lt;0.05]) ## [1] 51 cols &lt;- rep(&#39;black&#39;, p) cols[corr_ind] &lt;- &quot;red&quot; pchs &lt;- rep(1, p) pchs[useful_ind] &lt;- 16 plot(ols$coefficients[-1], beta, pch=pchs, col=cols) abline(a=0, b=1) 5.4 Demo on glmnet functionalities library(glmnet) lasso_cv &lt;- cv.glmnet(cbind(x, y), y, alpha=1) coef(lasso_cv) ## 102 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## 1 ## (Intercept) -0.003548851 ## V1 . ## V2 . ## V3 . ## V4 . ## V5 . ## V6 . ## V7 . ## V8 . ## V9 . ## V10 . ## V11 . ## V12 . ## V13 . ## V14 . ## V15 . ## V16 . ## V17 . ## V18 . ## V19 . ## V20 . ## V21 . ## V22 . ## V23 . ## V24 . ## V25 . ## V26 . ## V27 . ## V28 . ## V29 . ## V30 . ## V31 . ## V32 . ## V33 . ## V34 . ## V35 . ## V36 . ## V37 . ## V38 . ## V39 . ## V40 . ## V41 . ## V42 . ## V43 . ## V44 . ## V45 . ## V46 . ## V47 . ## V48 . ## V49 . ## V50 . ## V51 . ## V52 . ## V53 . ## V54 . ## V55 . ## V56 . ## V57 . ## V58 . ## V59 . ## V60 . ## V61 . ## V62 . ## V63 . ## V64 . ## V65 . ## V66 . ## V67 . ## V68 . ## V69 . ## V70 . ## V71 . ## V72 . ## V73 . ## V74 . ## V75 . ## V76 . ## V77 . ## V78 . ## V79 . ## V80 . ## V81 . ## V82 . ## V83 . ## V84 . ## V85 . ## V86 . ## V87 . ## V88 . ## V89 . ## V90 . ## V91 . ## V92 . ## V93 . ## V94 . ## V95 . ## V96 . ## V97 . ## V98 . ## V99 . ## V100 . ## V101 0.970849469 lasso_mod &lt;- glmnet(x, y, alpha=1, lambda=lasso_cv$lambda.1se) coef(lasso_mod) ## 101 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## s0 ## (Intercept) -0.01439717 ## V1 . ## V2 . ## V3 0.63016054 ## V4 . ## V5 . ## V6 . ## V7 -1.08611720 ## V8 . ## V9 . ## V10 . ## V11 . ## V12 . ## V13 -0.78424070 ## V14 . ## V15 1.00119944 ## V16 -0.12560558 ## V17 . ## V18 -0.43357503 ## V19 . ## V20 -0.22167606 ## V21 . ## V22 0.54286505 ## V23 . ## V24 . ## V25 . ## V26 . ## V27 0.95492277 ## V28 . ## V29 . ## V30 . ## V31 -0.02404034 ## V32 . ## V33 . ## V34 . ## V35 0.21629061 ## V36 . ## V37 . ## V38 . ## V39 . ## V40 . ## V41 -0.77218548 ## V42 0.63466208 ## V43 . ## V44 -0.33669665 ## V45 -0.01385245 ## V46 . ## V47 . ## V48 -0.72823984 ## V49 0.12356610 ## V50 . ## V51 . ## V52 . ## V53 . ## V54 . ## V55 . ## V56 0.60795498 ## V57 . ## V58 . ## V59 . ## V60 0.95210946 ## V61 . ## V62 . ## V63 0.73570502 ## V64 . ## V65 -0.97188025 ## V66 -0.67234579 ## V67 . ## V68 . ## V69 . ## V70 . ## V71 . ## V72 . ## V73 . ## V74 . ## V75 0.92864619 ## V76 . ## V77 . ## V78 0.62450119 ## V79 0.75367468 ## V80 -0.78590281 ## V81 0.61632560 ## V82 . ## V83 0.46600120 ## V84 . ## V85 0.93391607 ## V86 . ## V87 . ## V88 . ## V89 0.19659323 ## V90 -0.92332183 ## V91 . ## V92 . ## V93 . ## V94 . ## V95 . ## V96 0.43893157 ## V97 . ## V98 . ## V99 . ## V100 -0.62323217 sample_size &lt;- 1000 p &lt;- 100 useful_p &lt;- 50 # number of corr features collin_p &lt;- 50 useful_ind &lt;- sample(p, useful_p) corr_ind &lt;- sample(p, collin_p) # independent variables z &lt;- rnorm(sample_size) corrs &lt;- rep(0, p) corrs[corr_ind] &lt;- runif(collin_p, 0.3, 0.9) x &lt;- sapply(corrs, function(corr) corr * z + (1 - corr) * rnorm(sample_size)) noise &lt;- rnorm(sample_size) # y &lt;- 2 * z + noise beta &lt;- rep(0, p) beta[useful_ind] &lt;- runif(useful_p, -1.2, 1.2) y &lt;- x %*% beta + noise sim_num &lt;- 100 beta_mse &lt;- matrix(NA, ncol=3, nrow=sim_num) z_coeffs &lt;- matrix(NA, ncol=3, nrow=sim_num) for(i in seq_len(sim_num)){ if(i %% 10 == 0){ print(i) } noise &lt;- rnorm(sample_size) # y &lt;- 2 * z + noise y &lt;- x %*% beta + noise ols &lt;- lm(y ~ x) lasso_cv &lt;- cv.glmnet(x, y, alpha=1) ridge_cv &lt;- cv.glmnet(x, y, alpha=0) beta_mse[i, 1] &lt;- mean((beta - ols$coefficients[-1])^2) beta_mse[i, 2] &lt;- mean((beta - coef(lasso_cv)[-1])^2) beta_mse[i, 3] &lt;- mean((beta - coef(ridge_cv)[-1])^2) } ## [1] 10 ## [1] 20 ## [1] 30 ## [1] 40 ## [1] 50 ## [1] 60 ## [1] 70 ## [1] 80 ## [1] 90 ## [1] 100 boxplot(beta_mse) abline(h=2) plot(lasso_cv) sim_num &lt;- 100 z_coeffs &lt;- matrix(NA, ncol=3, nrow=sim_num) for(i in seq_len(sim_num)){ if(i %% 10 == 0){ print(i) } noise &lt;- rnorm(sample_size) y &lt;- 2 * z + noise #y &lt;- x %*% ols &lt;- lm(y ~ x + z) lasso_cv &lt;- cv.glmnet(x, y, alpha=1) ridge_cv &lt;- cv.glmnet(x, y, alpha=0) #z_coeffs[i, 1] &lt;- tail(ols$coefficients, 1) #z_coeffs[i, 2] &lt;- tail(coef(lasso_cv), 1)[1, 1] #z_coeffs[i, 3] &lt;- tail(coef(ridge_cv), 1)[1, 1] #if we aren&#39;t using z, maybe we should look at prediction accuracy #you could also use y according to the beta value instead #beta_mse[i,1] &lt;- mean((beta - ols$coefficients[-1])^2) #drops the intercept term } ## [1] 10 ## [1] 20 ## [1] 30 ## [1] 40 ## [1] 50 ## [1] 60 ## [1] 70 ## [1] 80 ## [1] 90 ## [1] 100 #boxplot(z_coeffs) #abline(h=2) 5.5 Principal Component Analysis Sparcity: Inbalanced data: 5.5.1 Principle Component Analysis hidden_p &lt;- 5 observ_p &lt;- 30 prob &lt;- NULL # runif(hidden_p) h2o &lt;- sample(hidden_p, #hidden to observed observ_p, replace=TRUE, prob=prob) h2o &lt;- sort(h2o) sample_size &lt;- 1000 hidden_z &lt;- sapply( seq_len(hidden_p), function(x) rnorm(sample_size)) corrs &lt;- runif(observ_p, 0.3, 0.8) #create five groups of colinear stuff observ_x &lt;- mapply( function(i, corr) { hidden_z[, i] * corr + rnorm(sample_size) * (1 - corr) *100 }, h2o, corrs) #observ_x is what you often see, but there is still hidden stuff image(cor(observ_x)) #This looks weird to what we expect. It doesn&#39;t look like five groups #if we sort instead it works h2o &lt;- sample(hidden_p, #hidden to observed observ_p, replace=TRUE, prob=prob) sample_size &lt;- 1000 hidden_z &lt;- sapply( seq_len(hidden_p), function(x) rnorm(sample_size)) #This effects how things are correlated! corrs &lt;- runif(observ_p, 0.3, 0.8) #create five groups of colinear stuff observ_x &lt;- mapply( function(i, corr) { hidden_z[, i] * corr + rnorm(sample_size) * (1 - corr) }, h2o, corrs) #observ_x is what you often see, but there is still hidden stuff image(cor(observ_x)) beta &lt;- runif(hidden_p, -10, 10) noise &lt;- rnorm(sample_size, sd=10) #hard to measure hidden forces! #we can only measure x, but x is only correlated to hidden stuff y &lt;- hidden_z %*% beta + noise df &lt;- data.frame(y, observ_x) #y depends on the hidden stuff not x Maybe there is a hidden gene inside of you that makes you sick. We can’t (yet) measure that hidden gene. But we can measure symptoms and things like your heart rate. This should be correlated. 5.6 Typical machine learning approach #training data set, first 800 points (80 percent) train &lt;- 1:800 ols &lt;- lm(y ~ ., df, subset=train) length(ols$residual) #correct length ## [1] 800 #predict on points we didn&#39;t use to train ols_pred &lt;- predict(ols, df[-train,]) #error: differrence between measured values against predict. Mean of this squared MSE! mean((df$y[-train] - ols_pred)^2) ## [1] 119.6959 #run ols #PCA TIME #only input X, (feature matrix) into the PCA function pr_out &lt;- prcomp(observ_x, scale=FALSE) #scale is used because Xs might not be in the same unit, so mean = 0, sd = 1 class(pr_out) ## [1] &quot;prcomp&quot; #it has its own class names(pr_out) ## [1] &quot;sdev&quot; &quot;rotation&quot; &quot;center&quot; &quot;scale&quot; &quot;x&quot; #squared value of sdev is eigenvalue eigen_val &lt;- pr_out$sdev^2 #cumulative sum as a fraction of total eigenvalues plot(cumsum(eigen_val) / sum(eigen_val)) abline(h=.9) #here after the 5th point, the slope tapers off! This is directly related to the hidden_p value at the beginning. It should help show you how many important hidden features there are. #it is the percent of variabilitiy caputured by the first k components #If you don&#39;t know what to choose, 90% variability is a good point plot(pr_out$sdev) #Very similar, but not as interpretable as percent of variability. #These steps is how k is chosen. #K is the dimension of W. Data came n x p. We need to shrink it to k. If you don&#39;t have #a clear cut, use 90% #we don&#39;t want to keep all the variability because not all features provide useful #information. Some of them are so colinear, they just add noise. cutoff &lt;- 5 #now we are looking at x #only pull out first k columns dim(pr_out$x) ## [1] 1000 30 dim(observ_x) ## [1] 1000 30 #these will be the same, but we choose a cutoff. W &lt;- pr_out$x[, 1:cutoff] df_w &lt;- data.frame(y, W) #PCA doesn&#39;t have to be lm. It could be ridge or lasso too #should be like the ols from above pca &lt;- lm(y ~ ., df_w, subset=train) #same prediction pca_pred &lt;- predict(pca, df_w[-train,]) #prediction error mean((df_w$y[-train] - pca_pred)^2) ## [1] 112.9948 #that was the classic view of PCA 5.7 What we would do in data mining k &lt;- 3 plot(pr_out$rotation[,k]) abline(h = 0) which(abs(pr_out$rotation[, k]) &gt; 0.2) ## [1] 9 10 11 12 18 23 #what is the truth we should be comparing to? pr_out &lt;- prcomp(observ_x) test_x &lt;- scale(observ_x, scale = FALSE) %*% pr_out$rotation pr_out$x ## PC1 PC2 PC3 PC4 ## [1,] 0.2958470397 0.2611423484 -1.071159033 -0.007153724 ## [2,] -2.0138562679 -0.7219724348 -0.706533223 0.926311897 ## [3,] -0.6549906757 -0.4907270993 1.332960292 -3.025254335 ## [4,] -0.5909699698 0.3291390501 -1.055727950 -0.437291336 ## [5,] 2.2224722210 1.2652430223 1.121896123 0.529088169 ## [6,] 3.0250596423 -0.4599698268 1.364695344 1.443923997 ## [7,] 2.3763278984 1.9805418864 -0.093394314 1.321748902 ## [8,] 1.6665349752 2.5995367230 -0.699635587 -1.532046172 ## [9,] -1.2824345419 1.6125604575 -2.242392370 -0.538110360 ## [10,] 1.6034029038 -2.3227010131 0.720323919 0.459272914 ## [11,] 1.0754217585 -0.4515726590 -2.153096881 -0.317474533 ## [12,] 3.8481670719 -2.5827833822 -0.367539127 -1.663281861 ## [13,] -0.4905189438 2.4074224813 0.849042692 0.782136248 ## [14,] 1.8612734930 -0.5087522776 2.157327121 3.883584410 ## [15,] 2.7192364516 0.8816139210 -0.762231652 -0.801808909 ## [16,] 1.2495982959 -0.9968587583 -2.050494652 -0.489441340 ## [17,] 1.0199167160 -0.2347963188 -1.485991897 0.473296025 ## [18,] -0.9439460077 0.6547271933 -1.019061107 0.066894049 ## [19,] -0.6001873073 -0.0179643547 -0.489042692 1.975795629 ## [20,] 3.0592526472 -3.1889557847 0.636991259 0.539916034 ## [21,] -2.3248899357 0.0344096842 1.749412483 -1.052815229 ## [22,] -0.0556518486 2.2821401096 2.476261819 0.289555907 ## [23,] -1.2953332843 0.2957649234 -1.120083566 -0.240297074 ## [24,] -1.1014204515 -0.1096159752 -1.739649770 0.897722914 ## [25,] 1.4875319291 -2.8793583588 1.065102099 -1.217430996 ## [26,] 3.8391758393 0.9780345502 -3.000042512 0.002165112 ## [27,] -1.5446137716 -2.8553876072 1.519011173 0.295680720 ## [28,] 2.0224678890 -0.2908957398 1.908730628 -2.214683224 ## [29,] -2.0866977743 -0.1746113971 -2.221982330 -0.550148099 ## [30,] 2.5454296346 -0.9814788662 0.450517166 0.311046353 ## [31,] -4.2350725469 0.6159541789 -0.625512809 1.454519840 ## [32,] 0.8727938183 1.7614366376 0.444435619 -0.540569488 ## [33,] -2.5309186895 2.1383531641 -1.663333954 -0.433504532 ## PC5 PC6 PC7 PC8 PC9 ## [1,] 1.0995247959 -0.147144713 -1.180851744 -0.074582401 -0.802019736 ## [2,] 2.3610468186 1.087102168 0.173123710 -0.945197361 -0.590679256 ## [3,] -0.2175928853 0.222637953 0.644177546 1.170161445 -1.166104069 ## [4,] -0.0847581451 0.200788139 0.122824928 0.206648570 -1.198853832 ## [5,] -0.6343586016 -0.865231978 -1.117684817 0.007911455 1.117324210 ## [6,] 0.2477126830 0.063727121 0.262519658 1.451928576 -0.649572209 ## [7,] -2.1639479763 0.456530092 -0.160151549 -0.672605339 0.245833015 ## [8,] -0.0925113145 -0.969462168 0.933274618 -0.935168162 -0.780909755 ## [9,] 2.0082222719 -0.847405165 -0.097387348 0.187409394 -0.003273817 ## [10,] -0.6455228162 -0.021543254 -0.239161676 0.148029761 -0.929989306 ## [11,] 0.4033603820 0.316104861 -0.089731353 0.628684350 -0.621774840 ## [12,] -1.0315126485 0.794384504 1.100558234 -0.287370455 0.212370607 ## [13,] -0.0092621676 0.248083181 0.159419292 -0.165054886 -0.120875707 ## [14,] -1.1674972738 -0.081458865 0.079293039 -0.153056623 0.514533785 ## [15,] 0.7160194011 0.376530213 -0.646997222 0.362879079 0.214558053 ## [16,] 0.4825273167 1.010691064 0.493837155 0.084917631 1.200065779 ## [17,] 0.2895444387 0.308355158 0.239458418 0.411951169 0.881906776 ## [18,] 1.0481751468 1.204655571 -0.093833024 0.569036788 0.346788657 ## [19,] -0.9086402741 -0.392470176 1.725231102 0.087939735 -0.995121148 ## [20,] -1.3363649167 -0.286251946 0.859263937 -0.702354554 -1.088810078 ## [21,] -2.4154430991 -0.215738464 -0.417752356 -0.734856092 -0.175109717 ## [22,] 1.4879908026 -0.716777626 0.087591090 -1.162863026 -0.592853233 ## [23,] 0.4326879365 0.174419351 -0.097926979 -0.289393807 -0.767932393 ## [24,] -0.9726118994 -1.642273300 -0.012621794 0.195615940 0.046731492 ## [25,] 1.7765709497 -0.027346464 0.470427789 -0.858898917 0.301532176 ## [26,] 0.1701717407 0.014859597 0.679047259 0.141523269 0.196945034 ## [27,] 0.7293161921 -1.371961237 0.514614457 0.804733156 0.479063921 ## [28,] 0.1947009748 -0.581981806 -0.608349726 -0.020611216 0.179308316 ## [29,] -0.3315187544 1.138462342 0.786084954 -0.145309415 -0.136760586 ## [30,] 1.5869389431 0.582620813 1.147743947 0.781396198 -0.129963738 ## [31,] 0.4049502486 0.546011615 1.842000361 -1.174971151 0.550044635 ## [32,] -0.0603608543 0.069342837 0.468662606 0.740440807 -0.950651509 ## [33,] 1.3580095672 0.397281015 0.510861099 0.100025483 0.836885571 ## PC10 PC11 PC12 PC13 ## [1,] 0.5437753062 -0.7836365303 0.479929028 -0.1207723178 ## [2,] 0.3589887545 0.9645661552 0.790479081 -0.4648002280 ## [3,] -0.1830775549 -0.0545967588 0.262957140 -0.0217880234 ## [4,] -0.2303124394 0.8385495851 1.034324001 0.1389010236 ## [5,] -0.6995117253 0.3046488620 1.196728741 0.1049148465 ## [6,] 0.1928991785 1.0301118179 0.510055356 -0.7755728459 ## [7,] -1.1981059738 0.2153719959 -0.332123114 0.8757114313 ## [8,] 0.1853046835 0.1659908777 -0.107726361 0.8455911908 ## [9,] 0.1810606656 -0.2718652117 -0.212688706 0.2118349717 ## [10,] -0.5324283318 -0.7048769326 0.506169023 -0.3788605877 ## [11,] 0.8972704255 -0.5570703232 -0.510007769 0.4916853844 ## [12,] -0.8130658332 0.2687734256 0.140669817 1.6133789109 ## [13,] -0.2340058438 -1.4552936772 0.236187130 -0.2484726396 ## [14,] 0.0894594596 -0.4141361624 -0.184453893 -0.4857055100 ## [15,] 0.1376340707 -0.2895517796 -0.463272395 -0.0409899407 ## [16,] 0.0231082647 -0.5551582455 -0.999857081 0.6350866319 ## [17,] -1.3262984960 -0.5943385256 0.600277990 -0.4079729384 ## [18,] 0.5122940368 0.5370254582 -0.692335397 -0.3770983366 ## [19,] 0.9194310186 -0.0255871240 0.595308752 0.5117958049 ## [20,] -0.5358289497 -0.2792705051 -0.063179833 0.2523130537 ## [21,] 1.0053556165 0.2535445551 -0.735184555 -0.9193320562 ## [22,] 0.1944381133 0.2776808447 0.211203585 -0.1621683158 ## [23,] 0.3324961963 0.3569986698 -0.393384796 -0.3315612187 ## [24,] -1.3871350320 0.3584947983 0.790696554 0.5411900977 ## [25,] -0.2892667647 0.0836470788 0.622780249 0.5733184308 ## [26,] -0.7200892783 -0.5760175412 -0.622057786 0.3871275953 ## [27,] 0.5309551745 -0.0795048221 -0.405655090 0.8478445706 ## [28,] 0.3300779570 0.7851538665 0.724615987 0.3814126611 ## [29,] -0.0191360826 0.3651346392 -1.241753502 -0.0175665191 ## [30,] 0.0095073126 0.0012017354 -0.452964804 0.0804336309 ## [31,] -0.4975856341 -0.8506398138 -0.113818910 0.5972892284 ## [32,] 0.5163256642 0.8667431992 0.340094774 -0.4436117041 ## [33,] 0.5130287441 -0.5496685843 0.457655315 0.0440828457 ## PC14 PC15 PC16 PC17 ## [1,] -0.0732249020 -0.3341726487 -0.1736591970 0.195228889 ## [2,] 0.3141231547 -1.2698536981 0.5224818750 -0.366895129 ## [3,] -0.1381878808 0.3954531897 -0.3379899895 0.248537636 ## [4,] 0.7227791303 -0.3017475056 -0.0787364138 0.592691715 ## [5,] 0.7837603242 -0.2132845778 -0.4384072707 0.296723558 ## [6,] -0.0970870333 -0.2625385613 0.1022708952 -0.214947584 ## [7,] 0.1771400743 -0.5957476099 -0.0348920260 0.225925342 ## [8,] -0.8808436255 0.4020156687 0.3279874792 -0.644656532 ## [9,] 0.3013182618 -0.0277628402 -0.4254019569 0.923417652 ## [10,] -0.5459264361 -0.0263919086 0.5732242213 0.557071184 ## [11,] -0.2139186622 0.1488366527 0.2950739978 -0.143607228 ## [12,] -0.7079434793 0.3291721026 0.9137059949 0.708320755 ## [13,] 1.9425808090 0.5342853430 -0.3898080121 -0.240086472 ## [14,] -0.1093244976 -0.4951920514 0.4054013786 -0.798711362 ## [15,] 0.9104772788 -0.0533660494 0.7123560813 -0.669146760 ## [16,] 0.6050876723 0.2818567040 0.0813391511 0.641508922 ## [17,] -0.2090171141 0.8507579071 0.0754898982 0.292291173 ## [18,] -0.0837622602 -0.7981763694 0.8634332939 -0.160390304 ## [19,] -0.8525140025 -0.0873522241 -0.2248447253 -0.071489870 ## [20,] -0.2218852388 -1.4939579089 0.6810839593 -0.233750092 ## [21,] 0.6180441980 0.0212572745 0.7337763877 0.148385426 ## [22,] -0.0435958377 0.2633830330 -0.0144362839 0.527240760 ## [23,] 0.2097677948 -0.0773029518 0.0993118155 0.160158771 ## [24,] 0.6666926782 -0.1303859171 0.0252092124 0.318845229 ## [25,] -0.7593072242 -0.1249911798 0.4213187670 -0.631426174 ## [26,] 0.8851644906 0.6168583361 -0.6330944595 0.086128321 ## [27,] -0.0576437056 1.0841517758 -0.4170187378 -0.068089433 ## [28,] -0.4612284737 -0.1208346482 0.0287091855 0.415327939 ## [29,] 0.7953232890 -0.1726967368 -0.2597175115 0.204668002 ## [30,] 0.2042662293 -0.1848225916 -0.0304071218 1.236353251 ## [31,] -0.7107423821 0.0194933612 -0.1388226110 -0.810265056 ## [32,] 0.4200354354 -0.0664945650 0.3908555006 -0.208419797 ## [33,] 0.7008433865 -0.3396164389 0.0180039917 -0.161364387 ## PC18 PC19 PC20 PC21 ## [1,] -0.4457362998 -4.610898e-03 -0.3078095779 0.0567392690 ## [2,] 1.0623718695 2.394451e-01 -0.5419162707 0.0252904680 ## [3,] -0.3067765693 -8.363524e-01 -0.5137208486 0.7112323765 ## [4,] -0.5189273868 -2.831232e-01 -0.2970294983 0.0167863953 ## [5,] 0.0383086229 3.050548e-01 0.1471425499 0.2627195701 ## [6,] 0.5982278717 1.536050e-02 -0.2604623636 0.3735616464 ## [7,] -0.1722642138 -1.079683e-01 -0.0901655537 -0.4508707308 ## [8,] -0.2884316074 9.226981e-02 -0.3744217280 0.2116295423 ## [9,] 0.9590343644 -4.152394e-01 -0.4418130496 0.3572943340 ## [10,] -1.3743018319 -5.672359e-01 0.1703548477 0.5826451206 ## [11,] -0.0586535632 -1.584388e-01 -0.2573719310 0.3145838663 ## [12,] 0.4138072560 -6.354171e-03 -0.0653126409 -0.2961914198 ## [13,] -0.3887540678 -2.881583e-01 -0.1549914943 -0.0973805181 ## [14,] 0.5133369117 4.935584e-01 0.2332614161 -0.0920206743 ## [15,] -0.0923178417 -3.017579e-01 0.0142772904 -0.4547184227 ## [16,] -0.3980090612 -1.004798e+00 0.7188059120 0.2251581876 ## [17,] -0.1853727025 5.534440e-03 -0.9049086228 0.6472282607 ## [18,] 0.0854285794 1.691068e-02 -0.0222811642 -0.1372681318 ## [19,] -0.6467037011 -2.435339e-01 -0.3253871862 0.1532740368 ## [20,] 0.6021908092 -6.513660e-03 0.3295586789 0.1367615005 ## [21,] 0.6521123203 3.240441e-01 -0.4126279464 -0.0291903475 ## [22,] -0.6938800078 -4.358869e-01 -0.4446220795 0.6612315056 ## [23,] -0.7163442651 -7.624555e-01 0.2544034601 -0.2122467115 ## [24,] 0.6416486533 5.465534e-02 0.0671909292 -0.2081305313 ## [25,] 0.3982146017 -1.844816e-01 -0.3252947242 -0.1375141304 ## [26,] 0.2706568234 -2.148996e-01 0.3156424496 -0.0494205681 ## [27,] -0.3984291822 1.807186e-01 -0.1885632787 0.0563889946 ## [28,] 0.5846515831 4.146499e-01 -0.2063168490 -0.6101147468 ## [29,] -0.2174321481 1.830913e-01 0.1152205038 0.3558752394 ## [30,] -1.2091442616 1.591550e-01 0.9012578149 -0.1150265417 ## [31,] -0.4493938904 1.912519e-01 -0.7645219151 0.5501554066 ## [32,] -0.1879901096 9.848562e-02 -0.3726156410 -0.0647924155 ## [33,] -0.9246690449 -4.696714e-01 -0.2444761648 -0.4401670316 ## PC22 PC23 PC24 PC25 ## [1,] -0.1022262860 -0.4039494655 -0.1692943919 -0.2327569431 ## [2,] -0.1199018532 0.0910462796 -0.4309741375 0.1818830285 ## [3,] 0.3557925548 -0.1157418332 0.0066484161 -0.0609868308 ## [4,] 0.4037655371 -0.2937421229 0.5956675582 0.6184004458 ## [5,] 0.1920809998 -0.2435180441 0.0371462705 0.0783549323 ## [6,] 0.3379655674 -0.1189286702 -0.1527143875 0.1107891023 ## [7,] 0.0618041706 0.0650182349 -0.2698727614 0.3220275404 ## [8,] -0.7358617121 -0.2490370211 -0.0492927494 0.0379738561 ## [9,] 0.4898837405 0.2860307789 0.5197192455 0.1821374875 ## [10,] -0.3747236631 0.2737925154 -0.1457765256 -0.0761614738 ## [11,] 0.3045892813 -0.0106082111 0.0201590112 -0.2693763165 ## [12,] 0.4465555980 0.7193906074 -0.2240837255 0.4639174892 ## [13,] -0.3067994486 -0.0979824992 0.5234244422 0.0820906609 ## [14,] -0.1739564430 0.4620908812 0.0723671860 -0.3522738715 ## [15,] 0.4216238175 0.3448204159 0.1519083876 -0.1617391120 ## [16,] 0.2482746276 0.1079170266 0.4646898431 -0.1512874293 ## [17,] -0.1602625209 0.2035741060 -0.5212603846 0.1126290423 ## [18,] -0.3619861096 0.3013196388 0.2377771248 0.1614614799 ## [19,] 0.6620320507 -0.2108095905 0.5913446695 -0.3189067650 ## [20,] 0.1122745377 -0.5038516870 -0.5657149283 0.1540179723 ## [21,] -0.0214711177 -0.5312467435 0.4543741280 0.2851634035 ## [22,] -0.2065743543 -0.0678946429 1.0608725232 -0.2652483161 ## [23,] 0.2842588782 -0.0976900752 0.2230457026 0.1015550942 ## [24,] 1.3341292315 -0.1522970625 0.0289499791 -0.2840327658 ## [25,] -0.3808818744 0.0989535406 0.2859288355 -0.3229577921 ## [26,] -0.2743774473 0.1373698712 0.1890767696 -0.1756523570 ## [27,] 0.3367836460 -0.3059694979 0.0487319559 -0.3087622269 ## [28,] 0.1405259746 0.1308026626 0.0831442113 0.1518874402 ## [29,] -0.1728043393 0.2837101844 -0.2930413498 0.0193721313 ## [30,] -0.1374709722 -0.0046887700 -0.1947329030 0.2528368483 ## [31,] -0.5339533269 0.0931337415 -0.0822783663 0.0353525931 ## [32,] -0.4106987626 0.8819906001 -0.0852000278 0.1837462198 ## [33,] 0.5796956977 -0.0645787657 -0.5812702762 0.2082504213 ## PC26 PC27 PC28 PC29 ## [1,] 1.117182e-01 -0.0966398520 0.4062097213 -7.211986e-01 ## [2,] -2.096347e-01 -0.4622280886 -0.0658737553 2.531054e-01 ## [3,] 3.002085e-01 -0.4337991652 0.3898869000 -2.687600e-02 ## [4,] 5.212339e-02 0.0163771004 -0.0414428429 1.646972e-01 ## [5,] 4.663576e-01 0.1843364315 0.0653079416 6.418099e-01 ## [6,] -2.126372e-02 -0.5292169127 0.0058714135 2.355877e-01 ## [7,] -2.797377e-01 -0.5616969959 0.3251031782 8.222250e-02 ## [8,] 3.917575e-01 0.1282764588 0.0277771364 -5.488517e-02 ## [9,] 2.960875e-01 0.0909223271 -0.0418896913 -1.665574e-01 ## [10,] 4.808022e-02 -0.2501098587 0.1687259366 -3.024521e-01 ## [11,] 3.315518e-01 0.0905121583 -0.0793419448 2.330636e-01 ## [12,] -9.395867e-02 -0.0619012622 -0.1003209908 -1.520693e-01 ## [13,] 2.134890e-01 -0.0470469660 -0.1568496043 2.092588e-01 ## [14,] 3.041331e-01 -0.0523779949 0.2209801191 -8.432698e-02 ## [15,] -4.721887e-01 0.4394827764 0.2771020418 2.314449e-02 ## [16,] 1.237427e-01 0.3437209357 -0.1938359505 -4.176686e-01 ## [17,] 4.324378e-02 -0.0585738072 -0.0079965896 4.320782e-01 ## [18,] 3.529554e-01 -0.0442805679 -0.4512113406 4.674863e-01 ## [19,] -3.470305e-02 -0.6317995371 0.3083612497 4.691560e-02 ## [20,] -6.335135e-01 0.2186552463 -0.2121070885 -4.519681e-02 ## [21,] -4.418800e-01 0.3512383251 0.0700632261 -6.633488e-02 ## [22,] 1.174937e-01 -0.0594580593 0.4233378201 2.547439e-01 ## [23,] -4.949624e-01 -0.4861072549 -0.0280564887 -1.329513e-01 ## [24,] 9.079160e-01 -0.1049299671 0.0993153961 1.903818e-01 ## [25,] -1.747998e-01 0.0888860546 -0.1294135551 3.046653e-01 ## [26,] -2.495438e-01 -0.3363519929 0.2262295608 -8.065229e-02 ## [27,] -4.716494e-02 -0.0408740928 0.1714566433 5.669629e-01 ## [28,] 4.239210e-02 -0.0004691487 -0.1780362726 2.769507e-02 ## [29,] -1.053177e-01 -0.3873306208 -0.2118740263 4.134341e-01 ## [30,] 8.545713e-02 -0.1201782678 0.0189254708 -4.238918e-02 ## [31,] -2.548485e-02 -0.3277920997 -0.0459242740 7.320668e-03 ## [32,] 1.363273e-02 -0.3845378708 -0.1249983326 -2.336636e-01 ## [33,] -3.773702e-01 -0.0628420943 -0.3331347868 -2.283481e-01 ## PC30 ## [1,] 0.0491771413 ## [2,] 0.2116874274 ## [3,] -0.5143813232 ## [4,] -0.5663522785 ## [5,] -0.6988054010 ## [6,] -0.3203439680 ## [7,] -0.0175446142 ## [8,] 0.0381789506 ## [9,] -0.0886259298 ## [10,] 0.0714796465 ## [11,] -0.1306303029 ## [12,] 0.1058736578 ## [13,] 0.1047178822 ## [14,] 0.2364788566 ## [15,] 0.2542225801 ## [16,] -0.4997859068 ## [17,] -0.3043303828 ## [18,] 0.0619361304 ## [19,] 0.1081317608 ## [20,] -0.2557260229 ## [21,] -0.1598611451 ## [22,] 0.4733555374 ## [23,] -0.2725161035 ## [24,] 0.2820923752 ## [25,] -0.2062812163 ## [26,] 0.1905601545 ## [27,] -0.0080134804 ## [28,] -0.1107170680 ## [29,] -0.1305718337 ## [30,] -0.2856534832 ## [31,] 0.6217773955 ## [32,] 0.4442087210 ## [33,] 0.0789279593 ## [ reached getOption(&quot;max.print&quot;) -- omitted 967 rows ] #Now looking at rotation head(observ_x, 1 ) %*% pr_out$rotation[,1] ## [,1] ## [1,] 0.3558461 #k is which column are we going to examine j &lt;- 2 plot(pr_out$rotation[, j]) abline(h = 0) which(abs(pr_out$rotation[, j]) &gt; 0.2) ## [1] 3 4 7 9 11 12 16 17 18 23 # what is the truth we should be comparing to? #What is rotation. Matrix (30 x 30 in this case) dim(pr_out$rotation) ## [1] 30 30 #it is actually p x p rather than p x k to give you more columns head(pr_out$rotation[,k]) ## [1] -0.012959590 0.125699214 -0.135540402 -0.152606801 -0.018739123 ## [6] -0.009272309 #kth eigenvector which correspods with the kth largest value (most significant) value #this will always be 1. Property of rotation matrix sum(pr_out$rotation[,k]^2) ## [1] 1 5.8 Principal Component Analysis Applied! 5.8.1 Other example In really poor countries it is super hard to measure wealth and income. There are no reciepts and corresponding taxes. People don’t have bank accounts. Instead, you measure features. Like do you have a fridge. Do you have cooking equipment? How many kids? How many room in your house? So you could run PCA on assets matrix. You can find correlations. If you have more rooms in your house, you likely have more education. The correlations will be baked into the principal driving component. Further, they use this as the Y to see if they can predict! But that is beyond the scope of this class. library(jsonlite) #citation count between his 15 papers and those he sighted citations &lt;- read.csv(&quot;Datasets/j_cunningham_citation.csv&quot;, head = FALSE) titles &lt;- read_json(&quot;Datasets/j_cunningham_citation_titles.json&quot;) 5.8.2 Explore the data dim(citations) ## [1] 15 754 head(citations) ## V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 V12 V13 V14 V15 V16 V17 V18 V19 V20 ## 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## V21 V22 V23 V24 V25 V26 V27 V28 V29 V30 V31 V32 V33 V34 V35 V36 V37 V38 ## 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## V39 V40 V41 V42 V43 V44 V45 V46 V47 V48 V49 V50 V51 V52 V53 V54 V55 V56 ## 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## V57 V58 V59 V60 V61 V62 V63 V64 V65 V66 V67 V68 V69 V70 V71 V72 V73 V74 ## 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## V75 V76 V77 V78 V79 V80 V81 V82 V83 V84 V85 V86 V87 V88 V89 V90 V91 V92 ## 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## V93 V94 V95 V96 V97 V98 V99 V100 V101 V102 V103 V104 V105 V106 V107 ## 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## V108 V109 V110 V111 V112 V113 V114 V115 V116 V117 V118 V119 V120 V121 ## 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## V122 V123 V124 V125 V126 V127 V128 V129 V130 V131 V132 V133 V134 V135 ## 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 ## V136 V137 V138 V139 V140 V141 V142 V143 V144 V145 V146 V147 V148 V149 ## 1 1 1 0 1 7 3 0 0 2 2 3 0 2 3 ## V150 V151 V152 V153 V154 V155 V156 V157 V158 V159 V160 V161 V162 V163 ## 1 0 3 0 3 3 8 0 0 3 2 6 0 0 0 ## V164 V165 V166 V167 V168 V169 V170 V171 V172 V173 V174 V175 V176 V177 ## 1 4 1 0 1 0 0 1 1 0 0 1 0 1 2 ## V178 V179 V180 V181 V182 V183 V184 V185 V186 V187 V188 V189 V190 V191 ## 1 0 0 1 0 1 0 0 1 4 0 1 0 0 1 ## V192 V193 V194 V195 V196 V197 V198 V199 V200 V201 V202 V203 V204 V205 ## 1 2 0 0 1 2 0 2 0 0 1 1 0 0 2 ## V206 V207 V208 V209 V210 V211 V212 V213 V214 V215 V216 V217 V218 V219 ## 1 1 0 3 0 1 1 0 0 3 0 2 3 0 0 ## V220 V221 V222 V223 V224 V225 V226 V227 V228 V229 V230 V231 V232 V233 ## 1 2 0 0 1 0 0 1 0 0 1 0 1 1 0 ## V234 V235 V236 V237 V238 V239 V240 V241 V242 V243 V244 V245 V246 V247 ## 1 1 1 0 1 0 1 1 0 1 1 0 0 0 0 ## V248 V249 V250 V251 V252 V253 V254 V255 V256 V257 V258 V259 V260 V261 ## 1 1 0 1 1 0 0 0 0 1 0 1 0 0 0 ## V262 V263 V264 V265 V266 V267 V268 V269 V270 V271 V272 V273 V274 V275 ## 1 1 0 1 0 0 0 0 1 1 0 0 0 0 1 ## V276 V277 V278 V279 V280 V281 V282 V283 V284 V285 V286 V287 V288 V289 ## 1 1 0 1 0 0 0 1 0 2 0 1 0 1 0 ## V290 V291 V292 V293 V294 V295 V296 V297 V298 V299 V300 V301 V302 V303 ## 1 2 1 1 1 0 1 0 1 0 0 0 1 1 1 ## V304 V305 V306 V307 V308 V309 V310 V311 V312 V313 V314 V315 V316 V317 ## 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 ## V318 V319 V320 V321 V322 V323 V324 V325 V326 V327 V328 V329 V330 V331 ## 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## V332 V333 V334 V335 V336 V337 V338 V339 V340 V341 V342 V343 V344 V345 ## 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## V346 V347 V348 V349 V350 V351 V352 V353 V354 V355 V356 V357 V358 V359 ## 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## V360 V361 V362 V363 V364 V365 V366 V367 V368 V369 V370 V371 V372 V373 ## 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## V374 V375 V376 V377 V378 V379 V380 V381 V382 V383 V384 V385 V386 V387 ## 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## V388 V389 V390 V391 V392 V393 V394 V395 V396 V397 V398 V399 V400 V401 ## 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## V402 V403 V404 V405 V406 V407 V408 V409 V410 V411 V412 V413 V414 V415 ## 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## V416 V417 V418 V419 V420 V421 V422 V423 V424 V425 V426 V427 V428 V429 ## 1 0 0 0 0 5 0 0 0 0 0 0 0 5 0 ## V430 V431 V432 V433 V434 V435 V436 V437 V438 V439 V440 V441 V442 V443 ## 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## V444 V445 V446 V447 V448 V449 V450 V451 V452 V453 V454 V455 V456 V457 ## 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## V458 V459 V460 V461 V462 V463 V464 V465 V466 V467 V468 V469 V470 V471 ## 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## V472 V473 V474 V475 V476 V477 V478 V479 V480 V481 V482 V483 V484 V485 ## 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## V486 V487 V488 V489 V490 V491 V492 V493 V494 V495 V496 V497 V498 V499 ## 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## V500 V501 V502 V503 V504 V505 V506 V507 V508 V509 V510 V511 V512 V513 ## 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## V514 V515 V516 V517 V518 V519 V520 V521 V522 V523 V524 V525 V526 V527 ## 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## V528 V529 V530 V531 V532 V533 V534 V535 V536 V537 V538 V539 V540 V541 ## 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## V542 V543 V544 V545 V546 V547 V548 V549 V550 V551 V552 V553 V554 V555 ## 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## V556 V557 V558 V559 V560 V561 V562 V563 V564 V565 V566 V567 V568 V569 ## 1 0 0 0 0 0 0 3 0 0 0 0 0 0 0 ## V570 V571 V572 V573 V574 V575 V576 V577 V578 V579 V580 V581 V582 V583 ## 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## V584 V585 V586 V587 V588 V589 V590 V591 V592 V593 V594 V595 V596 V597 ## 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## V598 V599 V600 V601 V602 V603 V604 V605 V606 V607 V608 V609 V610 V611 ## 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## V612 V613 V614 V615 V616 V617 V618 V619 V620 V621 V622 V623 V624 V625 ## 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## V626 V627 V628 V629 V630 V631 V632 V633 V634 V635 V636 V637 V638 V639 ## 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## V640 V641 V642 V643 V644 V645 V646 V647 V648 V649 V650 V651 V652 V653 ## 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ## V654 V655 V656 V657 V658 V659 V660 V661 V662 V663 V664 V665 V666 V667 ## 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## V668 V669 V670 V671 V672 V673 V674 V675 V676 V677 V678 V679 V680 V681 ## 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## V682 V683 V684 V685 V686 V687 V688 V689 V690 V691 V692 V693 V694 V695 ## 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## V696 V697 V698 V699 V700 V701 V702 V703 V704 V705 V706 V707 V708 V709 ## 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## V710 V711 V712 V713 V714 V715 V716 V717 V718 V719 V720 V721 V722 V723 ## 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## V724 V725 V726 V727 V728 V729 V730 V731 V732 V733 V734 V735 V736 V737 ## 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## V738 V739 V740 V741 V742 V743 V744 V745 V746 V747 V748 V749 V750 V751 ## 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## V752 V753 V754 ## 1 0 0 0 ## [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 5 rows ] citations[1:5,1:5] ## V1 V2 V3 V4 V5 ## 1 0 0 0 0 0 ## 2 0 0 0 0 0 ## 3 0 0 0 0 0 ## 4 0 0 0 0 0 ## 5 0 0 0 0 0 max(citations) ## [1] 14 #across all paers apply(citations, 1, max) ## [1] 8 7 14 5 5 8 6 8 11 5 3 5 11 3 4 names(titles) ## [1] &quot;auth_titles&quot; &quot;ref_titles&quot; Papers that he has written: head(titles[[&quot;auth_titles&quot;]],3) ## [[1]] ## [1] &quot;Value and choice as separable, stable representations in orbitofrontal cortex&quot; ## ## [[2]] ## [1] &quot;Calibrating deep convolutional gaussian processes&quot; ## ## [[3]] ## [1] &quot;Neural trajectories in the supplementary motor area and motor cortex exhibit distinct geometries, compatible with different classes of computation&quot; Papers he has cited head(titles[[&quot;ref_titles&quot;]],3) ## [[1]] ## [1] &quot;On improved estimation of normal precision matrix and discriminant coefficients&quot; ## ## [[2]] ## [1] &quot;Snakemake--a scalable bioinformatics workflow engine&quot; ## ## [[3]] ## [1] &quot;Bayesian source localization with the multivariate laplace prior&quot; Among the 15, there are four papers that reference the 2 most popular articles. Let us find them: ref_count &lt;- apply(citations, 2, function(x) sum(x &gt; 0)) targets &lt;- tail(names(sort(ref_count)),2) #These are the two columns we want target_ind &lt;- which(names(citations) %in% targets) target_ind ## [1] 186 428 titles[[&quot;ref_titles&quot;]][target_ind] ## [[1]] ## [1] &quot;A category-free neural population supports evolving demands during decision-making&quot; ## ## [[2]] ## [1] &quot;Reorganization between preparatory and movement population responses in motor cortex&quot; Explore this data: we know the index of the two. This can show the correlation between the two, meaning the papers are cited by certain papers. This would make sense. If you cite one of these, you almost certainly have to cite the other: citations[,target_ind] ## V186 V428 ## 1 4 5 ## 2 0 0 ## 3 1 1 ## 4 0 0 ## 5 0 0 ## 6 0 0 ## 7 0 0 ## 8 0 0 ## 9 3 1 ## 10 0 0 ## 11 0 0 ## 12 0 0 ## 13 0 0 ## 14 0 0 ## 15 2 1 A few things to remember. Longer papers should have more citations. There is also likely to be correlation between certain papers. We would intuitively just want to apply our “prcomp”, like we learned in last class. pr_out &lt;- prcomp(citations) plot(pr_out$sdev, main=&quot;&quot;) This plot is not very appealing. There is not a significant drop until the last term. Maybe between 1 and 2 and 3 and 4, but not a big drop. And if you only abandon 1 dimension, (14 instead of 15), you aren’t really saving a lot. 5.8.2.1 Try standardizing the citation matrix in different ways 5.8.2.1.1 Usual standardization, i.e. make each feature mean=0 and sd = 1 norm_citation &lt;- apply(citations, 2 , scale) #also pr_out &lt;- prcomp(norm_citation) plot(pr_out$sdev, main=&quot;&quot;) png(&quot;Datasets/loadings_normal_standardization.png&quot;, 900, 700) par(mfrow=c(4, 3)) for(i in seq_len(ncol(pr_out$rotation[,1:12]))){ eigenvec &lt;- pr_out$rotation[, i] plot(eigenvec, main = paste(&quot;Eigenvec&quot;,i)) abline(h=0) } dev.off() ## png ## 2 Plots using normal standardization Remember from the plot, if we square all the values and add them together they will equal one. So strong deviations will be papers that are relied on (by index). Too many papers that share the same weight, might not be helpful. We believe there are only a few really good papers that we want. Not like 50. So this behavior is still undesirable. This is bad! You subtract something away from 0 values. But we like 0s because they don’t affect the objective function. PCA is using the “frebenious norm” where everything squared and added together is 1. So we like 0s. So how can we scale differently, but while keeping the 0s. 5.8.2.1.2 Max normalized, i.e. make each feature min=0, max = 1 #lets divide by the max, then everything is between 0 and one norm_citation &lt;- apply(citations, 2, function(x) x / max(x)) pr_out &lt;- prcomp(norm_citation, center=FALSE, scale=FALSE) png(&quot;Datasets/max_normalized.png&quot;, 900, 700) par(mfrow=c(4, 3)) for(i in seq_len(ncol(pr_out$rotation[,1:12]))){ eigenvec &lt;- pr_out$rotation[, i] plot(eigenvec, main = paste(&quot;Eigenvec&quot;,i)) abline(h=0) } #dev.off() This should look much nicer. For example, Eigvec 4 looks better. Why did we normalize the columns? We often do this because the columns have different units. However, in this, the columns all have the same units. Instead, papers have different lengths, so the citation number can be affected by the length of the paper. So whay do we want to actually normalize? The rows! 5.8.2.1.3 Max normalized per paper, i.e. make each ROW min=0, max = 1 citations_norm_papers &lt;- apply(citations, 1, function(x) x / max(x)) # Just doing the above actually swapped things! We have 15 columns instead #It processes a row, and then stacks it as a column. So we need to transpose citations_norm_papers &lt;- t(citations_norm_papers) pr_out &lt;- prcomp(citations_norm_papers) plot(pr_out$sdev, main=&quot;&quot;) png(&quot;Datasets/loadings_norm_per_paper.png&quot;, 900, 700) par(mfrow=c(4, 3)) for(i in seq_len(ncol(pr_out$rotation[,1:12]))){ eigenvec &lt;- pr_out$rotation[, i] plot(eigenvec) abline(h=0) } #dev.off() Ther is a much more noticable drop between 11 and 12 pr_out &lt;- prcomp(citations) plot(pr_out$sdev, main=&quot;&quot;) png(&quot;Datasets/loadings.png&quot;, 900, 700) par(mfrow=c(4, 3)) for(i in seq_len(ncol(pr_out$rotation[,1:12]))){ eigenvec &lt;- pr_out$rotation[, i] plot(eigenvec) abline(h=0) } #dev.off() target_ind &lt;- which(abs(pr_out$rotation[, 2]) &gt; 0.15) titles[[&quot;ref_titles&quot;]][target_ind] ## [[1]] ## [1] &quot;Motor cortex embeds muscle-like commands in an untangled population response&quot; ## ## [[2]] ## [1] &quot;Flexible sensorimotor computations through rapid reconfiguration of cortical dynamics&quot; ## ## [[3]] ## [1] &quot;Neuronal activity in the supplementary and presupplementary motor areas for temporal organization of multiple movements&quot; ## ## [[4]] ## [1] &quot;Role for supplementary motor area cells in planning several movements ahead&quot; ## ## [[5]] ## [1] &quot;The role of human primary motor cortex in the production of skilled finger sequences&quot; 1st column second row is 2nd column 3rd row is a disaster. 5.9 PCA on weather data example Wrap up the citation problem Play with weather data, tmax, celsius df &lt;- read.csv(&quot;Datasets/ushcn.csv&quot;) station &lt;- read.csv(&quot;Datasets/station_metadata.csv&quot;) rownames(station) &lt;- station$id meta_sort &lt;- station[names(df)[-1], c(&quot;longitude&quot;, &quot;latitude&quot;)] prop_na &lt;- apply(df[, -1], 1, function(x) mean(is.na(x))) sdf &lt;- df[prop_na &lt; 0.1, ] sdf_sans_na &lt;- apply(sdf[, -1], 2, function(x){ x[is.na(x)] &lt;- mean(x, na.rm=TRUE) return(x) }) library(RColorBrewer) cols &lt;- brewer.pal(7, &quot;RdYlBu&quot;) pr_out &lt;- prcomp(sdf_sans_na) png(&quot;Pictures/no_norm_pca.png&quot;, 600, 800) par(mfrow=c(2, 1)) for(i in 1:2){ eigvec &lt;- pr_out$rotation[, i] breaks &lt;- seq(min(eigvec), max(eigvec), length.out=length(cols)+1) col_factors &lt;- cut(eigvec, breaks=breaks) plot(meta_sort$longitude, meta_sort$latitude, col=cols[col_factors], pch=16, cex=0.5) legend(&quot;bottomright&quot;, legend = levels(col_factors), fill=cols) } #dev.off() Eigenvector 1 and two mapped The two are telling in very different ways. The first shows the relationships of the coasts. The second shows east versus west. This is with default normalization: centering, but not scaling. 5.10 Different noramlizations Run PCA with 3 different types of normalization on the maximum temperature data, then plot the “maps” of the loading values corresponding to the first 2 eigenvectors. No normalization, i.e. no centering and no scaling Centering but no scaling Centering and scaling Write out what do you observe. 5.10.1 No normalization prop_na &lt;- apply(df[, -1], 1, function(x) mean(is.na(x))) sdf &lt;- df[prop_na &lt; 0.1, ] sdf_sans_na &lt;- apply(sdf[, -1], 2, function(x){ x[is.na(x)] &lt;- mean(x, na.rm=TRUE) return(x) }) cols &lt;- brewer.pal(7, &quot;RdYlBu&quot;) pr_out &lt;- prcomp(sdf_sans_na, scale = FALSE, center = FALSE) png(&quot;Pictures/no_standardization_pca.png&quot;, 600, 800) par(mfrow=c(2, 1)) for(i in 1:2){ eigvec &lt;- pr_out$rotation[, i] breaks &lt;- seq(min(eigvec), max(eigvec), length.out=length(cols)+1) col_factors &lt;- cut(eigvec, breaks=breaks) plot(meta_sort$longitude, meta_sort$latitude, col=cols[col_factors], pch=16, cex=0.5) legend(&quot;bottomright&quot;, legend = levels(col_factors), fill=cols) } #dev.off() 5.10.2 Centering and scaling prop_na &lt;- apply(df[, -1], 1, function(x) mean(is.na(x))) sdf &lt;- df[prop_na &lt; 0.1, ] sdf_sans_na &lt;- apply(sdf[, -1], 2, function(x){ x[is.na(x)] &lt;- mean(x, na.rm=TRUE) return(x) }) library(RColorBrewer) pr_out &lt;- prcomp(sdf_sans_na, center = TRUE, scale = TRUE) png(&quot;Pictures/centering_and_scaling_pca.png&quot;, 600, 800) par(mfrow=c(2, 1)) for(i in 1:2){ eigvec &lt;- pr_out$rotation[, i] breaks &lt;- seq(min(eigvec), max(eigvec), length.out=length(cols)+1) col_factors &lt;- cut(eigvec, breaks=breaks) plot(meta_sort$longitude, meta_sort$latitude, col=cols[col_factors], pch=16, cex=0.5) legend(&quot;bottomright&quot;, legend = levels(col_factors), fill=cols) } #dev.off() Scaling is super common. When you have different units you always usually scale. However, sometimes, with things like weather data, you might not have to. PCA is trying to find a very consice, uncorrelated description of your data. If things are close to 0, they won’t effect. Subset out the last 144 rows for testing, then pick one station to be our Y and the other stations to be our X. Run PCA on the X values, then fit an OLS Run OLS but only use the closest station (don’t bother with projecting, assuming Long/Lat are equidistance for now) as your X. Which one will do best? (if you have time, try Lasso with all of the data, this may take awhile….don’t do this unless you have time) #Takes last 144 for testing train &lt;- 1:(nrow(sdf_sans_na) - 144) # Samples and individual X to be the Y point y_ind &lt;- sample(ncol(sdf_sans_na), 1) y_train &lt;- sdf_sans_na[train, y_ind] x_train &lt;- sdf_sans_na[train, -y_ind] pr_out &lt;- prcomp(x_train, center = TRUE, scale = FALSE) ols &lt;- lm(y_train ~ x_train) library(tidyverse) station_new &lt;- station %&gt;% mutate(long_difference = longitude - station$longitude[y_ind]) %&gt;% mutate(lat_difference = latitude - station$latitude[y_ind]) %&gt;% select(latitude, longitude, long_difference, lat_difference) %&gt;% mutate(sum_lat_long = lat_difference + long_difference) #%&gt;% #order(decreasing = TRUE, sum_lat_long) station_new ## latitude longitude long_difference lat_difference ## USH00011084 31.0581 -87.0547 2.6620 -7.0586 ## USH00012813 30.5467 -87.8808 1.8359 -7.5700 ## USH00013160 32.8347 -88.1342 1.5825 -5.2820 ## USH00013511 32.7017 -87.5808 2.1359 -5.4150 ## USH00013816 31.8700 -86.2542 3.4625 -6.2467 ## USH00015749 34.7442 -87.5997 2.1170 -3.3725 ## USH00017157 34.1736 -86.8133 2.9034 -3.9431 ## USH00017304 34.6736 -86.0536 3.6631 -3.4431 ## USH00017366 32.4100 -87.0153 2.7014 -5.7067 ## USH00018024 33.4164 -86.1350 3.5817 -4.7003 ## USH00018178 31.5411 -87.8833 1.8334 -6.5756 ## USH00018323 31.8075 -85.9722 3.7445 -6.3092 ## USH00018380 33.2119 -87.6161 2.1006 -4.9048 ## USH00018438 32.0142 -85.7464 3.9703 -6.1025 ## USH00018469 34.5667 -85.6128 4.1039 -3.5500 ## USH00020080 32.3697 -112.8600 -23.1433 -5.7470 ## USH00021026 33.3761 -112.5828 -22.8661 -4.7406 ## USH00021248 36.1533 -109.5394 -19.8227 -1.9634 ## USH00021514 33.2058 -111.6819 -21.9652 -4.9109 ## USH00021614 34.3494 -111.6981 -21.9814 -3.7673 ## USH00023160 35.2681 -111.7428 -22.0261 -2.8486 ## USH00023596 36.0528 -112.1503 -22.4336 -2.0639 ## USH00024089 34.9094 -110.1544 -20.4377 -3.2073 ## USH00024645 35.2000 -114.0167 -24.3000 -2.9167 ## USH00024849 36.8644 -111.6022 -21.8855 -1.2523 ## USH00025512 33.4044 -110.8700 -21.1533 -4.7123 ## USH00026250 34.1547 -114.2897 -24.5730 -3.9620 ## USH00026353 31.9356 -109.8378 -20.1211 -6.1811 ## USH00026796 34.5706 -112.4322 -22.7155 -3.5461 ## USH00027281 33.6731 -111.1508 -21.4341 -4.4436 ## USH00027370 33.0800 -111.7417 -22.0250 -5.0367 ## USH00027390 32.8150 -109.6808 -19.9641 -5.3017 ## USH00027435 34.5172 -109.4028 -19.6861 -3.5995 ## USH00027716 35.3322 -112.8797 -23.1630 -2.7845 ## USH00028619 31.7056 -110.0569 -20.3402 -6.4111 ## USH00028815 32.2292 -110.9536 -21.2369 -5.8875 ## USH00029271 33.8169 -109.9833 -20.2666 -4.2998 ## USH00029287 33.9792 -112.7403 -23.0236 -4.1375 ## USH00029359 35.2406 -112.1903 -22.4736 -2.8761 ## USH00029652 32.6114 -114.6350 -24.9183 -5.5053 ## USH00030936 34.8822 -91.2153 -1.4986 -3.2345 ## USH00031596 35.0842 -92.4289 -2.7122 -3.0325 ## USH00031632 36.4197 -90.5858 -0.8691 -1.6970 ## USH00032356 36.4164 -93.7917 -4.0750 -1.7003 ## USH00032444 36.1006 -94.1744 -4.4577 -2.0161 ## USH00032930 36.4261 -94.4481 -4.7314 -1.6906 ## USH00034572 36.4947 -91.5350 -1.8183 -1.6220 ## USH00034756 34.5731 -94.2494 -4.5327 -3.5436 ## USH00035186 35.6042 -91.2744 -1.5577 -2.5125 ## USH00035512 35.5125 -93.8683 -4.1516 -2.6042 ## USH00035754 34.2256 -92.0189 -2.3022 -3.8911 ## USH00035820 36.2639 -90.9681 -1.2514 -1.8528 ## USH00035908 33.8203 -93.3878 -3.6711 -4.2964 ## USH00036253 33.8100 -91.2703 -1.5536 -4.3067 ## USH00036928 35.3028 -93.6369 -3.9202 -2.8139 ## USH00040693 37.8744 -122.2589 -32.5422 -0.2423 ## USH00040924 33.6131 -114.5972 -24.8805 -4.5036 ## USH00041048 32.9544 -115.5581 -25.8414 -5.1623 ## USH00041614 41.5336 -120.1736 -30.4569 3.4169 ## USH00041715 39.6911 -121.8211 -32.1044 1.5744 ## USH00041758 32.6400 -117.0858 -27.3691 -5.4767 ## USH00041912 39.0911 -120.9481 -31.2314 0.9744 ## USH00042239 32.9897 -116.5872 -26.8705 -5.1270 ## USH00042294 38.5350 -121.7761 -32.0594 0.4183 ## USH00042319 36.4622 -116.8669 -27.1502 -1.6545 ## USH00042728 38.3306 -120.6706 -30.9539 0.2139 ## USH00042910 40.8097 -124.1603 -34.4436 2.6930 ## USH00042941 34.7042 -118.4275 -28.7108 -3.4125 ## USH00043161 39.5092 -123.7567 -34.0400 1.3925 ## USH00043257 36.7800 -119.7194 -30.0027 -1.3367 ## USH00043747 36.3219 -119.6356 -29.9189 -1.7948 ## USH00043761 41.8042 -123.3758 -33.6591 3.6875 ## USH00043875 38.6175 -122.8731 -33.1564 0.5008 ## USH00044232 36.7981 -118.2036 -28.4869 -1.3186 ## USH00044259 33.7086 -116.2153 -26.4986 -4.4081 ## USH00044713 39.3183 -120.6392 -30.9225 1.2016 ## USH00044890 36.3817 -119.0264 -29.3097 -1.7350 ## USH00044997 37.6922 -121.7692 -32.0525 -0.4245 ## USH00045032 38.1061 -121.2878 -31.5711 -0.0106 ## USH00045385 39.1458 -121.5853 -31.8686 1.0291 ## USH00045532 37.2858 -120.5117 -30.7950 -0.8309 ## USH00045983 41.3206 -122.3081 -32.5914 3.2039 ## USH00046074 38.2778 -122.2647 -32.5480 0.1611 ## USH00046118 34.7675 -114.6189 -24.9022 -3.3492 ## USH00046175 33.6025 -117.8803 -28.1636 -4.5142 ## USH00046399 34.4478 -119.2275 -29.5108 -3.6689 ## USH00046506 39.7458 -122.1997 -32.4830 1.6291 ## USH00046508 41.3089 -123.5317 -33.8150 3.1922 ## USH00046719 34.1483 -118.1447 -28.4280 -3.9684 ## USH00046730 35.6278 -120.6856 -30.9689 -2.4889 ## USH00046826 38.2578 -122.6078 -32.8911 0.1411 ## USH00047195 39.9367 -120.9475 -31.2308 1.8200 ## USH00047304 40.5175 -122.2986 -32.5819 2.4008 ## USH00047306 34.0528 -117.1894 -27.4727 -4.0639 ## USH00047851 35.3056 -120.6639 -30.9472 -2.8111 ## USH00047902 34.4167 -119.6844 -29.9677 -3.7000 ## USH00047916 36.9906 -121.9911 -32.2744 -1.1261 ## USH00047965 38.4381 -122.6978 -32.9811 0.3214 ## USH00048702 40.4167 -120.6631 -30.9464 2.3000 ## USH00048758 39.1678 -120.1428 -30.4261 1.0511 ## USH00048839 35.0233 -118.7497 -29.0330 -3.0934 ## USH00049087 33.7025 -117.7539 -28.0372 -4.4142 ## USH00049122 39.1467 -123.2103 -33.4936 1.0300 ## USH00049200 38.3956 -121.9608 -32.2441 0.2789 ## USH00049452 35.5975 -119.3531 -29.6364 -2.5192 ## USH00049490 40.7222 -122.9331 -33.2164 2.6055 ## USH00049699 39.5231 -122.3058 -32.5891 1.4064 ## USH00049855 37.7500 -119.5897 -29.8730 -0.3667 ## USH00049866 41.7036 -122.6408 -32.9241 3.5869 ## USH00050848 39.9919 -105.2667 -15.5500 1.8752 ## USH00051294 38.4600 -105.2256 -15.5089 0.3433 ## USH00051528 39.2203 -105.2783 -15.5616 1.1036 ## USH00051564 38.8236 -102.3486 -12.6319 0.7069 ## USH00051741 39.2425 -107.9631 -18.2464 1.1258 ## USH00052184 37.6742 -106.3247 -16.6080 -0.4425 ## USH00052281 39.6261 -106.0353 -16.3186 1.5094 ## USH00052446 38.4775 -102.7808 -13.0641 0.3608 ## USH00053005 40.6147 -105.1314 -15.4147 2.4980 ## USH00053038 40.2600 -103.8156 -14.0989 2.1433 ## USH00053146 39.1653 -108.7331 -19.0164 1.0486 ## USH00053662 38.5258 -106.9675 -17.2508 0.4091 ## USH00053951 37.7717 -107.1097 -17.3930 -0.3450 ## USH00054076 38.0494 -102.1236 -12.4069 -0.0673 ## USH00054770 38.0936 -102.6306 -12.9139 -0.0231 ## USH00054834 38.0636 -103.2153 -13.4986 -0.0531 ## USH00055322 37.1742 -105.9392 -16.2225 -0.9425 ## USH00055722 38.4858 -107.8792 -18.1625 0.3691 ## USH00057167 38.0392 -103.6933 -13.9766 -0.0775 ## USH00057337 38.0858 -106.1444 -16.4277 -0.0309 ## USH00057936 40.4883 -106.8233 -17.1066 2.3716 ## USH00058204 37.9492 -107.8733 -18.1566 -0.1675 ## USH00058429 37.1786 -104.4869 -14.7702 -0.9381 ## USH00059243 40.0583 -102.2189 -12.5022 1.9416 ## USH00062658 41.9500 -73.3667 16.3500 3.8333 ## USH00063207 41.3506 -72.0394 17.6773 3.2339 ## USH00067970 41.1247 -73.5475 16.1692 3.0080 ## USH00068138 41.7950 -72.2283 17.4884 3.6783 ## USH00072730 39.2583 -75.5167 14.2000 1.1416 ## USH00073595 38.8161 -75.5761 14.1406 0.6994 ## USH00075915 38.8983 -75.4250 14.2917 0.7816 ## USH00076410 39.6694 -75.7514 13.9653 1.5527 ## USH00079605 39.7739 -75.5414 14.1753 1.6572 ## USH00080211 29.7258 -85.0206 4.6961 -8.3909 ## USH00080228 27.2181 -81.8739 7.8428 -10.8986 ## USH00080478 27.8986 -81.8433 7.8734 -10.2181 ## USH00080611 26.6928 -80.6711 9.0456 -11.4239 ## USH00082220 30.7244 -86.0939 3.6228 -7.3923 ## USH00082850 25.8489 -81.3897 8.3270 -12.2678 ## USH00082915 29.7550 -81.5389 8.1778 -8.3617 ## USH00082944 30.6589 -81.4636 8.2531 -7.4578 ## USH00083163 26.1019 -80.2011 9.5156 -12.0148 ## USH00083186 26.5850 -81.8614 7.8553 -11.5317 ## USH00083207 27.4622 -80.3539 9.3628 -10.6545 ## USH00084289 28.8031 -82.3125 7.4042 -9.3136 ## USH00084570 24.5550 -81.7522 7.9645 -13.5617 ## USH00084731 30.1853 -82.5942 7.1225 -7.9314 ## USH00085275 30.4517 -83.4119 6.3048 -7.6650 ## USH00086414 29.0803 -82.0778 7.6389 -9.0364 ## USH00086997 30.4781 -87.1869 2.5298 -7.6386 ## USH00087020 25.5819 -80.4361 9.2806 -12.5348 ## USH00087851 28.3378 -82.2600 7.4567 -9.7789 ## USH00088758 30.3931 -84.3533 5.3634 -7.7236 ## USH00088824 28.1586 -82.7644 6.9523 -9.9581 ## USH00088942 28.6242 -80.8158 8.9009 -9.4925 ## USH00090140 31.5339 -84.1489 5.5678 -6.5828 ## USH00090586 30.8228 -84.6175 5.0992 -7.2939 ## USH00091340 31.1681 -81.5022 8.2145 -6.9486 ## USH00091500 31.1903 -84.2036 5.5131 -6.9264 ## USH00092318 33.5972 -83.8436 5.8731 -4.5195 ## USH00092475 34.5292 -83.9900 5.7267 -3.5875 ## USH00092966 32.2003 -83.2058 6.5109 -5.9164 ## USH00093621 34.3006 -83.8600 5.8567 -3.8161 ## USH00093754 31.9881 -81.9522 7.7645 -6.1286 ## USH00094170 33.2842 -83.4681 6.2486 -4.8325 ## USH00095874 33.0831 -83.2497 6.4670 -5.0336 ## USH00095882 32.8703 -81.9672 7.7495 -5.2464 ## USH00096335 33.4544 -84.8178 4.8989 -4.6623 ## USH00097276 30.7836 -83.5692 6.1475 -7.3331 ## USH00097600 34.2453 -85.1514 4.5653 -3.8714 ## USH00097847 32.1300 -81.2100 8.5067 -5.9867 ## USH00098535 32.6875 -84.5197 5.1970 -5.4292 ## USH00098703 31.4461 -83.4767 6.2400 -6.6706 ## USH00098740 34.5786 -83.3319 6.3848 -3.5381 ## USH00099141 33.4028 -82.6222 7.0945 -4.7139 ## USH00099157 33.7264 -82.7058 7.0109 -4.3903 ## USH00099186 31.2514 -82.3128 7.4039 -6.8653 ## USH00099291 32.8694 -85.1892 4.5275 -5.2473 ## USH00100010 42.9536 -112.8253 -23.1086 4.8369 ## USH00100448 43.5936 -115.9236 -26.2069 5.4769 ## USH00100470 44.0425 -111.2739 -21.5572 5.9258 ## USH00100803 42.3353 -111.3850 -21.6683 4.2186 ## USH00101408 44.5733 -116.6753 -26.9586 6.4566 ## USH00101956 47.6789 -116.8017 -27.0850 9.5622 ## USH00102845 46.5022 -116.3217 -26.6050 8.3855 ## USH00103143 46.0931 -115.5356 -25.8189 7.9764 ## USH00103631 42.9403 -115.3231 -25.6064 4.8236 ## USH00103732 42.5872 -111.7275 -22.0108 4.4705 ## USH00104140 42.5972 -114.1378 -24.4211 4.4805 ## USH00104295 42.3528 -114.5739 -24.8572 4.2361 ## USH00104670 42.7325 -114.5192 -24.8025 4.6158 ## sum_lat_long ## USH00011084 -4.3966 ## USH00012813 -5.7341 ## USH00013160 -3.6995 ## USH00013511 -3.2791 ## USH00013816 -2.7842 ## USH00015749 -1.2555 ## USH00017157 -1.0397 ## USH00017304 0.2200 ## USH00017366 -3.0053 ## USH00018024 -1.1186 ## USH00018178 -4.7422 ## USH00018323 -2.5647 ## USH00018380 -2.8042 ## USH00018438 -2.1322 ## USH00018469 0.5539 ## USH00020080 -28.8903 ## USH00021026 -27.6067 ## USH00021248 -21.7861 ## USH00021514 -26.8761 ## USH00021614 -25.7487 ## USH00023160 -24.8747 ## USH00023596 -24.4975 ## USH00024089 -23.6450 ## USH00024645 -27.2167 ## USH00024849 -23.1378 ## USH00025512 -25.8656 ## USH00026250 -28.5350 ## USH00026353 -26.3022 ## USH00026796 -26.2616 ## USH00027281 -25.8777 ## USH00027370 -27.0617 ## USH00027390 -25.2658 ## USH00027435 -23.2856 ## USH00027716 -25.9475 ## USH00028619 -26.7513 ## USH00028815 -27.1244 ## USH00029271 -24.5664 ## USH00029287 -27.1611 ## USH00029359 -25.3497 ## USH00029652 -30.4236 ## USH00030936 -4.7331 ## USH00031596 -5.7447 ## USH00031632 -2.5661 ## USH00032356 -5.7753 ## USH00032444 -6.4738 ## USH00032930 -6.4220 ## USH00034572 -3.4403 ## USH00034756 -8.0763 ## USH00035186 -4.0702 ## USH00035512 -6.7558 ## USH00035754 -6.1933 ## USH00035820 -3.1042 ## USH00035908 -7.9675 ## USH00036253 -5.8603 ## USH00036928 -6.7341 ## USH00040693 -32.7845 ## USH00040924 -29.3841 ## USH00041048 -31.0037 ## USH00041614 -27.0400 ## USH00041715 -30.5300 ## USH00041758 -32.8458 ## USH00041912 -30.2570 ## USH00042239 -31.9975 ## USH00042294 -31.6411 ## USH00042319 -28.8047 ## USH00042728 -30.7400 ## USH00042910 -31.7506 ## USH00042941 -32.1233 ## USH00043161 -32.6475 ## USH00043257 -31.3394 ## USH00043747 -31.7137 ## USH00043761 -29.9716 ## USH00043875 -32.6556 ## USH00044232 -29.8055 ## USH00044259 -30.9067 ## USH00044713 -29.7209 ## USH00044890 -31.0447 ## USH00044997 -32.4770 ## USH00045032 -31.5817 ## USH00045385 -30.8395 ## USH00045532 -31.6259 ## USH00045983 -29.3875 ## USH00046074 -32.3869 ## USH00046118 -28.2514 ## USH00046175 -32.6778 ## USH00046399 -33.1797 ## USH00046506 -30.8539 ## USH00046508 -30.6228 ## USH00046719 -32.3964 ## USH00046730 -33.4578 ## USH00046826 -32.7500 ## USH00047195 -29.4108 ## USH00047304 -30.1811 ## USH00047306 -31.5366 ## USH00047851 -33.7583 ## USH00047902 -33.6677 ## USH00047916 -33.4005 ## USH00047965 -32.6597 ## USH00048702 -28.6464 ## USH00048758 -29.3750 ## USH00048839 -32.1264 ## USH00049087 -32.4514 ## USH00049122 -32.4636 ## USH00049200 -31.9652 ## USH00049452 -32.1556 ## USH00049490 -30.6109 ## USH00049699 -31.1827 ## USH00049855 -30.2397 ## USH00049866 -29.3372 ## USH00050848 -13.6748 ## USH00051294 -15.1656 ## USH00051528 -14.4580 ## USH00051564 -11.9250 ## USH00051741 -17.1206 ## USH00052184 -17.0505 ## USH00052281 -14.8092 ## USH00052446 -12.7033 ## USH00053005 -12.9167 ## USH00053038 -11.9556 ## USH00053146 -17.9678 ## USH00053662 -16.8417 ## USH00053951 -17.7380 ## USH00054076 -12.4742 ## USH00054770 -12.9370 ## USH00054834 -13.5517 ## USH00055322 -17.1650 ## USH00055722 -17.7934 ## USH00057167 -14.0541 ## USH00057337 -16.4586 ## USH00057936 -14.7350 ## USH00058204 -18.3241 ## USH00058429 -15.7083 ## USH00059243 -10.5606 ## USH00062658 20.1833 ## USH00063207 20.9112 ## USH00067970 19.1772 ## USH00068138 21.1667 ## USH00072730 15.3416 ## USH00073595 14.8400 ## USH00075915 15.0733 ## USH00076410 15.5180 ## USH00079605 15.8325 ## USH00080211 -3.6948 ## USH00080228 -3.0558 ## USH00080478 -2.3447 ## USH00080611 -2.3783 ## USH00082220 -3.7695 ## USH00082850 -3.9408 ## USH00082915 -0.1839 ## USH00082944 0.7953 ## USH00083163 -2.4992 ## USH00083186 -3.6764 ## USH00083207 -1.2917 ## USH00084289 -1.9094 ## USH00084570 -5.5972 ## USH00084731 -0.8089 ## USH00085275 -1.3602 ## USH00086414 -1.3975 ## USH00086997 -5.1088 ## USH00087020 -3.2542 ## USH00087851 -2.3222 ## USH00088758 -2.3602 ## USH00088824 -3.0058 ## USH00088942 -0.5916 ## USH00090140 -1.0150 ## USH00090586 -2.1947 ## USH00091340 1.2659 ## USH00091500 -1.4133 ## USH00092318 1.3536 ## USH00092475 2.1392 ## USH00092966 0.5945 ## USH00093621 2.0406 ## USH00093754 1.6359 ## USH00094170 1.4161 ## USH00095874 1.4334 ## USH00095882 2.5031 ## USH00096335 0.2366 ## USH00097276 -1.1856 ## USH00097600 0.6939 ## USH00097847 2.5200 ## USH00098535 -0.2322 ## USH00098703 -0.4306 ## USH00098740 2.8467 ## USH00099141 2.3806 ## USH00099157 2.6206 ## USH00099186 0.5386 ## USH00099291 -0.7198 ## USH00100010 -18.2717 ## USH00100448 -20.7300 ## USH00100470 -15.6314 ## USH00100803 -17.4497 ## USH00101408 -20.5020 ## USH00101956 -17.5228 ## USH00102845 -18.2195 ## USH00103143 -17.8425 ## USH00103631 -20.7828 ## USH00103732 -17.5403 ## USH00104140 -19.9406 ## USH00104295 -20.6211 ## USH00104670 -20.1867 ## [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 1018 rows ] #closest &lt;- which(min(station$longitude + station$latitude)) "],["comparing-different-models.html", "Chapter 6 Comparing different models 6.1 Question 0 6.2 Question 1 6.3 Question 2 6.4 Question 3 6.5 Question 4 6.6 Question 5 6.7 Question 6", " Chapter 6 Comparing different models This homework is to meant for you practice some feature engineering with the different regression models. Context: the US is increasingly polarized and Twitter, a social media platform, is providing the space for people to voice their opinions or find like-minded individuals. In this assignment, we will use Twitter data non_retweets_dc_inaug_steal.csv on CourseWorks that has contains the token frequencies. This has been processed somewhat to minimize the data size. The non-token variables are created_at, like_count, reply_count, retweet_count, tweet_body. Please re-download this dataset given there were issues with the version given in class. The tweets are collected overtime with the query words “steal”, “inaguration”, and “washington dc” using Twitter’s Recent Search API. Calls were made once a day around midnight and there is a limit on the number of queries that can be made freely. 6.1 Question 0 What dates do the tweets cover? Please visualize the temporal distribution of the tweets with the intent to understand where most Tweets in the dataset came from in time. 6.2 Question 1 Please combine all the token frequencies that contain the string inaug into a single column. You should remove the redundant tokens after this summarization. Please select the method of “combining” that is consistent with the “token frequency” concept. Please report the fraction of records that have non-zero frequencies for this new column. 6.3 Question 2 We want to discover the topics correlated with the inauguration over time. To achieve this, please model your new column from Question 1 against all other tokens (i.e. exclude the non-token variables) using the following models. This is time-consuming! You should use Sys.time() to have a realistic expectation of how long each model will take. Please plot the MSE from a 5-fold cross validation to compare the prediction accuracy between - OLS - Please ignore the rank deficient warning, this is a warning of high-collinearity that is common with high dimensional data and why OLS shouldn’t do well. - stepwise regression using AIC as the objective # ols = lm(y ~ ., df) # ols_summ = summary(ols)$coefficients # okay_features = rownames(ols_summ)[ols_summ[, 4] &lt; 0.05] # init_formula = paste(&#39;y ~&#39;, paste(okay_features, collapse=&#39;+&#39;)) # init_mod = lm(init_fomrula, df) # # &quot;~.&quot; sets the upper model for the `scope` as using all of the features # step_mod &lt;- step(init_mod, &quot;~.&quot;) Lasso corresponding to \\[\\lambda\\]=lambda.min from cv.glmnet hint: predict.cv.glmnet hint: you may want to convert the training X matrix into sparse matrices, i.e. Matrix(as.matrix(df_sans_y), sparse=TRUE) to speed things up. This essentially avoids many multiplications when a 0 is involved. Ridge regression corresponding to \\[\\lambda\\]=lambda.min from cv.glmnet Please do not normalize your features for this problem but use the raw frequencies. hint, adding some print statements can help with unnecessary panics: # library(caret) # test_folds &lt;- createFolds(DEP_VARIABLE, k=5) # for(i in seq_along(test_folds)){ # # # Some code that isolates the test/train data! # # print(paste(&quot;cross validation fold&quot;, i)) # t0 &lt;- Sys.time() # ols &lt;- lm(TRAIN_DEP_VARIABLE ~ TRAIN_INDEP_VARIABLE) # print(paste(&#39;running OLS took&#39;, Sys.time() - t0)) # } Side comment: - You may want to see how the lambda.1se compares. - (personal observation) Normalizing the features does help with the optimization but our features are all token frequencies so it’s not as big of a concern. 6.4 Question 3 For the algorithm that performed the best, please retrain the model with the following requirements: - Use all of the data, i.e. do not reserve data for test/train. - You may need to normalize the features. If Lasso/Ridge was best, note that sparsity will not hold after you normalize. Please plot the distribution of the coefficients. 6.5 Question 4 Please list out the top tokens corresponding to the strongest non-zero coefficients. Side comment: If you are not an American, what do these tokens and their coefficients suggest about the inauguration? 6.6 Question 5 Please write a function that represents the objective function for Ridge regression, i.e. your function should take in a vector of coefficients, a matrix X, and a vector Y and return a real number. - Please make sure you fit an intercept within the function but do not include the intercept in your regularization term for the objective. - Please add an additional argument called shrink_target that allows us to change the shrinkage of the coefficients to arbitrary vector. - Please make sure you set the default for this argument to align with the usual Ridge regression. 6.7 Question 6 Please perform PCA on the token frequencies without normalizing. - Plot the eigenvalues in order (not the cumulative eigenvalues) of their magnitude, how many components might have interesting features. There are many correct answers but also many wrong answers here depending on your plot. - Analyze the loadings from the first 2 components, do they seem meaningful to you given the dataset? - if yes, what key tokens are associated with these 2 components and why do they carry meaning? - if no, how could you modify the token frequencies to get more meaningful principle components? Side comment (things to think about): - what would you do after identifying the components? - again, if you were not involved in American politics, how would you have done some of these steps? - Twitter data is not like normal language, e.g. people do not use complete sentences. How might this look if we had journal articles? "],["homework-3-project.html", "Chapter 7 Homework 3 Project 7.1 HW1 7.2 R Markdown", " Chapter 7 Homework 3 Project library(caret) library(tidyverse) tweets_raw &lt;- read_csv(&quot;Datasets/non_retweets_dc_inaug_steal.csv&quot;) ## ## ── Column specification ────────────────────────────────────────────────── ## cols( ## .default = col_double(), ## `#abddarb` = col_character(), ## created_at = col_datetime(format = &quot;&quot;), ## tweet_body = col_character() ## ) ## ℹ Use `spec()` for the full column specifications. ## Warning: 2 parsing failures. ## row col expected actual file ## 83 -- 843 columns 781 columns &#39;Datasets/non_retweets_dc_inaug_steal.csv&#39; ## 84 -- 843 columns 63 columns &#39;Datasets/non_retweets_dc_inaug_steal.csv&#39; dim(tweets_raw) ## [1] 16005 843 7.1 HW1 This homework is to meant for you to brush up some prerequisites. If some of these topics are new to you, feel free to ask on Piazza how to approach these. Context - Is gold price inversely related to the market? There’s a belief that the money from the stock market will escape to gold when the stock market is not doing well. The demand for gold and the expectations for the market are often reflected in the pricing of the assets, i.e. high demand yields high gold prices and upward expectations also lead to higher stock prices. 7.1.1 Q1 Please use the ‘TIME_SERIES_WEEKLY’ API listed on Alpha Vantage to get the weekly time series data for ‘VOO’: an arbitrarily chosen ETF that tracks the market ‘GDXJ’: an arbitrarily chosen ETF for gold For this problem, simply show the code for your query and print out the number of weeks of data for each time series. Your API key should NOT appear in your solutions but the URL you’re using and the query should be shown. Hint: You will need to claim a free API key before you can query data The functions in httr should be helpful, here is some sample code if you have not done so before. #library(httr) # url_VOO &lt;- &quot;https://www.alphavantage.co/query?function=TIME_SERIES_WEEKLY&amp;symbol=VOO&amp;apikey=SEDREZCZEW7NAOQK&quot; # url_GDXJ &lt;- &quot;https://www.alphavantage.co/query?function=TIME_SERIES_WEEKLY&amp;symbol=GDXJ&amp;apikey=SEDREZCZEW7NAOQK&quot; # # responce &lt;- GET(url = url_VOO, query = params) function_name &lt;- &quot;TIME_SERIES_WEEKLY&quot; stock_ticker &lt;- &quot;VOO&quot; my_data_type &lt;-&quot;csv&quot; output_size &lt;- &quot;full&quot; api_call &lt;- paste0(&quot;https://www.alphavantage.co/query?function=&quot;, function_name, &quot;&amp;symbol=&quot;, stock_ticker, &quot;&amp;outputsize=&quot;, output_size, &quot;&amp;apikey=&quot;, api_key, &quot;&amp;datatype=&quot;, my_data_type) VOO &lt;- read.csv(url(api_call)) head(VOO) ## timestamp open high low close volume ## 1 2021-03-05 354.55 359.3900 341.915 352.69 30781752 ## 2 2021-02-26 355.84 360.5800 347.710 349.59 26216064 ## 3 2021-02-19 362.15 362.3700 356.490 358.59 13120646 ## 4 2021-02-12 357.92 361.1900 356.300 361.05 12023707 ## 5 2021-02-05 343.63 357.1312 341.400 356.44 13716229 ## 6 2021-01-29 352.71 354.6450 338.570 340.18 20225448 #For VOO function_name &lt;- &quot;TIME_SERIES_WEEKLY&quot; stock_ticker &lt;- &quot;GDXJ&quot; my_data_type &lt;-&quot;csv&quot; output_size &lt;- &quot;full&quot; api_call &lt;- paste0(&quot;https://www.alphavantage.co/query?function=&quot;, function_name, &quot;&amp;symbol=&quot;, stock_ticker, &quot;&amp;outputsize=&quot;, output_size, &quot;&amp;apikey=&quot;, api_key, &quot;&amp;datatype=&quot;, my_data_type) GDXJ &lt;- read.csv(url(api_call)) head(GDXJ) ## timestamp open high low close volume ## 1 2021-03-05 46.54 46.8700 44.000 45.61 40024061 ## 2 2021-02-26 48.29 50.6500 45.065 45.76 48304711 ## 3 2021-02-19 49.15 50.3782 47.040 47.54 24748594 ## 4 2021-02-12 51.30 51.6700 49.250 50.25 26097487 ## 5 2021-02-05 53.21 54.3200 48.280 50.37 39557597 ## 6 2021-01-29 50.65 52.3100 47.590 50.08 36177470 7.1.2 Q2 Please plot the close price for VOO against the different weeks and overlay the regression line for this scatter plot. You do not need to label your week index but the prices should be labeled. library(ggplot2) library(dplyr) VOO$timestamp &lt;- as.Date(VOO$timestamp) ggplot(VOO, aes(x = timestamp,y = close)) + geom_line() + xlab(&quot;day&quot;) + ylab(&quot;price&quot;) + geom_smooth(method = &quot;lm&quot;, formula = y~x) 7.1.3 Q3 Please plot the residuals from the regression in Q2 against the close price of GDXJ. label your axes with units. Your title should include the correlation value, rounded to the nearest hundredth. Please show the code that demonstrates your decision on merging the 2 time series. linear &lt;- lm(VOO$close ~ VOO$timestamp) subset_GDXJ &lt;- GDXJ[1:length(linear$residuals),] #subset_GDXJ gold_vs_stock &lt;- lm(subset_GDXJ$close ~ linear$residuals) summary(gold_vs_stock) ## ## Call: ## lm(formula = subset_GDXJ$close ~ linear$residuals) ## ## Residuals: ## Min 1Q Median 3Q Max ## -20.0033 -6.6721 -0.3523 5.3648 28.1189 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 33.12556 0.40671 81.45 &lt; 2e-16 *** ## linear$residuals 0.09952 0.01813 5.49 6.16e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.512 on 545 degrees of freedom ## Multiple R-squared: 0.05241, Adjusted R-squared: 0.05067 ## F-statistic: 30.14 on 1 and 545 DF, p-value: 6.162e-08 plot(x = linear$residuals, y = subset_GDXJ$close, xlab=&quot;Difference in stock market performance to linear&quot;, ylab = &quot;Closing price of GDXJ&quot;, main = &quot;Correlation coeff: 0.049&quot;, abline(lm(subset_GDXJ$close ~ linear$residuals))) 7.1.4 Q4 Relying only on the scatter plot, would you say the belief between gold and the market is supported or rejected? Please explain. Relying on the scatter plot and the line I plotted, gold is not inversely related. In fact, when the market it beating its performace, on average gold is closing higher as well. Also, when the stock market is doing really poorly (compared to the linear regression) gold also closes really low as seen by the strong tail around -40. 7.2 R Markdown library(&#39;jsonlite&#39;) library(&#39;RJSONIO&#39;) votes &lt;- fromJSON(&#39;Datasets/votes.json&#39;) senator_id &lt;- sort(names(votes)) session_id &lt;- sort(unique(names(unlist(unname( votes ))))) voting_matrix &lt;- matrix(NA, nrow = length(session_id), ncol = length(senator_id)) rownames(voting_matrix) &lt;- session_id colnames(voting_matrix) &lt;- senator_id # Making the matrix for(i in senator_id) { for (j in session_id) { if (j %in% names(votes[[i]])) { voting_matrix[j, i] &lt;- votes[[i]][[j]] } } } # Dimensions print(paste( &quot;Number of rows = &quot;, as.character(nrow(voting_matrix)), &quot;, number of columns = &quot;, as.character(ncol(voting_matrix)) )) ## [1] &quot;Number of rows = 803 , number of columns = 231&quot; # Percentage of matrix that does not contain -1, 1 or 0 print(paste(as.character(( sum(is.na(voting_matrix)) + sum(voting_matrix == -9999, na.rm = TRUE) ) / ( nrow(voting_matrix) * ncol(voting_matrix) ) * 100), &quot;%&quot;)) ## [1] &quot;56.737451008933 %&quot; cor_matrix &lt;- cor(voting_matrix, use = &quot;pairwise.complete.obs&quot;) ## Warning in cor(voting_matrix, use = &quot;pairwise.complete.obs&quot;): the standard ## deviation is zero lower_cor &lt;- cor_matrix[lower.tri(cor_matrix)] hist(lower_cor, main = &quot;Histogram of Correlation Values&quot;, xlab = &quot;Correlation&quot;) left &lt;- lower_cor[lower_cor &lt; 0.25] right &lt;- lower_cor[lower_cor &gt;= 0.25] quantile(left, na.rm = TRUE) ## 0% 25% 50% 75% 100% ## -1.000000000 -0.193671052 -0.086741418 -0.002155501 0.249932462 quantile(right, na.rm = TRUE) ## 0% 25% 50% 75% 100% ## 0.2501651 0.4675799 0.5855325 0.6795527 1.0000000 cor_with_mitch &lt;- cor_matrix[&quot;S174&quot;, ] not_NA &lt;- names(cor_with_mitch[!is.na(cor_with_mitch)]) mitch_cor_mat &lt;- cor_matrix[ , not_NA] mitch_order &lt;- rev(names(sort(mitch_cor_mat[&quot;S174&quot;, ]))) mitch_cor_mat &lt;- mitch_cor_mat[ mitch_order , mitch_order]# Visualize image(mitch_cor_mat) voters &lt;- fromJSON(&quot;Datasets/voters.json&quot;) cor_with_mitch &lt;- mitch_cor_mat[1, ] # Defining Those with Negative Correlation with Mitch neg_cor_with_mitch &lt;- names(cor_with_mitch[cor_with_mitch &lt; 0]) # Defining Possible Republicans poss_republican &lt;- names(cor_with_mitch[cor_with_mitch &gt;= 0.2 &amp; names(cor_with_mitch) != &quot;S174&quot;]) # Finding Those with avg greater than 0.2 correlation with poss_repubicans subset_mat &lt;- mitch_cor_mat[neg_cor_with_mitch, poss_republican] averages &lt;- rowMeans(subset_mat, na.rm = TRUE) senators_criteria &lt;- names(averages[averages &gt; 0.2]) senator_info &lt;- voters[senators_criteria] print(senator_info) ## $S374 ## first_name last_name party state ## &quot;Tom&quot; &quot;Cotton&quot; &quot;R&quot; &quot;AR&quot; ## ## $S118 ## first_name last_name party state ## &quot;Orrin&quot; &quot;Hatch&quot; &quot;R&quot; &quot;UT&quot; ## ## $S300 ## first_name last_name party state ## &quot;Richard&quot; &quot;Burr&quot; &quot;R&quot; &quot;NC&quot; ## ## $S382 ## first_name last_name party state ## &quot;Ben&quot; &quot;Sasse&quot; &quot;R&quot; &quot;NE&quot; ## ## $S383 ## first_name last_name party state ## &quot;Dan&quot; &quot;Sullivan&quot; &quot;R&quot; &quot;AK&quot; lisa_id &lt;- names(which(unlist(voters) == &quot;Murkowski&quot;)) lisa_id &lt;- &quot;S157&quot; cor_with_lisa &lt;- cor_matrix[lisa_id, ] cor_max &lt;- names(which.max(cor_with_lisa[names(cor_with_lisa) != lisa_id])) cor_min &lt;- names(which.min(cor_with_lisa)) ols &lt;- lm(voting_matrix[ , lisa_id] ~ voting_matrix[ , cor_max] + voting_matrix[ , cor_min]) summary(ols) ## ## Call: ## lm(formula = voting_matrix[, lisa_id] ~ voting_matrix[, cor_max] + ## voting_matrix[, cor_min]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.22610 -0.00646 0.06774 0.06774 1.48007 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.26319 0.04690 5.612 8.27e-08 *** ## voting_matrix[, cor_max] 0.70617 0.04556 15.500 &lt; 2e-16 *** ## voting_matrix[, cor_min] -0.03710 0.03523 -1.053 0.294 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4139 on 165 degrees of freedom ## (635 observations deleted due to missingness) ## Multiple R-squared: 0.6113, Adjusted R-squared: 0.6065 ## F-statistic: 129.7 on 2 and 165 DF, p-value: &lt; 2.2e-16 ols &lt;- lm(voting_matrix[ , lisa_id] ~ voting_matrix[ , cor_max]) res &lt;- voting_matrix[ , lisa_id] - cbind(1, voting_matrix[ , cor_max]) %*% ols$coefficients cor_with_res &lt;-function(x){return(cor(voting_matrix[ , x], res, use = &quot;pairwise.complete.obs&quot;))} senator_id_nolisa &lt;- senator_id[senator_id != lisa_id] all_cor_with_res &lt;- sapply(senator_id_nolisa, cor_with_res) ## Warning in cor(voting_matrix[, x], res, use = &quot;pairwise.complete.obs&quot;): ## the standard deviation is zero max_cor_with_res &lt;- names(which.max(all_cor_with_res)) ols2 &lt;- lm(voting_matrix[ , lisa_id] ~ voting_matrix[ , cor_max] + voting_matrix[ , max_cor_with_res]) summary(ols2) ## ## Call: ## lm(formula = voting_matrix[, lisa_id] ~ voting_matrix[, cor_max] + ## voting_matrix[, max_cor_with_res]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.19242 0.05088 0.05088 0.05088 0.91741 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.01585 0.04289 0.370 0.712 ## voting_matrix[, cor_max] 0.37835 0.05110 7.404 6.06e-12 ## voting_matrix[, max_cor_with_res] 0.55491 0.05926 9.364 &lt; 2e-16 ## ## (Intercept) ## voting_matrix[, cor_max] *** ## voting_matrix[, max_cor_with_res] *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3441 on 168 degrees of freedom ## (632 observations deleted due to missingness) ## Multiple R-squared: 0.7304, Adjusted R-squared: 0.7272 ## F-statistic: 227.6 on 2 and 168 DF, p-value: &lt; 2.2e-16 "]]
