--- 
title: "Data Mining"
author: "Noah Love"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
---

# A Brief Introduction


<!--chapter:end:index.Rmd-->

# Tidyverse tools

```{r}
library(tidyverse)
```

## Select

### Original Data Set

```{r}
head(storms)

```

You can use (data source, columns)

```{r}
dplyr::select(storms, name, pressure)
```

We can also do (data source, -columns) to choose all but that column, i.e.:

```{r}
dplyr::select(storms, -name)
```

### Other select functions:

| Function call | Description                                              |
|---------------|----------------------------------------------------------|
| \-            | select everything but                                    |
| :             | select range                                             |
| contains()    | Select columns whose name contains a character string    |
| ends_with()   | Select columns whose name ends with a string             |
| everything()  | Select every column                                      |
| matches()     | Select columns who name matches a regular expression     |
| num_range()   | Select columns named x1,x2,x3...                         |
| one_of()      | Select columns whose names are in a group of names       |
| starts_with() | Select columns whose name starts with a character string |

: Other useful selection function

## Filter

This will apply a test to every row in the data frame and return just the rows that pass the test. You can combine by putting a comma in the state. It acts as AND.

```{r}
dplyr::filter(storms, wind >= 50,
              name %in% c("alberto", "Alex", "allison"))
```

### Logical Tests in R

The columns on the right show Boolean operator, these combine 2 or more logical tests into a single one, so you get true or false. The columns on the left actually do logical operations.

| Logical Test | ?Comparison           | Boolean Operator | ?base::Logic |
|--------------|-----------------------|------------------|--------------|
| \<           |                       | &                | boolean and  |
| \>           | Greater than          | \|               | boolean or   |
| ==           | Equal to              | xor              | exactly or   |
| \<=          | Less than or equal    | !                | not          |
| \>=          | Greater than or equal | any              | any true     |
| %in%         | Group membership      | all              | all true     |
| !=           | Not equal             |                  |              |
| is.na        | is NA                 |                  |              |
| !is.na       | Is not NA             |                  |              |

: Logical tests in R

## Mutate

The mutate function takes your data frame and it returns a copy of the data with, with additional variables that you derive from the variable set that exists in the data. So mutate is there for anything you can derive from the data you already have, and want to make a new column from it.

```{r}
storms %>% 
  mutate(ratio = pressure / wind) %>% 
  select(name, pressure, wind, ratio)
```

You can also make multiple columns at the same time, even using columns that are created at the same time, as long as those come before chronologically, i.e.

```{r}
storms %>% 
  mutate(ratio = pressure / wind, inverse = ratio^-1) %>% 
  select(name, pressure, wind, ratio, inverse)
```

One thing to note: mutate doesn't affect the original data frame. It simply returns a new temporary dataframe. If you want to save the columns you need to assign it to a new data frame (or overwrite the original)

### Useful mutate functions:

| Function                                               | Description                       |
|--------------------------------------------------------|-----------------------------------|
| pmin(), pmax()                                         | Element wise min and max          |
| cummin(), cummax()                                     | cumulative min and max            |
| cumsum(), cumprod()                                    | Cumulative sum and product        |
| between()                                              | Are values between a and b?       |
| cume_dist()                                            | Cumulative distribution of values |
| cumall(), cumany()                                     | Cumulative all and any            |
| cummean()                                              | Cumulative mean                   |
| lead(), lag()                                          | Copy with values one position     |
| ntile()                                                | Bin vector into n buckets         |
| dense_rank(), min_rank(), percent_rank(), row_number() | Various ranking methods           |

## Summarise / Summarize

Summarize allows us to take a data frame and calculate a summary statistic from it and get back a new data frame that is much smaller.

```{r}
storms %>% 
  summarize(median = median(pressure), variance = var(pressure), n = n())
```

Works very similar to mutate.

### Useful summary functions

| Function     | Description                                 |
|--------------|---------------------------------------------|
| min(), max() | Minimum and maximum values                  |
| mean()       | Mean value                                  |
| median()     | Median Value                                |
| sum()        | Sum of values                               |
| var, sd()    | Variance and standard deviation of a vector |
| first()      | First value in a vector                     |
| last()       | Last value in a vector                      |
| nth          | Nth value in a vector                       |
| n()          | The number of values in a vector            |
| n_distinct() | The number of distinct values in a vector   |

: Useful summary functions, top 5 are specific to dplyr

## Arrange()

This doesn't add or subtract from your data, but helps you organize your rows!

```{r}
storms %>% 
  arrange(desc(wind)) %>% 
  select(name, wind, pressure, year)
```

This way, ties are just ordered in the way they originally appeared in the dataframe. Alternatively, you can give a second column to sort by to break those ties, such as by year:

```{r}
storms %>% 
  arrange(desc(wind), desc(year)) %>% 
  select(name, wind, pressure, year)
```

## Group_by()

A very powerful function to get summary statistics from just certain groups. Say we want to see median wind speed based on year for example:

```{r}
storms %>% 
  group_by(year) %>% 
  summarize(median = median(pressure), variance = var(pressure), n = n())
```

We can save that in a in data frame and graph it too!

```{r}
storms_graph <- storms %>% 
  group_by(year) %>% 
  summarize(median = median(pressure), variance = var(pressure), n = n())

ggplot(data = storms_graph, mapping = aes(x = year, y = median)) + 
  geom_point() + 
  geom_smooth() + 
  labs(title = "Median wind speed of storms by year")
```

You can also group by multiple things, just add a comma!

<!--chapter:end:02-Tidyverse-Sheet.Rmd-->

# Linear Regresssion

```{r}
library(tidyverse)
```

We have a sample of 100, 5 different variables (p), and an X matrix. We are making uniform variables, and we will jam them into a matrix such that the number of rows is equal to n.

```{r}
n <- 100
p <- 5
X <- matrix(runif(p * n),
            nrow=n)
```

Then we can create some Y's based on betas and noise. To do so, we need betas. We will make all of them zeros, except for one of them! We also need to define our noise.

```{r}
beta <- rep(0, p)
beta[sample(p, 1)] <- 1
noise <- rnorm(n)

# %*% Matrix multiplication in R
#X is a matrix of 100 by 5
#Beta is a matrix of 5
Y <- X %*% beta + noise

```

The above is generating data according to a linear model! So what does this do? We have X and a bunch of 0 betas, except for one special one. The Y then will depend only on the one column!

If we don't know which beta matters, we just fit y against x and print out a summary.

```{r}
ols <- lm(Y ~ X)
summary(ols)
```

So only one of them is statistically significant! This shows it should be the 5th! We can see the truth:

```{r}
beta
```

This is a good example of fitting basic regression.

Lets look at the y and subtract the the coefficients to find the residuals: note: length(ols\$coefficients) is 6, but dimensions of X is going to be 5.So we will need to cbind. So this is y minus the fitted data from the regression. When we do summary of ols, we get a bunch of estimates. The intercept and the estimated coefficients. These are estimates from the regression based on the data. Using these, we can recover and estimate and then find the difference to see what the residuals are!

```{r}
resid <-  Y - cbind(1, X) %*% ols$coefficients
#this is the manual version of:
resid <-  ols$residuals
```

Now let's do the residuals against the true beta! We are subtracting the x values against the true beta. True beta means this is the beta that created the data. This is how god created the world. It is the actual physical value. The above residuals are from the regression of the fitted data.

```{r}
resid_from_truth <- Y - X %*% beta
```

We can plot this as well!

```{r}
plot(density(resid_from_truth))
lines(density(resid), col = "red")
abline(v = 0)
```

This might not be enough contrast for us to tell. How can we quantify the difference? Let's see the difference between the two:

```{r}
mean(abs(resid))
mean(abs(resid_from_truth))
```

We want the smaller value! The simulated data based values will always have a smaller residual mean than the real values! This is rather disturbing. The fitted coefficients from your regression will always be better than the "true" coefficients. Why is this bad?

Normally we want to use regression to find the natural coefficients or natural facts about the world. These are based on some "truth". If you can collect noisy data but our algo prefers the fitted data rather than the truth we have a problem. We want our error minimized at the "truth". Our regression here doesn't like the true answer and it prefers something else. It actually prefers the training data.

This is because we use the same data to train and evaluate. Our data is just optimized for this one thing. If we know how you are going to evaluate me, I will just optimize for that specific thing.

So let's generate another set of data: new Y using the same beta and x but with new noise values: Testing data generated from the same population but not used for training the model

```{r}
new_noise <- rnorm(n)

new_Y <- X %*% beta + new_noise

new_resid <- new_Y - cbind(1, X) %*% ols$coefficients
new_resid_from_truth <-  new_Y - X %*% beta

mean(abs(new_resid))
mean(abs(new_resid_from_truth))
```

Our takeaway is don't always cross over your data because you risk overfitting.

## Non linear data

Let's create new data that is nonlinear. This is mini sine data.

```{r}
n <- 100
X <- runif(100, max = 2 * pi / 4 * 3)
Y <- 0.1+-3 * sin(X) + rnorm(n, sd = 0.5)
```

Then we can recreate our linear regression and the plot.

```{r}
ols <- lm(Y ~ X)
summary(ols)

plot(X, Y)
abline(lm(Y ~ X))
```

As expected our linear model created a line that doesn't look to fit too well. Let's look at the residual plot instead. We

```{r}
par(mfrow=c(1,2))
plot(ols$residuals)
abline(h=0)
plot(X, ols$residuals) 
abline(h = 0)
```

Xs were generated in a random order so we we just use the index, it looks random and normal. But when we sort by x value, we see that there is definitely a problem and we need to add a quadratic term.

We can then find the mean squared error:

```{r}
n <- 100
Y <- 0.1+-3 * sin(X) + rnorm(n, sd = 0.5)

#Y - y hat squared
mean((Y - ols$fitted.values) ^ 2)
```

**Plot the MSE vs the number of polynomials used on the x axis. Do this for both the training vs the testing data.**

We can regress on matrices in R which makes this really easy. So let us create an X_matrix, where for each degree, we raise x to that number of degrees. This matrix will be n rows and degrees columns.

```{r}
n <- 100
degrees <- 1:50

X <- runif(n, max = 2 * pi / 4 * 3)
Y <- 0.1+-3 * sin(X) + rnorm(n, sd = 0.5)
new_Y <- 0.1+-3 * sin(X) + rnorm(n, sd = 0.5)
X_mat <- sapply(degrees, function(i)
  X ^ i)
```


For example

```{r}
plot(X, X_mat[,5])
```

We can do this through a loop.

```{r}
#create an empty vector
MSEs <- rep(NA, length(degrees))
#Create empty vector for tests
test_MSEs <- MSEs

for (i in seq_along(degrees)) {
  # regress for each power on each loop
  ols <- lm(Y ~ X_mat[, 1:i])
  # record the MSEs
  MSEs[i] <- mean(ols$residuals ^ 2)
  # do again for new set, only word because we use the same X
  new_errors <- new_Y - ols$fitted.values
  # record
  test_MSEs[i] <- mean(new_errors ^ 2)
}
```

Plot in base R

```{r}
plot(degrees, MSEs, type = "b",
     ylim = c(0, max(test_MSEs)))
lines(degrees, test_MSEs, type = "b", col = "red")
legend("topright",
       legend = c("Test", "Train"),
       fill = c("red", "black"))
```

Plot in tidyverse

```{r eval=FALSE, include=FALSE}
ggplot(data = NULL, mapping = aes(x = degrees, y = MSEs))+
  geom_point(aes(color = "black")) + geom_line(aes(color = "black")) +  
  geom_point(mapping = aes(x = degrees, y = test_MSEs, color = "red")) + geom_line(mapping = aes(x = degrees, y = test_MSEs, color = "red")) +
  scale_color_manual(name = "model",
                     breaks = c("Test", "Train"),
                     values = c("black" = "black", "red" = "red"))
```


```{r}
summary(ols)
```

This should be really concerning. It would mean strong colinearity. R is doing us a favor and automatically dropping some redundant features.

Create data under the so called regression model. Regression is an algo, but it is also a model for creating data. ODS is a least square algoristhm.

```{r}
n <- 100
p <- 5
# God knows this!
params <- runif(p, -10, 10)

features <- matrix(rnorm(n * p),
                   nrow=n, ncol=p)
X <- features
X1 <- matrix(rnorm(n * p),
              nrow=n, ncol=p)

noise <- rnorm(n)
noise1 <- rnorm(n)
Y1 <- X %*% params + noise
Y2 <- 0
for(i in seq_len(p)){
  Y2 <- Y2 + params[i] * X1[, i]
}
Y2 <- Y2 + noise1
plot(X[,1], Y1)
points(X1[, 1], Y2, col="red")


```

```{r}
df <- as.data.frame(cbind(Y,X))

dim(df)

head(df,2)

names(df) <- c("Y", paste0("X", 1:p))

ols <- lm(Y ~ ., df)
summary(ols)

class(ols)
predict(ols)

#if you pass in a lm or glm, predict will use predict.lm or predict.glm anyways. It is smart. 

#There is an argument in predict that uses "new data". You need to pass in what the new data is. It should be tempting for us to just pass in X1. 
#This shouldn't work! But why?
#predict(ols, newdata = as.data.frame(X1))

#convert to df

#now there is an error that we don't know what X2 is. The data frame you are passing it needs to have the same names that you are training on. 

names(df)

df1  <- as.data.frame(X1)
names(df1) <- names(df)[-1]

test_preds <- predict(
  ols, 
  newdata = df1)

#The data you are passing, you need the data to look identical to the data you trained the model with. The names of the data frames must agree. 
#classic workflow
plot(test_preds , Y2)

test_errors <- Y2 - test_preds
test_rmse <- sqrt(mean(test_errors ^ 2))
test_rmse


sd(noise)
sd(noise1)
#You cannot reduce beyond this. 

```

R will automatically throw out extremely high colinearity instances. In the real world this would be rare. This is unique to R.

```{r}
names(ols)

#probably the most important
ols$coefficients

plot(ols$coefficients, c(0,params))
abline(a = 0, b=1)

```

```{r}
train_features <- cbind(1,as.matrix(df[,-1])) #take out y column
#fitted_vals <- train_features %*% 
  
fitted_vals <- train_features %*% ols$coefficients
sum(abs(fitted_vals - ols$fitted.values))

res <- df$Y - ols$fitted.values
sum(abs(res - ols$residuals))
plot(ols$residuals)
abline(h = 0)

```

```{r}
#you can also put plot onto the regression function itself
plot(ols)
#residuals vs fitted values. This is what we saw earlier but much fancier

#QQ Plot

#scale location not that important

#leverage to look for outliers
```

#### Traps

```{r}
df_missing <- df
df_missing[20, "Y"]  <- NA #purposeffully lose the value
ols <- lm(Y ~., df_missing)
length(ols$residuals)
#lm drops missing value before matrix multiplication. So the residuals will change. 


```

### Interactions + subtracting variables

#### Another trap

```{r}
ols <- lm(Y ~ X1 + X2 + X2*X3, df)
ols <- lm(Y ~ X1 + X2 + X2:X3, df)

summary(ols)
test_preds <- predict(ols, df1)
head(df1,2)
#when you start manipulating the data inbetween then you get the problems

#If you do feature engineering for test, then ADD SOMETHING
```

```{r}
ols <- lm(Y ~ . - X4, df)
summary(ols)

#get rid of intercept
ols <- lm(Y  ~ . -1, df)
summary(ols)
```

INSERT STUFF ABOUT INTERACTION TERMS: COLON THING

## Last trap, ording of the data

```{r}
x <- runif(n, -2 ,2)
y <- x^2 * 3

ols <-  lm(y~x)
summary(ols)

predict(ols, data.frame(x = runif(n,-2,2)))

predict(ols, data.frame(x = runif(n, -2,2)))

plot(x,y)

plot(ols$residuals)
#we should expect the residuals to be quadratic as well
# we need to order the data correctly

plot(ols$fitted.values,
     ols$residuals)

#remember the reisdualds are ordered the same as the data. If the data was random, then the residuals will be random. 

plot(ols)

#naming comment
#If we decided to 
dim(X)
length(Y)


ols <- lm(Y ~ X[,1:2])
summary(ols)
#the naming is a mess!

wrong_stuff <- predict(ols, data.frame("X[, 1:2]" = 1:3,
                        "X[, 1:2]2" = 1:3))
                        

mysterious_vals <- predict(ols)                       
sum(abs(mysterious_vals - ols$fitted.values))
```

### Missing

If there is no overlap in the data. Then there is no overlapping data. You can't run regression in that case.

you should probably have a reason to say you need a certain amount of overlap. The best way to get there is to find the needed level of confidence and then back in the answer.

### GLM

```{r}
inv_logit <- function(x) {
  return(exp(x) / (1 + exp(x)))
}
y <- rbinom(n, 1, prob = inv_logit(X %*% params))
plot(X[, 1], y)
plot(X[, 2], y)

df <- as.data.frame(cbind(y, X))
names(df) <- c("Y", paste0("X", 1:5))
log_reg <- glm(Y ~ ., df,
               family = binomial(logit))

summary(log_reg)
predict(log_reg)
myst_vals <- predict(log_reg, type = "response")

X_test <- matrix(rnorm(n * p), nrow = n)
X_test_df <- as.data.frame(X_test)
names(X_test_df) <- names(df)[-1]

test_preds <-
  predict(log_reg, type = "response", newdata = X_test_df)
head(test_preds)
params

```

<!--chapter:end:03-linear-regression.Rmd-->

# Classification

## Music dataset

From the [CORGIS data project](https://corgis-edu.github.io/corgis/) I've obtained a [music dataset](https://corgis-edu.github.io/corgis/csv/music/).

```{r}
music <- read.csv("Datasets/music_hottest_song_only.csv")
y <- as.numeric(music$artist.terms == "hip hop")
x <- music$song.hotttnesss
```

## Logistic Regression Review

$Y \sim Binomial(n=1, p(X))$

$p(X) = \frac{exp(X\beta)}{\exp(X\beta) + 1}$

```{r}
logit <- function(x){exp(x) / (1 + exp(x))}
logistic_obj <- function(params, X, y){
  xbeta <- cbind(1, X) %*% params
  p <- logit(xbeta)
  return(-sum(dbinom(y, size=1, prob=p, log=TRUE)))
}
optim(c(1, 1), logistic_obj, X=x, y=y)


```

```{r}
#Wayne created in class
my_model <- glm(y ~ x,
                family = binomial(logit))
summary(my_model)
```

How to run logistic regression in R?

```{r}
train <- sample(c(TRUE, FALSE), nrow(music), replace = TRUE)


mod <- glm(y ~ x, family = binomial(link = "logit"),
           subset = train)
#To use the model, we pass it to the predict function. We pass in the new data
#We pass in the compliment of the subset we used (train = !test)
#We also pass in response. 

#What is the probability that you think they are a 1 vs a 0
test_probs <-
  predict(mod, newdata = data.frame(x = x[!train]), type = "response")

class(test_probs)

#This should feel absolutely like it is 0 and not one
hist(test_probs)

#Round the numbers
test_pred <- test_probs > 0.5

#Head the numbers
head(test_pred)

#How good is my prediction
#First, how many predications did we get correct

same_as_data <- as.numeric(test_pred) == y[!train]
mean(same_as_data)

#About 96 percent. Seems really good.



```

Then we ask is the data unbalanced if you do you this for cancer in the general population for example. Could be bad if you have 96 percent accuracy

We call this a classification error. The histogram should be a red flag. We are saying everything is 0, and then are just wrong when its 1. Iin this case, every song is not hip hop, and we are right 96% of the time.

|               | Y = 1 | Y = 0 |
|:--------------|:-----:|------:|
| $\hat{Y}$ = 1 |   A   |     B |
| $\hat{Y}$ = 0 |   C   |     D |

$\frac{A}{A + C}$ our recall! Of all the records that are "1", how many are our model capturing

$\frac{B}{B + D}$ our precision! Of all the records that are "1", how many are our model capturing

The amazing thing is that we can actually make one of these one!

```{r}
alpha <- 0.5
test_pred <- test_probs > alpha
truly1 <- y[!train] ==1
called1 <- as.numeric(test_pred)

recall <- sum(called1 & truly1)/ sum(truly1)
precision <- sum(truly1 & called1)/ sum(called1)
recall
precision
```

Right now the recall is really bad: but lets make everything one and make our recall really good!

```{r}
alpha <- 0
test_pred <- test_probs > alpha
truly1 <- y[!train] ==1
called1 <- as.numeric(test_pred)

recall <- sum(called1 & truly1)/ sum(truly1)
precision <- sum(truly1 & called1)/ sum(called1)
recall
precision

```

Now lets make a sequence!

```{r}
alphas <- seq(0,1, length.out = 10000)
recalls <- rep(NA, length(alphas))
precisions <- rep(NA, length(alphas))

for(i in seq_along(alphas)){
  alpha <- alphas[i]
  test_pred <- test_probs > alpha
  truly1 <- y[!train] ==1
  called1 <- as.numeric(test_pred)
  
  recall <- sum(called1 & truly1)/ sum(truly1)
  precision <- sum(truly1 & called1)/ sum(called1)
  recall[i] <- recall
  precision[i] <- precision
}

#plot(recalls, precisions)

```

What are the most popular "artist.terms" in the dataset?

```{r}
artists_sorted <- sort(table(music$artist.terms), decreasing =T)
artists_sorted[1]

```

-   Choose one "artist.terms" to predict for, then try running logistic regression vs usual lm() on all of the other variables, do they pick up different variables? What would your next steps be?

```{r}
logit <- function(x){exp(x) / (1 + exp(x))}
logistic_obj <- function(params, X, y){
  xbeta <- cbind(1, X) %*% params
  p <- logit(xbeta)
  return(-sum(dbinom(y, size=1, prob=p, log=TRUE)))
}
optim(c(1, 1), logistic_obj, X=x, y=y)
```

Imagine you're doing fake news prediction, what metric(s) would you care more about the most? How would you recommend a target for these metrics for a company like Facebook?

```{r}
artist_cols <-
  grepl("^artist", names(music))

my_mod <-
  glm(y ~ ., data = music[,!artist_cols])

ols <-
  lm(y ~ ., music[,!artist_cols])

```

We can plot this data and see the relationship!

```{r}
plot(summary(ols)$coefficients[, 4], summary(my_mod)$coefficients[, 4])
```

If we plot the coefficients, of logistic versus regression, against each other, you will get a very very strong relationship. Things that are significant in one will be significant in the other.

If you only care about what features are important, there is very little difference between logistic and regression. Ultimately, when we are looking at the optimization, there is an xbeta term we created. It is just like in regression. Then you penalize by some magical calculation but eitherway it is all dependent on the xbeta term. In a sense, they are the same category of models.

<!--chapter:end:04-classification.Rmd-->

---
title: "Resampling Notes"
author: "Noah Love"
date: "1/28/2021"
output: html_document
---

## Resampling

Often with inbalanced data sets you want to upsample (or downsample). You can create or get rid of data to make the sample more proportionate.

Alternatively you can Bootstrap the data. It is like asking what is the distribution of something we don't know. We can use simulation to get a sense of what the distribution is (often used for uncertainty).

However, we always need to avoid overfitting. It is important to also do cross-validation then.

### Up-sampling

![Up-sampling](A:%5CDocuments%5CSemester%206%5Cdata-mining-at-columbia%5CMy%20Class%20Notes%5CImages%5Cupsampling.JPG)

It works, but it is important to remember you aren't creating new data. You are essentially adding more weight to the already existing data. But it does help.

### Bootstrapping

If the data changes, how much will my model change? You have these questions about slightly different sets. So you create a similar bootstrap sample and do the same model on both. Then you look at the results. By collapsing across the samples (and doing more), you can see how sensitive the data is and how it changes with different inputs.

![Bootstrapping](A:%5CDocuments%5CSemester%206%5Cdata-mining-at-columbia%5CMy%20Class%20Notes%5CImages%5Cbootstrap.JPG)

### Cross Validation

When you generalize beyond the data set you have, how bad is your error? This is the problem of overfitting. You can't generalize. Cross validation tries to tell you how bad of a problem you have. If you use tuning hyperparameters however, you can help fix this. You can leave part of the original set out to test on later. You switch the training set and validation set for all of the test.

In general, when tuning for hyperparameters (like degrees of polynomials) you ahve to make a choice. For example earlier with polynomials we graphed it using validation and training. Once we pick the optimal hyperparameter from the training set tested on the validation set. Then we apply it onto the test set to get the generalization error.

In short: - Cross validation between train/validation creates the hyperparameter. (i.e. degrees of polynomial) - Then you use all the training and validation data to get the parameters. (coefficients given the polynomial) - Then predict on the test data to get the generalization error.

This should prevent the overfitting from the hyperparameter. If you reapply onto a test set that wasn't used to help you pick this should tell you if there is a problem.

There is some debate on double cross validation. You can then change the test set and revalidate. Some people argue why not? Our model should be more robust if we cycle through everything.

Reasons not to: Computational. It is resource intesnive. You can also leak data and ruin you generalization error.

## Initial Work

```{r}
df <- read.csv("Datasets/small_non_retweets_dc_inaug_steal.csv")
head(df,2)
```

Let's understand the flags, like steal_flag.

```{r}
df$steal_flag <- as.numeric(df$steal_flag == "True")
mean(df$steal_flag)
```

We are turning true or false into 0 or 1. This flag is rare. It is about 1.7 percent.

Steal means if the word "steal" was present in a tweet. We can examine some of them for example by using:

```{r}
head(df[df$steal_flag > 0, "text"], 4)
```

## A basic model with upsampling

We can create a new data object that stores only the indices of the places where steal_flag exists.

```{r}
pos_ind <- which(df$steal_flag > 0)
head(pos_ind)
```

So in this case, tweets 28, 91, 114... etc have the word steal.

Let's sample the indices now. This requires us to figure out what we want to upsample our data to. To do so we will make N 10 times the length of our current amount of steal_flags. What we are doing is artificially increasing our data set. Upsampling is somewhat of a hack in the sense. It makes the objective function give more weight to the positive yet rare cases in our data. It is sorta dirty but also helpful, especially for preliminary exploration. There are other ways to be better at weighing certain aspects.

Note: you can also downsample the data. However, this is much more rare. It should have a very similar affect but usually you don't want to throw data away!

```{r}
N <- length(pos_ind)*10
sample_ind <- sample(pos_ind, N, replace = TRUE)
```

```{r}
table(sample_ind)
```

This table shows the number of times we get each entry. The above number is the specific record and below it is the number of times.

So let's create a new dataframe. We want to grab the particular indices from the above dataframe multiple times. Something like this seems intuitive.

```{r}
new_df <- df[sample_ind,]
```

However there is a major problem!

```{r}
dim(new_df)
dim(df)
```

We lost a lot of cases! In fact, we threw away all of the negative cases. So instead, we need to stack the dataframes together.

```{r}
new_df <- rbind(df[sample_ind, ], df)
dim(new_df)
```

Note the features themselves haven't changed at all. Instead, the proportion of the features have changed. There we go. Also note we did not overwrite df, we created a new dataframe. This is good practice in functional programming. Now we can do things like linear regression!

```{r}
ols <- lm(steal_flag ~ trump + capitol.police, data = new_df)
summary(ols)
```

Regressing with trump and capitol police, it appears capitol police isn't significant but trump is. That is his slogan so it shouldn't be too surprising but there it is.

How to mix upsample and also cross validate can only be done one way. You *don't* want to upsample first! If you did that, you could end up with the same record in both the train and test. Then you would be too confident in those cases. 

<!--chapter:end:05-Resampling.Rmd-->

# Linear Model Selection

## Regression requires more data than features

```{r}
library(glmnet)
```

import the dataset

```{r}
df <- read.csv("Datasets/non_retweets_dc_inaug_steal.csv")
```

Down-sampling for faster processing

```{r}
samp_ind <- sample(nrow(df), 8000)
df <- df[samp_ind,]
```

Combining the different flags

```{r}
steal_index <- grep("steai", names(df))
names(df)[steal_index]
df[, "X.stopthesteai2020"] <- (
  df[, "X.stopthesteai2020"] +
    df[, "X.stopthesteai"] +
    df[, "X.stopthesteai2021"])

df <- df[, -grep("(X.stopthesteai$|X.stopthesteai2021$)", names(df))]
```

Removing features that are not numeric

```{r}
text <- df[, grep("tweet_body", names(df))]
df <- df[,-grep("(tweet_body|created_at)", names(df))]
names(df)[grep("_", names(df))]
head(df$created_at)
df <- df[, sapply(df, class) == "numeric"]
```

Fitting the model

```{r}
y_ind <- grep("X.stopthesteai2020", names(df))
x <- as.matrix(df[, -y_ind])
x <- scale(x)
y <- as.numeric(df[, y_ind])
#bad_data <- which(is.na(x[, colnames(x) == "xinaugur"]))
bad_data <- which(is.na(x[, colnames(x) == "zone"]))
x <- x[-bad_data,]
y <- y[-bad_data]

```

```{r}

table(sapply(df,class))

names(df)[sapply(df,class) == "character"]

sum(apply(x,2, function(x) mean(is.na(x) > 0)))
```

Examine the data

```{r}
head(x[,colnames(x)=="zone"])

tail(x[,colnames(x)=="zone"])

head(y)
```

Run the model

```{r}
ols <- lm(y ~ x)
ols_coeffs <- summary(ols)$coefficients

```

To do data mining with p-values, do not treat them like probabilities, but you can use them as a metric to guide you.

```{r}

length(ols$coefficients)
```

```{r}
p_ord <- order(ols_coeffs[, 4])
rownames(ols_coeffs)[head(p_ord, 10)]
```

## Lasso

```{r}

lasso_cv <- cv.glmnet(as.matrix(df[, -y_ind]),
                      as.numeric(df[, y_ind]),
                      alpha=1,
                      nfolds = 5,
                      intercept=TRUE)

plot(lasso_cv)
abline(v=log(lasso_cv$lambda.min),
       col="blue")
abline(v=log(lasso_cv$lambda.1se),
       col="green")
```

```{r}
lasso.fits <- lasso_cv$glmnet.fit
lasso_cv$lambda[which.min(lasso_cv$cvm)]
lasso_cv$lambda.min
names(lasso_cv)
lambda_ind <- which.min(lasso_cv$cvm)
plot(ols$coefficients[-1],
     lasso_cv$glmnet.fit$beta[, lambda_ind], 
     xlab="OLS Coeffs", ylab="LASSO Coeffs")
dim(lasso_cv$glmnet.fit$beta)
abline(h=0)
abline(a=0,b=1)
```

```{r}
lasso_coeffs <- lasso_cv$glmnet.fit$beta[, lambda_ind]
lasso_coeffs[abs(lasso_coeffs) > 0.5]

hist(lasso_cv$glmnet.fit$beta[, lambda_ind])


```

```{r}
lasso <- glmnet(as.matrix(df[, -y_ind]),
                df[, y_ind], lambda=lasso_cv$lambda.1se,
                intercept=TRUE)
```

## simulating collinearity + sparsity

```{r}
sample_size <- 1000
p <- 100
useful_p <- 50

# number of corr features
collin_p <- 50
useful_ind <- sample(p, useful_p)
corr_ind <- sample(p, collin_p)

# independent variables
z <- rnorm(sample_size)
corrs <- rep(0, p)
corrs[corr_ind] <- runif(collin_p, 0.3, 0.9)
x <- sapply(corrs,
            function(corr) corr * z + (1 - corr) * rnorm(sample_size))

noise <- rnorm(sample_size)

#first option: generate y according to z
#y <- 2 * z + noise

#second option create a beta that defaults to beta (useless) and for some unknown 
#locations of beta, assign random useful coefficients. 
beta <- rep(0, p)
beta[useful_ind] <- runif(useful_p, -1.2, 1.2)

#matrix multiplication to get Y: most beta is zero and then add noise on top
y <- x %*% beta + noise

ols <- lm(y ~ x)
ols_summ <- summary(ols)$coefficients

rownames(ols_summ)[ols_summ[,4] < 0.05]
length(rownames(ols_summ)[ols_summ[,4]<0.05])


cols <- rep('black', p)
cols[corr_ind] <- "red"
pchs <- rep(1, p)
pchs[useful_ind] <- 16
plot(ols$coefficients[-1], beta,
     pch=pchs, col=cols)
abline(a=0, b=1)
```

## Demo on glmnet functionalities

```{r}
library(glmnet)
lasso_cv <- cv.glmnet(cbind(x, y), y, alpha=1)
coef(lasso_cv)
lasso_mod <- glmnet(x, y, alpha=1, lambda=lasso_cv$lambda.1se)
coef(lasso_mod)
```

```{r}


sample_size <- 1000
p <- 100
useful_p <- 50
# number of corr features
collin_p <- 50
useful_ind <- sample(p, useful_p)
corr_ind <- sample(p, collin_p)

# independent variables
z <- rnorm(sample_size)
corrs <- rep(0, p)
corrs[corr_ind] <- runif(collin_p, 0.3, 0.9)
x <- sapply(corrs,
            function(corr) corr * z + (1 - corr) * rnorm(sample_size))

noise <- rnorm(sample_size)
# y <- 2 * z + noise
beta <- rep(0, p)
beta[useful_ind] <- runif(useful_p, -1.2, 1.2)
y <- x %*% beta + noise


sim_num <- 100

beta_mse <- matrix(NA, ncol=3, nrow=sim_num)
z_coeffs <- matrix(NA, ncol=3, nrow=sim_num)
for(i in seq_len(sim_num)){
  if(i %% 10 == 0){
    print(i)
  }
  noise <- rnorm(sample_size)
  # y <- 2 * z + noise
  y <- x %*% beta + noise

  ols <- lm(y ~ x)
  lasso_cv <- cv.glmnet(x, y, alpha=1)
  ridge_cv <- cv.glmnet(x, y, alpha=0)
  beta_mse[i, 1] <- mean((beta - ols$coefficients[-1])^2)
  beta_mse[i, 2] <- mean((beta - coef(lasso_cv)[-1])^2)
  beta_mse[i, 3] <- mean((beta - coef(ridge_cv)[-1])^2)
}

boxplot(beta_mse)
abline(h=2)

plot(lasso_cv)
```

```{r}
sim_num <- 100
beta_mse
z_coeffs <- matrix(NA, ncol=3, nrow=sim_num)
for(i in seq_len(sim_num)){
  if(i %% 10 == 0){
    print(i)
  }
  noise <- rnorm(sample_size)
  y <- 2 * z + noise
  #y <- x %*%
  
  ols <- lm(y ~ x + z)
  lasso_cv <- cv.glmnet(x, y, alpha=1)
  ridge_cv <- cv.glmnet(x, y, alpha=0)
  #z_coeffs[i, 1] <- tail(ols$coefficients, 1)
  #z_coeffs[i, 2] <- tail(coef(lasso_cv), 1)[1, 1]
  #z_coeffs[i, 3] <- tail(coef(ridge_cv), 1)[1, 1]
  
  #if we aren't using z, maybe we should look at prediction accuracy
  
  #you could also use y according to the beta value instead
  #beta_mse[i,1] <- mean((beta - ols$coefficients[-1])^2) #drops the intercept term
}


boxplot(z_coeffs)
abline(h=2)
```

## Principal Component Analysis

Sparcity:

Inbalanced data:

### Principle Component Analysis

```{r}

hidden_p <- 5
observ_p <- 30
prob <- NULL # runif(hidden_p)
h2o <- sample(hidden_p,   #hidden to observed
              observ_p,
              replace=TRUE,
              prob=prob)
h2o <- sort(h2o)
sample_size <- 1000
hidden_z <- sapply(
  seq_len(hidden_p),
  function(x) rnorm(sample_size))
corrs <- runif(observ_p, 0.3, 0.8)

#create five groups of colinear stuff
observ_x <- mapply(
  function(i, corr) {
  hidden_z[, i] * corr + rnorm(sample_size) * (1 - corr) *100
    },
  h2o, corrs)

#observ_x is what you often see, but there is still hidden stuff

image(cor(observ_x))

#This looks weird to what we expect. It doesn't look like five groups
#if we sort instead it works


h2o <- sample(hidden_p,   #hidden to observed
              observ_p,
              replace=TRUE,
              prob=prob)

sample_size <- 1000
hidden_z <- sapply(
  seq_len(hidden_p),
  function(x) rnorm(sample_size))

#This effects how things are correlated!
corrs <- runif(observ_p, 0.3, 0.8)

#create five groups of colinear stuff
observ_x <- mapply(
  function(i, corr) {
  hidden_z[, i] * corr + rnorm(sample_size) * (1 - corr)
    },
  h2o, corrs)

#observ_x is what you often see, but there is still hidden stuff

image(cor(observ_x))


```

```{r}

beta <- runif(hidden_p, -10, 10)
noise <- rnorm(sample_size, sd=10)
#hard to measure hidden forces!
#we can only measure x, but x is only correlated to hidden stuff
y <- hidden_z %*% beta + noise
df <- data.frame(y, observ_x)

#y depends on the hidden stuff not x


```

Maybe there is a hidden gene inside of you that makes you sick. We can't (yet) measure that hidden gene. But we can measure symptoms and things like your heart rate. This should be correlated.

## Typical machine learning approach

```{r}
#training data set, first 800 points (80 percent)
train <- 1:800

ols <- lm(y ~ ., df, subset=train)
length(ols$residual) #correct length

#predict on points we didn't use to train
ols_pred <- predict(ols, df[-train,])

#error: differrence between measured values against predict. Mean of this squared MSE!
mean((df$y[-train] - ols_pred)^2)

#run ols 

#PCA TIME
#only input X, (feature matrix) into the PCA function
pr_out <- prcomp(observ_x, scale=FALSE)

#scale is used because Xs might not be in the same unit, so mean = 0, sd = 1
class(pr_out)

#it has its own class
names(pr_out)

#squared value of sdev is eigenvalue
eigen_val <- pr_out$sdev^2 

#cumulative sum as a fraction of total eigenvalues
plot(cumsum(eigen_val) / sum(eigen_val))
abline(h=.9)
#here after the 5th point, the slope tapers off! This is directly related to the hidden_p value at the beginning. It should help show you how many important hidden features there are. 

#it is the percent of variabilitiy caputured by the first k components

#If you don't know what to choose, 90% variability is a good point 


plot(pr_out$sdev)
#Very similar, but not as interpretable as percent of variability. 

#These steps is how k is chosen.

#K is the dimension of W. Data came n x p. We need to shrink it to k. If you don't have
#a clear cut, use 90%

#we don't want to keep all the variability because not all features provide useful
#information. Some of them are so colinear, they just add noise. 

cutoff <- 5

#now we are looking at x

#only pull out first k columns

dim(pr_out$x)
dim(observ_x)
#these will be the same, but we choose a cutoff. 



W <- pr_out$x[, 1:cutoff]
df_w <- data.frame(y, W)


#PCA doesn't have to be lm. It could be ridge or lasso too
#should be like the ols from above
pca <- lm(y ~ ., df_w, subset=train)
#same prediction
pca_pred <- predict(pca, df_w[-train,])

#prediction error
mean((df_w$y[-train] - pca_pred)^2)

#that was the classic view of PCA 
```

## What we would do in data mining

```{r}
k <- 3
plot(pr_out$rotation[,k])
abline(h = 0)
which(abs(pr_out$rotation[, k]) > 0.2)

#what is the truth we should be comparing to?

pr_out <- prcomp(observ_x)


test_x <- scale(observ_x, scale = FALSE) %*%
pr_out$rotation
pr_out$x

#Now looking at rotation

head(observ_x, 1 ) %*% pr_out$rotation[,1]





#k is which column are we going to examine

j <- 2
plot(pr_out$rotation[, j])
abline(h = 0)
which(abs(pr_out$rotation[, j]) > 0.2)
# what is the truth we should be comparing to?


#What is rotation. Matrix (30 x 30 in this case)
dim(pr_out$rotation)
#it is actually p x p rather than p x k to give you more columns

head(pr_out$rotation[,k])
#kth eigenvector which correspods with the kth largest value (most significant) value


#this will always be 1. Property of rotation matrix
sum(pr_out$rotation[,k]^2)


```

## Principal Component Analysis Applied!

### Other example

In really poor countries it is super hard to measure wealth and income. There are no reciepts and corresponding taxes. People don't have bank accounts. Instead, you measure features. Like do you have a fridge. Do you have cooking equipment? How many kids? How many room in your house?

So you could run PCA on assets matrix. You can find correlations. If you have more rooms in your house, you likely have more education. The correlations will be baked into the principal driving component. Further, they use this as the Y to see if they can predict! But that is beyond the scope of this class.

```{r}
library(jsonlite)

#citation count between his 15 papers and those he sighted
citations <- read.csv("Datasets/j_cunningham_citation.csv", head = FALSE)

titles <- read_json("Datasets/j_cunningham_citation_titles.json")
```

### Explore the data

```{r}
dim(citations)

head(citations)
citations[1:5,1:5]
max(citations)

#across all paers
apply(citations, 1, max)

```

```{r}
names(titles)




```

Papers that he has written:

```{r}
head(titles[["auth_titles"]],3)
```

Papers he has cited

```{r}
head(titles[["ref_titles"]],3)
```

Among the 15, there are four papers that reference the 2 most popular articles. Let us find them:

```{r}
ref_count <- apply(citations, 2, function(x) 
sum(x > 0))

targets <- tail(names(sort(ref_count)),2)

#These are the two columns we want
target_ind <- which(names(citations) %in% targets)

target_ind

titles[["ref_titles"]][target_ind]
```

Explore this data: we know the index of the two. This can show the correlation between the two, meaning the papers are cited by certain papers. This would make sense. If you cite one of these, you almost certainly have to cite the other:

```{r}
citations[,target_ind]
```

A few things to remember. Longer papers should have more citations. There is also likely to be correlation between certain papers.

We would intuitively just want to apply our "prcomp", like we learned in last class.

```{r}

pr_out <- prcomp(citations)
plot(pr_out$sdev, main="")
```

This plot is not very appealing. There is not a significant drop until the last term. Maybe between 1 and 2 and 3 and 4, but not a big drop. And if you only abandon 1 dimension, (14 instead of 15), you aren't really saving a lot.

#### Try standardizing the citation matrix in different ways

##### Usual standardization, i.e. make each feature mean=0 and sd = 1

```{r}

norm_citation <- apply(citations, 2 , scale)
#also

pr_out <- prcomp(norm_citation)

plot(pr_out$sdev, main="")

png("Datasets/loadings_normal_standardization.png", 900, 700)
par(mfrow=c(4, 3))
for(i in seq_len(ncol(pr_out$rotation[,1:12]))){
    eigenvec <- pr_out$rotation[, i]
    plot(eigenvec, main = paste("Eigenvec",i))
    abline(h=0)
}
dev.off()

```

![Plots using normal standardization](Datasets/loadings_normal_standardization.png)

Remember from the plot, if we square all the values and add them together they will equal one. So strong deviations will be papers that are relied on (by index). Too many papers that share the same weight, might not be helpful. We believe there are only a few really good papers that we want. Not like 50. So this behavior is still undesirable.

This is bad! You subtract something away from 0 values. But we like 0s because they don't affect the objective function. PCA is using the "frebenious norm" where everything squared and added together is 1. So we like 0s. So how can we scale differently, but while keeping the 0s.

##### Max normalized, i.e. make each feature min=0, max = 1

```{r}
#lets divide by the max, then everything is between 0 and one
norm_citation <- apply(citations, 2,
                       function(x) x / max(x))

pr_out <- prcomp(norm_citation, center=FALSE,
                 scale=FALSE)

png("Datasets/max_normalized.png", 900, 700)
par(mfrow=c(4, 3))
for(i in seq_len(ncol(pr_out$rotation[,1:12]))){
    eigenvec <- pr_out$rotation[, i]
    plot(eigenvec, main = paste("Eigenvec",i))
    abline(h=0)
}

dev.off()
```

This should look much nicer. For example, Eigvec 4 looks better. Why did we normalize the columns? We often do this because the columns have different units. However, in this, the columns all have the same units. Instead, papers have different lengths, so the citation number can be affected by the length of the paper. So whay do we want to actually normalize? The rows!

##### Max normalized per paper, i.e. make each ROW min=0, max = 1

```{r}
citations_norm_papers <-  apply(citations, 1,
                       function(x) x / max(x))


# Just doing the above actually swapped things! We have 15 columns instead
#It processes a row, and then stacks it as a column. So we need to transpose
citations_norm_papers <- t(citations_norm_papers)


pr_out <- prcomp(citations_norm_papers)

plot(pr_out$sdev, main="")
png("Datasets/loadings_norm_per_paper.png", 900, 700)
par(mfrow=c(4, 3))
for(i in seq_len(ncol(pr_out$rotation[,1:12]))){
    eigenvec <- pr_out$rotation[, i]
    plot(eigenvec)
    abline(h=0)
}
dev.off()

```

Ther is a much more noticable drop between 11 and 12

```{r}
pr_out <- prcomp(citations)
plot(pr_out$sdev, main="")
png("Datasets/loadings.png", 900, 700)
par(mfrow=c(4, 3))
for(i in seq_len(ncol(pr_out$rotation[,1:12]))){
    eigenvec <- pr_out$rotation[, i]
    plot(eigenvec)
    abline(h=0)
}
dev.off()

target_ind <- which(abs(pr_out$rotation[, 2]) > 0.15)
titles[["ref_titles"]][target_ind]
```

1st column second row is

2nd column 3rd row is a disaster.

## PCA on weather data example

-   Wrap up the citation problem
-   Play with weather data, tmax, celsius

```{r}
df <- read.csv("Datasets/ushcn.csv")
station <- read.csv("Datasets/station_metadata.csv")
rownames(station) <- station$id
meta_sort <- station[names(df)[-1], c("longitude", "latitude")]

prop_na <- apply(df[, -1], 1,
                 function(x) mean(is.na(x)))

sdf <- df[prop_na < 0.1, ]
sdf_sans_na <- apply(sdf[, -1], 2, function(x){
  x[is.na(x)] <- mean(x, na.rm=TRUE)
  return(x)
})

library(RColorBrewer)
cols <- brewer.pal(7, "RdYlBu")

pr_out <- prcomp(sdf_sans_na)
png("Pictures/no_norm_pca.png", 600, 800)
par(mfrow=c(2, 1))
for(i in 1:2){
  eigvec <- pr_out$rotation[, i]
  breaks <- seq(min(eigvec), max(eigvec), length.out=length(cols)+1)
  col_factors <- cut(eigvec, breaks=breaks)
  plot(meta_sort$longitude,
       meta_sort$latitude,
       col=cols[col_factors],
       pch=16, cex=0.5)
  legend("bottomright",
         legend = levels(col_factors),
         fill=cols)
}
dev.off()

```

![Eigenvector 1 and two mapped](Pictures/no_norm_pca.png)

The two are telling in very different ways. The first shows the relationships of the coasts. The second shows east versus west. This is with default normalization: centering, but not scaling.

## Different noramlizations

Run PCA with 3 different types of normalization on the maximum temperature data, then plot the "maps" of the loading values corresponding to the first 2 eigenvectors.

-   No normalization, i.e. no centering and no scaling
-   Centering but no scaling
-   Centering and scaling

Write out what do you observe.

### No normalization

```{r}
prop_na <- apply(df[, -1], 1,
                 function(x) mean(is.na(x)))

sdf <- df[prop_na < 0.1, ]
sdf_sans_na <- apply(sdf[, -1], 2, function(x){
  x[is.na(x)] <- mean(x, na.rm=TRUE)
  return(x)
})

cols <- brewer.pal(7, "RdYlBu")

pr_out <- prcomp(sdf_sans_na, scale = FALSE, center = FALSE)
png("Pictures/no_standardization_pca.png", 600, 800)
par(mfrow=c(2, 1))
for(i in 1:2){
  eigvec <- pr_out$rotation[, i]
  breaks <- seq(min(eigvec), max(eigvec), length.out=length(cols)+1)
  col_factors <- cut(eigvec, breaks=breaks)
  plot(meta_sort$longitude,
       meta_sort$latitude,
       col=cols[col_factors],
       pch=16, cex=0.5)
  legend("bottomright",
         legend = levels(col_factors),
         fill=cols)
}
dev.off()

```

### Centering and scaling

```{r}
prop_na <- apply(df[, -1], 1,
                 function(x) mean(is.na(x)))

sdf <- df[prop_na < 0.1, ]
sdf_sans_na <- apply(sdf[, -1], 2, function(x){
  x[is.na(x)] <- mean(x, na.rm=TRUE)
  return(x)
})

library(RColorBrewer)

pr_out <- prcomp(sdf_sans_na, center = TRUE, scale = TRUE)
png("Pictures/centering_and_scaling_pca.png", 600, 800)
par(mfrow=c(2, 1))
for(i in 1:2){
  eigvec <- pr_out$rotation[, i]
  breaks <- seq(min(eigvec), max(eigvec), length.out=length(cols)+1)
  col_factors <- cut(eigvec, breaks=breaks)
  plot(meta_sort$longitude,
       meta_sort$latitude,
       col=cols[col_factors],
       pch=16, cex=0.5)
  legend("bottomright",
         legend = levels(col_factors),
         fill=cols)
}
dev.off()

```

Scaling is super common. When you have different units you always usually scale. However, sometimes, with things like weather data, you might not have to. PCA is trying to find a very consice, uncorrelated description of your data. If things are close to 0, they won't effect.

Subset out the last 144 rows for testing, then pick one station to be our Y and the other stations to be our X.

-   Run PCA on the X values, then fit an OLS
-   Run OLS but only use the closest station (don't bother with projecting, assuming Long/Lat are equidistance for now) as your X.

Which one will do best?

(if you have time, try Lasso with all of the data, this may take awhile....don't do this unless you have time)

```{r}
#Takes last 144 for testing
train <- 1:(nrow(sdf_sans_na) - 144)

# Samples and individual X to be the Y point
y_ind <- sample(ncol(sdf_sans_na), 1)


y_train <- sdf_sans_na[train, y_ind]

x_train <- sdf_sans_na[train, -y_ind]

pr_out <- prcomp(x_train, center = TRUE, scale = FALSE)

ols <- lm(y_train ~ x_train)

library(tidyverse)

station_new <- station %>% 
  mutate(long_difference = longitude - station$longitude[y_ind]) %>% 
  mutate(lat_difference = latitude - station$latitude[y_ind]) %>% 
  select(latitude, longitude, long_difference, lat_difference) %>% 
  mutate(sum_lat_long = lat_difference + long_difference) #%>% 
  #order(decreasing = TRUE, sum_lat_long)
  

station_new

#closest <- which(min(station$longitude + station$latitude))


```

<!--chapter:end:06-linear-model-selection.Rmd-->

# Comparing different models

This homework is to meant for you practice some feature engineering with the
different regression models.

Context: the US is increasingly polarized and [Twitter](https://twitter.com/home), a social
media platform, is providing the space for people to voice their opinions or find like-minded
individuals. In this assignment, we will use Twitter data `non_retweets_dc_inaug_steal.csv` on CourseWorks that has contains the token frequencies. This has been processed somewhat to minimize the data size. 
The non-token variables are `created_at`, `like_count`, `reply_count`, `retweet_count`, `tweet_body`.

Please **re-download** this dataset given there were issues with the version given in class.

The tweets are collected overtime with the query words "steal", "inaguration", and "washington dc" using
Twitter's [Recent Search API](https://developer.twitter.com/en/docs/twitter-api/tweets/search/api-reference/get-tweets-search-recent).
Calls were made once a day around midnight and there is a limit on the number of queries that
can be made freely.

## Question 0
What dates do the tweets cover? Please visualize the temporal distribution of the tweets
with the intent to understand where most Tweets in the dataset came from in time.

## Question 1
Please combine all the token frequencies that contain the string `inaug` into a single column.
You should remove the redundant tokens after this summarization. Please select the method
of "combining" that is consistent with the "token frequency" concept.

Please report the fraction of records that have non-zero frequencies for this new column.

## Question 2
We want to discover the topics correlated with the inauguration over time. To achieve this,
please model your new column from Question 1 against all other tokens (i.e. exclude the non-token variables)
using the following models. This is **time-consuming**! You should use `Sys.time()` to have a realistic expectation of how long each model will take.

Please plot the MSE from a 5-fold cross validation to compare the prediction accuracy between
- OLS
  - Please ignore the rank deficient warning, this is a warning of high-collinearity
    that is common with high dimensional data and why OLS shouldn't do well.
- stepwise regression using AIC as the objective




```{r}
# ols = lm(y ~ ., df)
# ols_summ = summary(ols)$coefficients
# okay_features = rownames(ols_summ)[ols_summ[, 4] < 0.05]
# init_formula = paste('y ~', paste(okay_features, collapse='+'))
# init_mod = lm(init_fomrula, df)
# # "~." sets the upper model for the `scope` as using all of the features
# step_mod <- step(init_mod, "~.")
```



- Lasso corresponding to $$\lambda$$=`lambda.min` from cv.glmnet
  - hint: [predict.cv.glmnet](https://www.rdocumentation.org/packages/glmnet/versions/4.1/topics/predict.cv.glmnet)
  - hint: you may want to convert the training X matrix into sparse matrices, i.e. `Matrix(as.matrix(df_sans_y), sparse=TRUE)`
    to speed things up. This essentially avoids many multiplications when a 0 is involved.
- Ridge regression corresponding to $$\lambda$$=`lambda.min` from cv.glmnet

Please **do not** normalize your features for this problem but use the raw frequencies.

hint, adding some print statements can help with unnecessary panics:
```{r}
# library(caret)
# test_folds <- createFolds(DEP_VARIABLE, k=5)
# for(i in seq_along(test_folds)){
# 
#     # Some code that isolates the test/train data!
#     
#     print(paste("cross validation fold", i))
#     t0 <- Sys.time()
#     ols <- lm(TRAIN_DEP_VARIABLE ~ TRAIN_INDEP_VARIABLE)
#     print(paste('running OLS took', Sys.time() - t0))
#     }
```

Side comment:
- You may want to see how the `lambda.1se` compares.
- (personal observation) Normalizing the features does help with the optimization but our features are all token frequencies so it's not as big of a concern.

## Question 3
For the algorithm that performed the best, please retrain the model with the following requirements:
- Use all of the data, i.e. do not reserve data for test/train.
- You may need to normalize the features. If Lasso/Ridge was best, note that sparsity will not hold after
  you normalize.

Please plot the distribution of the coefficients.

## Question 4
Please list out the top tokens corresponding to the strongest non-zero coefficients.

Side comment: If you are not an American, what do these tokens and their coefficients
suggest about the inauguration?


## Question 5

Please write a function that represents the objective function for Ridge regression, i.e. your function should take in a vector of coefficients, a matrix X, and a vector Y and return a real number.
- Please make sure you fit an intercept within the function but do not include the intercept in your regularization term for the objective.
- Please add an additional argument called `shrink_target` that allows us to change the shrinkage of the coefficients to arbitrary vector.
  - Please make sure you set the default for this argument to align with the usual Ridge regression.

## Question 6
Please perform PCA on the token frequencies without normalizing.
- Plot the eigenvalues in order (not the cumulative eigenvalues) of their magnitude, how many components might have interesting features. There are many correct answers but also many wrong answers here depending on your plot.
- Analyze the loadings from the first 2 components, do they seem meaningful to you given the dataset?
  - if yes, what key tokens are associated with these 2 components and why do they carry meaning?
  - if no, how could you modify the token frequencies to get more meaningful principle components?

Side comment (things to think about):
- what would you do after identifying the components?
- again, if you were not involved in American politics, how would you have done some of these steps?
- Twitter data is not like normal language, e.g. people do not use complete sentences. How might this look if we had journal articles?

<!--chapter:end:07-comparing_different_models.Rmd-->

# Homework 3 Project

```{r}
library(caret)
library(tidyverse)
```


```{r}
tweets_raw <- read_csv("Datasets/non_retweets_dc_inaug_steal.csv")
dim(tweets_raw)
```



<!--chapter:end:08-homework-3.Rmd-->

---
title: "HW1"
author: "Noah Love"
date: "1/18/2021"
output:
  pdf_document: default
  html_document: default
---

## HW1

This homework is to meant for you to brush up some prerequisites. If some of these topics are new to you, feel free to ask on Piazza how to approach these.
Context - Is gold price inversely related to the market?

There’s a belief that the money from the stock market will escape to gold when the stock market is not doing well. The demand for gold and the expectations for the market are often reflected in the pricing of the assets, i.e. high demand yields high gold prices and upward expectations also lead to higher stock prices.

### Q1

Please use the ‘TIME_SERIES_WEEKLY’ API listed on Alpha Vantage to get the weekly time series data for

    ‘VOO’: an arbitrarily chosen ETF that tracks the market
    ‘GDXJ’: an arbitrarily chosen ETF for gold

For this problem, simply show the code for your query and print out the number of weeks of data for each time series. Your API key should NOT appear in your solutions but the URL you’re using and the query should be shown.

Hint:

    You will need to claim a free API key before you can query data
    The functions in httr should be helpful, here is some sample code if you have not done so before.



```{r, echo=FALSE}
#For VOO
api_key <- "SEDREZCZEW7NAOQK"
```


```{r}
#library(httr)
# url_VOO <- "https://www.alphavantage.co/query?function=TIME_SERIES_WEEKLY&symbol=VOO&apikey=SEDREZCZEW7NAOQK"
# url_GDXJ <- "https://www.alphavantage.co/query?function=TIME_SERIES_WEEKLY&symbol=GDXJ&apikey=SEDREZCZEW7NAOQK"
# 
# responce <- GET(url = url_VOO, query = params)

function_name <- "TIME_SERIES_WEEKLY"
stock_ticker <- "VOO"
my_data_type <-"csv"
output_size <- "full"
api_call <- paste0("https://www.alphavantage.co/query?function=",
                   function_name,
                   "&symbol=",
                   stock_ticker,
                   "&outputsize=",
                   output_size,
                   "&apikey=",
                   api_key,
                   "&datatype=",
                   my_data_type)

VOO <- read.csv(url(api_call))

head(VOO)

```

```{r}
#For VOO

function_name <- "TIME_SERIES_WEEKLY"
stock_ticker <- "GDXJ"
my_data_type <-"csv"
output_size <- "full"
api_call <- paste0("https://www.alphavantage.co/query?function=",
                   function_name,
                   "&symbol=",
                   stock_ticker,
                   "&outputsize=",
                   output_size,
                   "&apikey=",
                   api_key,
                   "&datatype=",
                   my_data_type)

GDXJ <- read.csv(url(api_call))

head(GDXJ)
```

                   
                   
### Q2
Please plot the close price for VOO against the different weeks and overlay the regression line for this scatter plot.

You do not need to label your week index but the prices should be labeled.

```{r}
library(ggplot2)
library(dplyr)



VOO$timestamp <- as.Date(VOO$timestamp)

ggplot(VOO, aes(x = timestamp,y = close)) + 
  geom_line() +
  xlab("day") + 
  ylab("price") +
  geom_smooth(method = "lm", formula = y~x)
```




### Q3
Please plot the residuals from the regression in Q2 against the close price of GDXJ.

label your axes with units.
Your title should include the correlation value, rounded to the nearest hundredth.
Please show the code that demonstrates your decision on merging the 2 time series.

```{r}
linear <- lm(VOO$close ~ VOO$timestamp)


subset_GDXJ <- GDXJ[1:length(linear$residuals),]
#subset_GDXJ


gold_vs_stock <- lm(subset_GDXJ$close ~ linear$residuals)
summary(gold_vs_stock)


plot(x = linear$residuals, y = subset_GDXJ$close,
  xlab="Difference in stock market performance to linear",
  ylab = "Closing price of GDXJ",
  main = "Correlation coeff: 0.049",
  abline(lm(subset_GDXJ$close ~ linear$residuals)))

```



###  Q4
Relying only on the scatter plot, would you say the belief between gold and the market is supported or rejected? Please explain.                   
 
Relying on the scatter plot and the line I plotted, gold is not inversely related. In fact, when the market it beating its performace, on average gold is closing higher as well. Also, when the stock market is doing really poorly (compared to the linear regression) gold also closes really low as seen by the strong tail around -40.                  
                   

<!--chapter:end:09-HW1.Rmd-->

---
title: "HW 2 Solutions"
author: "Noah Love (adapted from Jaesung Son)"
date: "2/13/2021"
output: html_document
---

## R Markdown

```{r}
library('jsonlite')
library('RJSONIO')
```

```{r}
votes <- fromJSON('votes.json')
senator_id <-
  sort(names(votes))
session_id <- sort(unique(names(unlist(unname(
  votes
)))))
voting_matrix <-
  matrix(NA, nrow = length(session_id), ncol = length(senator_id))
rownames(voting_matrix) <-  session_id
colnames(voting_matrix) <-  senator_id

# Making the matrix

for(i in senator_id) {
  for (j in session_id) {
    if (j %in% names(votes[[i]])) {
      voting_matrix[j, i] <- votes[[i]][[j]]
    }
  }
}

# Dimensions
print(paste(
  "Number of rows = ",
  as.character(nrow(voting_matrix)),
  ", number of columns = ",
  as.character(ncol(voting_matrix))
))
```


```{r}
# Percentage of matrix that does not contain -1, 1 or 0

print(paste(as.character((
  sum(is.na(voting_matrix)) + sum(voting_matrix == -9999, na.rm = TRUE)
) / (
  nrow(voting_matrix) * ncol(voting_matrix)
) * 100), "%"))
```


```{r}
cor_matrix <- cor(voting_matrix, use = "pairwise.complete.obs")
```


```{r}
lower_cor <- cor_matrix[lower.tri(cor_matrix)]
```

```{r}
hist(lower_cor, main = "Histogram of Correlation Values", xlab = "Correlation")
```



```{r}
left <- lower_cor[lower_cor < 0.25]
right <- lower_cor[lower_cor >= 0.25]
quantile(left, na.rm = TRUE)
```


```{r}
quantile(right, na.rm = TRUE)
```

```{r}
cor_with_mitch <- cor_matrix["S174", ]
not_NA <- names(cor_with_mitch[!is.na(cor_with_mitch)])
mitch_cor_mat <- cor_matrix[ , not_NA]
mitch_order <- rev(names(sort(mitch_cor_mat["S174", ])))
mitch_cor_mat <- mitch_cor_mat[ mitch_order , mitch_order]# Visualize
image(mitch_cor_mat)
```

```{r}
voters <- fromJSON("voters.json")
cor_with_mitch <- mitch_cor_mat[1, ]

# Defining Those with Negative Correlation with Mitch
neg_cor_with_mitch <- names(cor_with_mitch[cor_with_mitch < 0])

# Defining Possible Republicans
poss_republican <-
  names(cor_with_mitch[cor_with_mitch >= 0.2 &
                         names(cor_with_mitch) != "S174"])

# Finding Those with avg greater than 0.2 correlation with poss_repubicans
subset_mat <- mitch_cor_mat[neg_cor_with_mitch, poss_republican]
averages <- rowMeans(subset_mat, na.rm = TRUE)
senators_criteria <- names(averages[averages > 0.2])

senator_info <- voters[senators_criteria]

print(senator_info)
```

```{r}
lisa_id <- names(which(unlist(voters) == "Murkowski"))
lisa_id <- "S157"


cor_with_lisa <- cor_matrix[lisa_id, ]
cor_max <- names(which.max(cor_with_lisa[names(cor_with_lisa) != lisa_id]))
cor_min <- names(which.min(cor_with_lisa))


ols <- lm(voting_matrix[ , lisa_id] ~ voting_matrix[ , cor_max] + voting_matrix[ , cor_min])

summary(ols)
```


```{r}
ols <- lm(voting_matrix[ , lisa_id] ~ voting_matrix[ , cor_max])

res <- voting_matrix[ , lisa_id] - cbind(1, voting_matrix[ , cor_max]) %*% ols$coefficients

cor_with_res <-function(x){return(cor(voting_matrix[ , x], res, use = "pairwise.complete.obs"))}

senator_id_nolisa <- senator_id[senator_id != lisa_id]

all_cor_with_res <- sapply(senator_id_nolisa, cor_with_res)


max_cor_with_res <- names(which.max(all_cor_with_res))

ols2 <- lm(voting_matrix[ , lisa_id] ~ voting_matrix[ , cor_max] + voting_matrix[ , max_cor_with_res])

summary(ols2)
```


<!--chapter:end:10-HW2_solutions.Rmd-->

