
# Principal Component Analysis

Sparcity: 

Inbalanced data:

## Principle Component Analysis

```{r}

hidden_p <- 5
observ_p <- 30
prob <- NULL # runif(hidden_p)
h2o <- sample(hidden_p,   #hidden to observed
              observ_p,
              replace=TRUE,
              prob=prob)
h2o <- sort(h2o)
sample_size <- 1000
hidden_z <- sapply(
  seq_len(hidden_p),
  function(x) rnorm(sample_size))
corrs <- runif(observ_p, 0.3, 0.8)

#create five groups of colinear stuff
observ_x <- mapply(
  function(i, corr) {
  hidden_z[, i] * corr + rnorm(sample_size) * (1 - corr) *100
    },
  h2o, corrs)

#observ_x is what you often see, but there is still hidden stuff

image(cor(observ_x))

#This looks weird to what we expect. It doesn't look like five groups
#if we sort instead it works


h2o <- sample(hidden_p,   #hidden to observed
              observ_p,
              replace=TRUE,
              prob=prob)

sample_size <- 1000
hidden_z <- sapply(
  seq_len(hidden_p),
  function(x) rnorm(sample_size))

#This effects how things are correlated!
corrs <- runif(observ_p, 0.3, 0.8)

#create five groups of colinear stuff
observ_x <- mapply(
  function(i, corr) {
  hidden_z[, i] * corr + rnorm(sample_size) * (1 - corr)
    },
  h2o, corrs)

#observ_x is what you often see, but there is still hidden stuff

image(cor(observ_x))


```
```{r}

beta <- runif(hidden_p, -10, 10)
noise <- rnorm(sample_size, sd=10)
#hard to measure hidden forces!
#we can only measure x, but x is only correlated to hidden stuff
y <- hidden_z %*% beta + noise
df <- data.frame(y, observ_x)

#y depends on the hidden stuff not x


```
Maybe there is a hidden gene inside of you that makes you sick. We can't (yet) measure that hidden gene. But we can measure symptoms and things like your heart rate. This should be correlated. 


## Typical machine learning approach
```{r}
#training data set, first 800 points (80 percent)
train <- 1:800

ols <- lm(y ~ ., df, subset=train)
length(ols$residual) #correct length

#predict on points we didn't use to train
ols_pred <- predict(ols, df[-train,])

#error: differrence between measured values against predict. Mean of this squared MSE!
mean((df$y[-train] - ols_pred)^2)

#run ols 

#PCA TIME
#only input X, (feature matrix) into the PCA function
pr_out <- prcomp(observ_x, scale=FALSE)

#scale is used because Xs might not be in the same unit, so mean = 0, sd = 1
class(pr_out)

#it has its own class
names(pr_out)

#squared value of sdev is eigenvalue
eigen_val <- pr_out$sdev^2 

#cumulative sum as a fraction of total eigenvalues
plot(cumsum(eigen_val) / sum(eigen_val))
abline(h=.9)
#here after the 5th point, the slope tapers off! This is directly related to the hidden_p value at the beginning. It should help show you how many important hidden features there are. 

#it is the percent of variabilitiy caputured by the first k components

#If you don't know what to choose, 90% variability is a good point 


plot(pr_out$sdev)
#Very similar, but not as interpretable as percent of variability. 

#These steps is how k is chosen.

#K is the dimension of W. Data came n x p. We need to shrink it to k. If you don't have
#a clear cut, use 90%

#we don't want to keep all the variability because not all features provide useful
#information. Some of them are so colinear, they just add noise. 

cutoff <- 5

#now we are looking at x

#only pull out first k columns

dim(pr_out$x)
dim(observ_x)
#these will be the same, but we choose a cutoff. 



W <- pr_out$x[, 1:cutoff]
df_w <- data.frame(y, W)


#PCA doesn't have to be lm. It could be ridge or lasso too
#should be like the ols from above
pca <- lm(y ~ ., df_w, subset=train)
#same prediction
pca_pred <- predict(pca, df_w[-train,])

#prediction error
mean((df_w$y[-train] - pca_pred)^2)

#that was the classic view of PCA 
```


## What we would do in data mining
```{r}
k <- 3
plot(pr_out$rotation[,k])
abline(h = 0)
which(abs(pr_out$rotation[, k]) > 0.2)

#what is the truth we should be comparing to?

pr_out <- prcomp(observ_x)


test_x <- scale(observ_x, scale = FALSE) %*%
pr_out$rotation
pr_out$x

#Now looking at rotation

head(observ_x, 1 ) %*% pr_out$rotation[,1]





#k is which column are we going to examine

j <- 2
plot(pr_out$rotation[, j])
abline(h = 0)
which(abs(pr_out$rotation[, j]) > 0.2)
# what is the truth we should be comparing to?


#What is rotation. Matrix (30 x 30 in this case)
dim(pr_out$rotation)
#it is actually p x p rather than p x k to give you more columns

head(pr_out$rotation[,k])
#kth eigenvector which correspods with the kth largest value (most significant) value


#this will always be 1. Property of rotation matrix
sum(pr_out$rotation[,k]^2)







```