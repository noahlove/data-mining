# Linear Regresssion

```{r}
library(tidyverse)
```

We have a sample of 100, 5 different variables (p), and an X matrix. We are making uniform variables, and we will jam them into a matrix such that the number of rows is equal to n.

```{r}
n <- 100
p <- 5
X <- matrix(runif(p * n),
            nrow=n)
```

Then we can create some Y's based on betas and noise. To do so, we need betas. We will make all of them zeros, except for one of them! We also need to define our noise.

```{r}
beta <- rep(0, p)
beta[sample(p, 1)] <- 1
noise <- rnorm(n)

# %*% Matrix multiplication in R
#X is a matrix of 100 by 5
#Beta is a matrix of 5
Y <- X %*% beta + noise

```

The above is generating data according to a linear model! So what does this do? We have X and a bunch of 0 betas, except for one special one. The Y then will depend only on the one column!

If we don't know which beta matters, we just fit y against x and print out a summary.

```{r}
ols <- lm(Y ~ X)
summary(ols)
```

So only one of them is statistically significant! This shows it should be the 5th! We can see the truth:

```{r}
beta
```

This is a good example of fitting basic regression.

Lets look at the y and subtract the the coefficients to find the residuals: note: length(ols\$coefficients) is 6, but dimensions of X is going to be 5.So we will need to cbind. So this is y minus the fitted data from the regression. When we do summary of ols, we get a bunch of estimates. The intercept and the estimated coefficients. These are estimates from the regression based on the data. Using these, we can recover and estimate and then find the difference to see what the residuals are!

```{r}
resid <-  Y - cbind(1, X) %*% ols$coefficients
#this is the manual version of:
resid <-  ols$residuals
```

Now let's do the residuals against the true beta! We are subtracting the x values against the true beta. True beta means this is the beta that created the data. This is how god created the world. It is the actual physical value. The above residuals are from the regression of the fitted data.

```{r}
resid_from_truth <- Y - X %*% beta
```

We can plot this as well!

```{r}
plot(density(resid_from_truth))
lines(density(resid), col = "red")
abline(v = 0)
```

This might not be enough contrast for us to tell. How can we quantify the difference? Let's see the difference between the two:

```{r}
mean(abs(resid))
mean(abs(resid_from_truth))
```

We want the smaller value! The simulated data based values will always have a smaller residual mean than the real values! This is rather disturbing. The fitted coefficients from your regression will always be better than the "true" coefficients. Why is this bad?

Normally we want to use regression to find the natural coefficients or natural facts about the world. These are based on some "truth". If you can collect noisy data but our algo prefers the fitted data rather than the truth we have a problem. We want our error minimized at the "truth". Our regression here doesn't like the true answer and it prefers something else. It actually prefers the training data.

This is because we use the same data to train and evaluate. Our data is just optimized for this one thing. If we know how you are going to evaluate me, I will just optimize for that specific thing.

So let's generate another set of data: new Y using the same beta and x but with new noise values: Testing data generated from the same population but not used for training the model

```{r}
new_noise <- rnorm(n)

new_Y <- X %*% beta + new_noise

new_resid <- new_Y - cbind(1, X) %*% ols$coefficients
new_resid_from_truth <-  new_Y - X %*% beta

mean(abs(new_resid))
mean(abs(new_resid_from_truth))
```

Our takeaway is don't always cross over your data because you risk overfitting.

## Non linear data

Let's create new data that is nonlinear. This is mini sine data.

```{r}
n <- 100
X <- runif(100, max = 2 * pi / 4 * 3)
Y <- 0.1+-3 * sin(X) + rnorm(n, sd = 0.5)
```

Then we can recreate our linear regression and the plot.

```{r}
ols <- lm(Y ~ X)
summary(ols)

plot(X, Y)
abline(lm(Y ~ X))
```

As expected our linear model created a line that doesn't look to fit too well. Let's look at the residual plot instead. We

```{r}
par(mfrow=c(1,2))
plot(ols$residuals)
abline(h=0)
plot(X, ols$residuals) 
abline(h = 0)
```

Xs were generated in a random order so we we just use the index, it looks random and normal. But when we sort by x value, we see that there is definitely a problem and we need to add a quadratic term.

We can then find the mean squared error:

```{r}
n <- 100
Y <- 0.1+-3 * sin(X) + rnorm(n, sd = 0.5)

#Y - y hat squared
mean((Y - ols$fitted.values) ^ 2)
```

**Plot the MSE vs the number of polynomials used on the x axis. Do this for both the training vs the testing data.**

We can regress on matrices in R which makes this really easy. So let us create an X_matrix, where for each degree, we raise x to that number of degrees. This matrix will be n rows and degrees columns.

```{r}
n <- 100
degrees <- 1:50

X <- runif(n, max = 2 * pi / 4 * 3)
Y <- 0.1+-3 * sin(X) + rnorm(n, sd = 0.5)
new_Y <- 0.1+-3 * sin(X) + rnorm(n, sd = 0.5)
X_mat <- sapply(degrees, function(i)
  X ^ i)
```


For example

```{r}
plot(X, X_mat[,5])
```

We can do this through a loop.

```{r}
#create an empty vector
MSEs <- rep(NA, length(degrees))
#Create empty vector for tests
test_MSEs <- MSEs

for (i in seq_along(degrees)) {
  # regress for each power on each loop
  ols <- lm(Y ~ X_mat[, 1:i])
  # record the MSEs
  MSEs[i] <- mean(ols$residuals ^ 2)
  # do again for new set, only word because we use the same X
  new_errors <- new_Y - ols$fitted.values
  # record
  test_MSEs[i] <- mean(new_errors ^ 2)
}
```

Plot in base R

```{r}
plot(degrees, MSEs, type = "b",
     ylim = c(0, max(test_MSEs)))
lines(degrees, test_MSEs, type = "b", col = "red")
legend("topright",
       legend = c("Test", "Train"),
       fill = c("red", "black"))
```

Plot in tidyverse

```{r eval=FALSE, include=FALSE}
ggplot(data = NULL, mapping = aes(x = degrees, y = MSEs))+
  geom_point(aes(color = "black")) + geom_line(aes(color = "black")) +  
  geom_point(mapping = aes(x = degrees, y = test_MSEs, color = "red")) + geom_line(mapping = aes(x = degrees, y = test_MSEs, color = "red")) +
  scale_color_manual(name = "model",
                     breaks = c("Test", "Train"),
                     values = c("black" = "black", "red" = "red"))
```


```{r}
summary(ols)
```

This should be really concerning. It would mean strong colinearity. R is doing us a favor and automatically dropping some redundant features.

Create data under the so called regression model. Regression is an algo, but it is also a model for creating data. ODS is a least square algoristhm.

```{r}
n <- 100
p <- 5
# God knows this!
params <- runif(p, -10, 10)

features <- matrix(rnorm(n * p),
                   nrow=n, ncol=p)
X <- features
X1 <- matrix(rnorm(n * p),
              nrow=n, ncol=p)

noise <- rnorm(n)
noise1 <- rnorm(n)
Y1 <- X %*% params + noise
Y2 <- 0
for(i in seq_len(p)){
  Y2 <- Y2 + params[i] * X1[, i]
}
Y2 <- Y2 + noise1
plot(X[,1], Y1)
points(X1[, 1], Y2, col="red")


```

```{r}
df <- as.data.frame(cbind(Y,X))

dim(df)

head(df,2)

names(df) <- c("Y", paste0("X", 1:p))

ols <- lm(Y ~ ., df)
summary(ols)

class(ols)
predict(ols)

#if you pass in a lm or glm, predict will use predict.lm or predict.glm anyways. It is smart. 

#There is an argument in predict that uses "new data". You need to pass in what the new data is. It should be tempting for us to just pass in X1. 
#This shouldn't work! But why?
#predict(ols, newdata = as.data.frame(X1))

#convert to df

#now there is an error that we don't know what X2 is. The data frame you are passing it needs to have the same names that you are training on. 

names(df)

df1  <- as.data.frame(X1)
names(df1) <- names(df)[-1]

test_preds <- predict(
  ols, 
  newdata = df1)

#The data you are passing, you need the data to look identical to the data you trained the model with. The names of the data frames must agree. 
#classic workflow
plot(test_preds , Y2)

test_errors <- Y2 - test_preds
test_rmse <- sqrt(mean(test_errors ^ 2))
test_rmse


sd(noise)
sd(noise1)
#You cannot reduce beyond this. 

```

R will automatically throw out extremely high colinearity instances. In the real world this would be rare. This is unique to R.

```{r}
names(ols)

#probably the most important
ols$coefficients

plot(ols$coefficients, c(0,params))
abline(a = 0, b=1)

```

```{r}
train_features <- cbind(1,as.matrix(df[,-1])) #take out y column
#fitted_vals <- train_features %*% 
  
fitted_vals <- train_features %*% ols$coefficients
sum(abs(fitted_vals - ols$fitted.values))

res <- df$Y - ols$fitted.values
sum(abs(res - ols$residuals))
plot(ols$residuals)
abline(h = 0)

```

```{r}
#you can also put plot onto the regression function itself
plot(ols)
#residuals vs fitted values. This is what we saw earlier but much fancier

#QQ Plot

#scale location not that important

#leverage to look for outliers
```

#### Traps

```{r}
df_missing <- df
df_missing[20, "Y"]  <- NA #purposeffully lose the value
ols <- lm(Y ~., df_missing)
length(ols$residuals)
#lm drops missing value before matrix multiplication. So the residuals will change. 


```

### Interactions + subtracting variables

#### Another trap

```{r}
ols <- lm(Y ~ X1 + X2 + X2*X3, df)
ols <- lm(Y ~ X1 + X2 + X2:X3, df)

summary(ols)
test_preds <- predict(ols, df1)
head(df1,2)
#when you start manipulating the data inbetween then you get the problems

#If you do feature engineering for test, then ADD SOMETHING
```

```{r}
ols <- lm(Y ~ . - X4, df)
summary(ols)

#get rid of intercept
ols <- lm(Y  ~ . -1, df)
summary(ols)
```

INSERT STUFF ABOUT INTERACTION TERMS: COLON THING

## Last trap, ording of the data

```{r}
x <- runif(n, -2 ,2)
y <- x^2 * 3

ols <-  lm(y~x)
summary(ols)

predict(ols, data.frame(x = runif(n,-2,2)))

predict(ols, data.frame(x = runif(n, -2,2)))

plot(x,y)

plot(ols$residuals)
#we should expect the residuals to be quadratic as well
# we need to order the data correctly

plot(ols$fitted.values,
     ols$residuals)

#remember the reisdualds are ordered the same as the data. If the data was random, then the residuals will be random. 

plot(ols)

#naming comment
#If we decided to 
dim(X)
length(Y)


ols <- lm(Y ~ X[,1:2])
summary(ols)
#the naming is a mess!

wrong_stuff <- predict(ols, data.frame("X[, 1:2]" = 1:3,
                        "X[, 1:2]2" = 1:3))
                        

mysterious_vals <- predict(ols)                       
sum(abs(mysterious_vals - ols$fitted.values))
```

### Missing

If there is no overlap in the data. Then there is no overlapping data. You can't run regression in that case.

you should probably have a reason to say you need a certain amount of overlap. The best way to get there is to find the needed level of confidence and then back in the answer.

### GLM

```{r}
inv_logit <- function(x) {
  return(exp(x) / (1 + exp(x)))
}
y <- rbinom(n, 1, prob = inv_logit(X %*% params))
plot(X[, 1], y)
plot(X[, 2], y)

df <- as.data.frame(cbind(y, X))
names(df) <- c("Y", paste0("X", 1:5))
log_reg <- glm(Y ~ ., df,
               family = binomial(logit))

summary(log_reg)
predict(log_reg)
myst_vals <- predict(log_reg, type = "response")

X_test <- matrix(rnorm(n * p), nrow = n)
X_test_df <- as.data.frame(X_test)
names(X_test_df) <- names(df)[-1]

test_preds <-
  predict(log_reg, type = "response", newdata = X_test_df)
head(test_preds)
params

```
