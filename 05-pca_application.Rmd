# Principal Component Analysis Applied!

## Citation data

### Other example
In really poor countries it is super hard to measure wealth and income. There are no reciepts and corresponding taxes. People don't have bank accounts. Instead, you measure features. Like do you have a fridge. Do you have cooking equipment? How many kids? How many room in your house? 

So you could run PCA on assets matrix. You can find correlations. If you have more rooms in your house, you likely have more education. The correlations will be baked into the principal driving component. Further, they use this as the Y to see if they can predict! But that is beyond the scope of this class. 




```{r}
library(jsonlite)

#citation count between his 15 papers and those he sighted
citations <- read.csv("Datasets/j_cunningham_citation.csv", head = FALSE)

titles <- read_json("Datasets/j_cunningham_citation_titles.json")
```

#Explore the data
```{r}
dim(citations)

head(citations)
citations[1:5,1:5]
max(citations)

#across all paers
apply(citations, 1, max)

```

```{r}
names(titles)




```

Papers that he has written: 
```{r}
head(titles[["auth_titles"]],3)
```

Papers he has cited
```{r}
head(titles[["ref_titles"]],3)
```

Among the 15, there are four papers that reference the 2 most popular articles. Let us find them:

```{r}
ref_count <- apply(citations, 2, function(x) 
sum(x > 0))

targets <- tail(names(sort(ref_count)),2)

#These are the two columns we want
target_ind <- which(names(citations) %in% targets)

target_ind

titles[["ref_titles"]][target_ind]
```

Explore this data: we know the index of the two. This can show the correlation between the two, meaning the papers are cited by certain papers. This would make sense. If you cite one of these, you almost certainly have to cite the other:

```{r}
citations[,target_ind]
```
A few things to remember. Longer papers should have more citations. There is also likely to be correlation between certain papers. 

We would intuitively just want to apply our "prcomp", like we learned in last class. 
```{r}

pr_out <- prcomp(citations)
plot(pr_out$sdev, main="")
```
This plot is not very appealing. There is not a significant drop until the last term. Maybe between 1 and 2 and 3 and 4, but not a big drop. And if you only abandon 1 dimension, (14 instead of 15), you aren't really saving a lot. 


#### Try standardizing the citation matrix in different ways

##### Usual standardization, i.e. make each feature mean=0 and sd = 1
```{r}

norm_citation <- apply(citations, 2 , scale)
#also

pr_out <- prcomp(norm_citation)

plot(pr_out$sdev, main="")

png("Datasets/loadings_normal_standardization.png", 900, 700)
par(mfrow=c(4, 3))
for(i in seq_len(ncol(pr_out$rotation[,1:12]))){
    eigenvec <- pr_out$rotation[, i]
    plot(eigenvec, main = paste("Eigenvec",i))
    abline(h=0)
}
dev.off()

```
![Plots using normal standardization](Datasets/loadings_normal_standardization.png)

Remember from the plot, if we square all the values and add them together they will equal one. So strong deviations will be papers that are relied on (by index). Too many papers that share the same weight, might not be helpful. We believe there are only a few really good papers that we want. Not like 50. So this behavior is still undesirable. 

This is bad! You subtract something away from 0 values. But we like 0s because they don't affect the objective function. PCA is using the "frebenious norm" where everything squared and added together is 1. So we like 0s. So how can we scale differently, but while keeping the 0s. 


##### Max normalized, i.e. make each feature min=0, max = 1
```{r}
#lets divide by the max, then everything is between 0 and one
norm_citation <- apply(citations, 2,
                       function(x) x / max(x))

pr_out <- prcomp(norm_citation, center=FALSE,
                 scale=FALSE)

png("Datasets/max_normalized.png", 900, 700)
par(mfrow=c(4, 3))
for(i in seq_len(ncol(pr_out$rotation[,1:12]))){
    eigenvec <- pr_out$rotation[, i]
    plot(eigenvec, main = paste("Eigenvec",i))
    abline(h=0)
}

dev.off()
```
This should look much nicer. For example, Eigvec 4 looks better. Why did we normalize the columns? We often do this because the columns have different units. However, in this, the columns all have the same units. Instead, papers have different lengths, so the citation number can be affected by the length of the paper. So whay do we want to actually normalize? The rows!


##### Max normalized per paper, i.e. make each ROW min=0, max = 1
```{r}
citations_norm_papers <-  apply(citations, 1,
                       function(x) x / max(x))


# Just doing the above actually swapped things! We have 15 columns instead
#It processes a row, and then stacks it as a column. So we need to transpose
citations_norm_papers <- t(citations_norm_papers)


pr_out <- prcomp(citations_norm_papers)

plot(pr_out$sdev, main="")
png("Datasets/loadings_norm_per_paper.png", 900, 700)
par(mfrow=c(4, 3))
for(i in seq_len(ncol(pr_out$rotation[,1:12]))){
    eigenvec <- pr_out$rotation[, i]
    plot(eigenvec)
    abline(h=0)
}
dev.off()

```

Ther is a much more noticable drop between 11 and 12




```{r}
pr_out <- prcomp(citations)
plot(pr_out$sdev, main="")
png("Datasets/loadings.png", 900, 700)
par(mfrow=c(4, 3))
for(i in seq_len(ncol(pr_out$rotation[,1:12]))){
    eigenvec <- pr_out$rotation[, i]
    plot(eigenvec)
    abline(h=0)
}
dev.off()

target_ind <- which(abs(pr_out$rotation[, 2]) > 0.15)
titles[["ref_titles"]][target_ind]
```

1st column second row is 

2nd column 3rd row is a disaster. 


### Fishing out various titles




